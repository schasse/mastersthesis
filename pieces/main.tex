\chapter{Introduction}
\label{chap:intro}
\section{Automation to Support Maintenance Work}

% TODO: microservices case study von klems
DevOps~\cite{devops_definition} and microservices~\cite{microservices_fowler} are
practices or even cultures, which break down the silos between the development and
operation teams and their conflicting goals. For instance Amazon and Facebook successfully
use those practices to build and run their software products~\cite{build_run,dev_at_fb}.

However the responsibility of maintaining a software product changed. A single team is
responsible in two aspects: their own code and the dependencies they are using and relying
on. The need for maintenance of own code exists, for instance with non-functional changes
concerning performance, scalability or bug fixes. Code written by others needs to be
maintained as well via adapting and updating to new versions.

DevOps practices interconnect operations including those maintenance necessities with
development work. Nevertheless there is room for improvement in terms of continuous
automatable processes regarding maintenance. The \gls{cd}
pipeline~\cite{cd_humble_pipeline} is such a continuous automatable practice. But it
covers the process only up to the deploy, whereas maintenance involves more than just the
delivery.

Instead of employing more engineers, Google combats workforce problems with
automation~\cite{sre_automation}. They recognized that automation does not only save
valuable programmer time, it furthermore adds reliability to the process. Google explains
how valuable automation is and successfully illustrates it with their site reliability
engineering approach~\cite{sre_intro}.

This thesis proposes with \gls{nprt} a practice which addresses the amount of maintenance
work. The practice is designed for teams which have the responsibility of developing and
operating their own code as well as code dependencies. \gls{nprt} extends the \gls{cd}
pipeline and automates continuous testing of non-functional changes in production. I
implemented \gls{nprt} at two companies, \gls{din} and GapFish, in order to make an
evaluation.

\section{Thesis Outline}

\paragraph{\ref{chap:problem} \nameref{chap:problem}}
Chapter~\ref{chap:problem} elaborates on the problem of the amount of maintenance work and
demonstrates how the thesis contributes to solve the issue.

\paragraph{\ref{chap:background} \nameref{chap:background}}
The glossary explains the practices and techniques required in order to understand the
problem in depth. In contrast to this, chapter~\ref{chap:background} briefly explains the
major Kubernetes resources on the example of a 3 tier web app and names the software
dependencies.

\paragraph{\ref{chap:macro} \nameref{chap:macro}}
Chapter~\ref{chap:macro} describes the conceptual macro view of \gls{nprt}. The text goes
through all the steps of the pipeline and demonstrates the components and how they
communicate with each other.

\paragraph{\ref{chap:detail} \nameref{chap:detail}}
Chapter~\ref{chap:detail} goes more into detail of the thesis contributions. It
demonstrates the design of \deployer, the software I wrote in order to enable \gls{nprt}
in practice. The chapter also elaborates on the collection and comparison of the metrics.

\paragraph{\ref{chap:eval} \nameref{chap:eval}}
Chapter~\ref{chap:eval} evaluates \gls{nprt} in three aspects. Firstly, it discusses the
performance with the help of experiments. Secondly, it provides a qualitative evaluation
of \gls{nprt} based on the experiences at \gls{din} and GapFish. And lastly, it relates
\gls{nprt} to other practices, tools and sophisticated metric analysis.

\paragraph{\ref{chap:conclusion} \nameref{chap:conclusion}}
Chapter~\ref{chap:conclusion} summarizes the thesis and suggests more automation
opportunities and extensions to the implementation.

\chapter{Problem Definition}
\label{chap:problem}

\section{Maintenance Responsibility Shift}

% devops/microservices aproach successful development and operation
The DevOps\footnote{see glossary: \gls{g:devops}} approach and the
microservices\footnote{see glossary: \gls{g:microservices}} culture create practices to
solve the issues of the conflicting tension~\cite{sre_intro} between a development and
operation team. On the one hand the aim is to change the software product quickly and on
the other hand to provide a stable and reliable software product of high quality. Vogels,
the chief technology officer of amazon, once said in an interview ”You build it, you run
it”~\cite{build_run}. This quote represents the awareness of teams to not only be
responsible for the development and deliver features. Moreover the team is also
responsible of operating their product in production, because operation is an essential
part of a successful software product. One team is consequently responsible of the
development, operation and maintenance of their own code.

% responsibility of deps shifts from ops to dev
Furthermore the responsibility of software dependencies shifts. In the past, development
teams delivered code and depended on the previously installed software
dependencies. Operation teams were responsible of maintaining those installed
dependencies. Virtualization changed this. Teams operating virtual infrastructure operate
and maintain the hardware and software of this infrastructure. They do not maintain the
specific dependencies of the code, which the development team develops. The development
team can choose which dependencies they want to use. Accordingly there is a clear
isolation between the dependencies of the virtual machine infrastructure and the
development team.

% contra arg vms operated by ops team
You could argue that virtual machines still have the same functionality as bare metal
servers and need to be maintained in the same way by the same operation
teams. Nevertheless it becomes easier to maintain dynamic virtual
infrastructure\footnote{see glossary: \gls{g:dynamic}} in comparison to bare metal
infrastructure. And the easier it becomes, the more it enables developers to operate their
software product themselves.

Containerization\footnote{see glossary: \gls{g:container}} manifests the responsibility
shift. It is a further step of isolation. The operating system is able to isolate CPU and
memory for every process. Yet with containerization the operating system can also isolate
the disk between processes, which means it can isolate the installed dependencies for
every process. Containerization techniques even potentiate the ease of managing
dependencies. So the responsibility of the software dependencies shifts resultantly
towards the team, which builds and runs the software product.


\section{Lack of Focus on Maintenance}

% is risky
If developers did not do maintenance properly, there is a risk of decreasing the quality
of the software product. For instance there can be security vulnerabilities. Those can be
very harmful to the company of the software product. If developers do not maintain the
software products properly more issues arise, such as bugs or performance problems. On
account of that, maintaining a software product comes with responsibility. If developers
disregard maintenance and operations work, the quality of the software product suffers.

% operate and maintaining software means: config, monitoring, incident management
What does it mean to operate and maintain a software product in production? It is all the
work related to running the software product. It includes the configuration of the
servers. It includes monitoring of performance, errors and security issues. And it
includes handling of incidents. For example if there are any issues, you would need to
react to those issues and to make the software product for instance scale or apply
security patches.

% (maintenance is a lot of work and) is easily disregarded in favor of velocity
Agile and continuous practices focus primarily on feature development. Those practices
altered software development to deliver features faster into production. Highsmith argues
”velocity is killing agility”~\cite{velocity_kills}. Also Fitzgerald observes the issue of
a ”misplaced focus on speed rather than continuity”~\cite{continuous_fitzgerald}. Focusing
on development velocity only has a major drawback. Operating software in production and
maintaining it needs to be done properly to ensure the quality of a software
product. Companies easily disregard the importance of operation and maintenance of the
software product.


\section{Underestimated Amount of Maintenance Work}

% maintenance of the written code itself is a major source of changes
The software written by the development team needs to be maintained as well. As I
illustrated previously, different issues occur in production. For instance performance and
scaling issues or simply bugs. The developers need to refactor the software without
changing the actual functionality. Those changes are mostly invisible or do not alter any
function visible to users. At Facebook those invisible changes are more than 50\% of the
total changes~\cite{dev_at_fb}.

% dependencies are becoming more
Moreover, software products have a high amount of dependencies. This is very reasonable,
because when programming a software product you do not want to do it from
scratch. Firstly, the dependencies are different abstraction levels, such as a processor,
a programming language and a programming framework. And secondly, the dependencies are
modules which encapsulate a certain functionality. Those functionality can be common
functionalities for applications, for instance authentication of a user. The abstraction
as well as the reusable software modules make it easier and faster for developers to reach
their goals.

% many versions in open source
The open source communities change and release their software continuously. Sometimes
those changes consist of new feature, often times just refactorings and every now and then
security patches. This results in releases and new versions.

% almost every software product relies on open source
On the web, there is rarely a software product which does not depend on open source
software. Most web servers run Linux and there are many popular open source frameworks
which make up the basis of the running software products.

% RedHat/Ubuntu companies provide bundling and update service
Companies like RedHat or Ubuntu provide enterprise support to those open source
projects. Those companies bundle open source projects, care about the bugs and
security issues and provide tools to upgrade from one version to another easily. As a
result companies with a software product can outsource the maintenance work of the
dependencies. There is still the need though, to integrate those changes to the own
software product.

% hard to upgrade at once
One problem is that it is not always easy to upgrade from one version to
another. Refactorings result in breaking changes and the depending software needs to adapt
to those changes. As a result software such as Windows XP is still out in the wild. Due to
the fact that companies are not able to adapt to those major changes at once. However the
very same applies to open source frameworks such as Django or Ruby on Rails.

% microservices create ability to continuously upgrade one part by another
% it is much work though
One reason for the introduction of microservices is the flexibility to update only a small
specific part of the whole software product~\cite{microservices_fowler}. With a
microservice architecture, developers are able to update a single service without having
to upgrade the whole product at once. Developers can choose a microservice for the update
depending on its importance. Or developers can update microservices one by one without the
major problem of upgrading the whole software product at once. Consequently this means
developers need to update more often and the amount of work of updates itself is higher.

\section{Disregard of Automation for Maintenance Work}

% compensation by automation instead of more humans
There are several things companies can do to keep up with the amount of maintenance
work. Facebook~\cite{dev_at_fb} for example hires more developers. This, however, is
expensive. A much better solution is automation. Developers should automate some of the
maintenance work.

% practices exist which connect dev and ops
You could argue that DevOps practices already exist and connect development and operations
including maintenance of a software product. However, the focus lies mostly on testing the
quality of a change in a lean fashion and to reduce risk of a change in
production~\cite{devops_definition}. It is true that those practices help to deliver
changes faster to production and help to detect issues at low risk. Nevertheless,
monitoring and reacting to errors requires manual work.

% testing in production is already done
There exist practices which already enable production testing~\cite{dev_at_fb}. One
example is A/B testing, where you compare two different versions in production. Another
canary releasing\footnote{see glossary: \gls{g:canary}}, with which you change only a few
replicated parts of the production environment to reduce risk. There is also shadow
releasing, where users are not affected by the change, which goes into production. Netflix
tests its services with the simian army in production to detect unforeseen events in a
controlled environment~\cite{antifragile_org}.

% containerization reduces risk for updates
Containerization reduces the risk of updating a service. Morris describes phoenix
replacement~\cite{infra_as_code_phoenix} in immutable infrastructure\footnote{see
  glossary: \gls{g:immutable}}. Kubernetes\footnote{see glossary: \gls{g:kubernetes}} has
the approach of rolling updates~\footnote{see glossary: \gls{g:rolling} and \gls{g:zdp}}
which implements the same practices. A containerized ecosystem is typically
immutable. Rather than changing a running container directly, the container image
repository\footnote{see glossary: \gls{g:image} and \gls{g:registry}} would receive and
store an updated container image including the changes to deploy. Then Kubernetes would
create a new container based on that updated image. After Kubernetes started this new
container, it can stop the old container. The container image repository still stores
every container image version. Those saved images allow to easily rollback to a previous
version. Automation enables developers to deploy more frequently which usually leads to
deploys with few changes. Few changes per deploy and the ability to rollback makes updates
less risky. In result, updates in an automated containerized immutable infrastructure are
less risky compared to a manually operated infrastructure.

Murphy et~al. illustrate the value of automation~\cite{sre_automation}. Software
reliability engineers automate tasks to firstly save valuable human resources and secondly
reduce the risk in those tasks because of human errors. This is a good approach and
focuses on the quality of the software product.

Immutability reduces some of the risks and automation brings value. Combining those two
arguments, there is an obvious need of an automated process for updating services.

% cd provides good automation, but has lack of maintenance focus
There is a lack of truly automatable practices for maintaining a software
product. \gls{cd}\footnote{see glossary: \gls{g:cd}} brought up good automatable practices
to continuously deliver changes and features into production. It has the lack of good
practices though to maintain a software product.

\gls{cd} does not focus on the maintenance work and the DevOps approach does not focus on
automating the maintenance work as previously examined. I conclude that there is a need
for practices, which continuously support and automate maintenance work.

\section{Contribution of this Thesis}

The contribution of this mater's thesis is a practice, which can fully be automated in
order to maintain a software product. It extends the \gls{cd} and continuous deployment
pipeline\footnote{see glossary: \gls{g:cdep}} as those go until release and no
further. \gls{nprt} extends the pipeline to the production environment, where we need to
operate and maintain a system.

The thesis suggests \gls{nprt} as a practice which consists of different techniques. Those
techniques cover three major steps. First, quickly deploy a change to production. Second,
monitor the change, having it in production at low risk. And third in case of a regression
automatically roll back.

I implemented the tool, \deployer, in order to connect essential components of the
practice, \gls{nprt}. The thesis evaluates the performance of the tool as well as the
practice itself via experiments as well as in the production environment of an existing
software product.

\chapter{Background}
\label{chap:background}

\section{Typical 3 Tier Web App in Kubernetes}

I'll explain the typical implementation of a 3 tier web app in Kubernetes as it helps a
lot to understand the next chapters. The application tier is usually implemented with a
\myemph{Deployment}\footnote{see glossary: \gls{g:deployment} and \gls{g:deploy}} which
monitors the presence of a specified number of stateless \myemph{Pods}\footnote{see
  glossary: \gls{g:pod}}. A \myemph{Service}\footnote{see glossary: \gls{g:service}} acts as
a load balancer and selects the Pods and forwards the request. The data tier is
implemented with a \myemph{StatefulSet}\footnote{see glossary: \gls{g:statefulset}} which
monitors the presence of the stateful Pods. Those stateful Pods keep their host name and
mount the same volumes when they are being restarted.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0 \textwidth]{3tierwebapp}
  \caption{Typical 3 Tier Web App in Kubernetes}
  \label{fig:3tierwebapp}
\end{figure}

When the Service receives a request from the client, it selects a Pod via round robin and
proxies the request to the Pod. The Pod probably communicates with the database and sends
the request back to the client, where the Service acts again as a reverse proxy.

\section{Software Dependencies}

This section shortly mentions the dependencies, which I used to implement \deployer and the
tools for other components of the pipeline.

\deployer is implemented in Ruby\footnote{\url{https://www.ruby-lang.org}}. it is a
web server and Sinatra\footnote{\url{http://sinatrarb.com}} is its framework. \deployer uses
Git\footnote{\url{https://git-scm.com}} and
Subversion\footnote{\url{https://subversion.apache.org}} to communicate with either Git or
Subversion as the control version system. Moreover, \deployer has a plugin which sends
messages to Bugsnag\footnote{\url{https://www.bugsnag.com}} and
Slack\footnote{\url{https://slack.com}} to inform the developer in case it identifies a
problem. \deployer uses kubectl to interact with
Kubernetes\footnote{\url{https://kubernetes.io}, see also
  \cite{kubernetes_architecture,kubernetes_architecture2}} master. \deployer itself is
containerized with Docker\footnote{\url{https://www.docker.com}} and its natural hosting
solution would be Kubernetes. We published \deployer on
Github\footnote{\url{https://github.com/gapfish/deployer}} under GPL-3.0, a free software
license.

The previously developed
\gemupdater\footnote{\url{https://github.com/schasse/gem_updater}} creates pull request
with dependency updates.

The monitoring system\footnote{see glossary: \gls{g:mon}}, which I used in the evaluation,
is Datadog\footnote{\url{https://www.datadoghq.com}}, a commercial service. Nevertheless
Prometheus\footnote{\url{https://prometheus.io}} is also a suitable open source solution
as monitoring system. The \gls{ci}\footnote{see glossary: \gls{g:ci}} systems we use for
the evaluation are Codeship\footnote{\url{https://codeship.com}} and
Jenkins\footnote{\url{https://jenkins.io}}. As an image repository, we use Docker
Hub\footnote{\url{https://hub.docker.com}}. Goreplay\footnote{\url{https://github.com/buger/goreplay}}
served to make experiments with production traffic.

\chapter{NPRT Approach}
\label{chap:macro}

In \gls{nprt} the pipeline deploys a non-functional change in form of a canary to the
production environment. The new version is compared with the stable version side by side
in production. In case the new version shows a regression, the pipeline will automatically
roll it back. The novelty of this approach is that it creates a feedback loop between
development and production, which is fully automatable. The following elaborates on the
term in more detail.

\myemph{Non-functional} refers to the change, which the monitoring system
monitors. Usually maintenance changes are non-functional changes, for instance performance
improvements or security updates. \myemph{Production} refers to the environment, in which
the versions are monitored. The production traffic reaches the new version as well as the
stable version. The term \myemph{regression} refers to the \myemph{testing} strategy. The
monitoring system tests the metrics of the stable version and the new version for a
regression.

\gls{nprt} provides some further features, which are not included in the term. Indeed the
testing approach is completely automatable and developers are able to continuously apply
it to new versions. I designed \gls{nprt} in respect to failing as fast as possible and to
inform developers as early as possible. \gls{nprt} is most useful for maintenance
changes. Even automated maintenance changes, for instance dependency updates, are
possible.

\gls{nprt} naturally evolved from common practices such as \gls{ci}, \gls{cd} and
continuous deployment and extends those practices. The already established practices
support developers before and until the software deployment. In contrast to that,
\gls{nprt}, supports developers during and after the deploy. In other words it supports
developers to run and maintain applications in production, which formerly has been a
business of operations teams.

\section{Pipeline Overview}

\begin{figure}[htbp]
  \includegraphics[width=0.97\textwidth]{nprt_overview}
  \caption{Overview of NPRT}
  \label{fig:nprt_flow}
\end{figure}

To understand the testing approach as a whole, it is necessary to show a complete overview
of the whole pipeline and environment. Figure \ref{fig:nprt_flow} depicts an overview. In
the following I will go through the steps of the pipeline and discuss them.

\section{Continuous Integration as Requirement}

The first parts of the pipeline are commonly known and established practices: \gls{ci},
\gls{cd} and continuous deployment. It is necessary, though, to touch on them and
integrate them in the whole picture. It will illustrate important design decisions for
\gls{nprt}.

First the developer changes the code on his local machine and creates a new version. He
then pushes the new version to the \gls{vcs} as shown in~\texttt{(1)}.

After the push of the new version, the second step is a message~\texttt{(2)} to the
\gls{ci} system. This message holds the reference of the new version and the \gls{ci}
server pulls the new version from the \gls{vcs}. Now the \gls{ci} system has three major
jobs. Firstly it starts a build process~\texttt{(3-4)}, secondly it runs the
tests~\texttt{(5)} and the thirdly gives the deploy signal~\texttt{(6)}.

In step~\texttt{(3)}, namely the build, the \gls{ci} system typically compiles binaries,
renders assets and may create further artifacts. For our purposes it is especially
necessary to build at least one or multiple container images. The \gls{ci} system then
pushes the ready built container image to an image repository in~\texttt{(4)}. Later the
image repository serves the built images.

One import thing is that every built container image relates to a specific version. The
version reference\footnote{see glossary: \gls{g:versionref}}, which was created by the
\gls{vcs}, serves for this purpose. It is important to be able to trace the version
through every step in the pipeline. With this thought in mind, the \gls{ci} system tags
the container image with the version reference and an extra name. Maybe it should be
mentioned that the extra name is not absolutely necessary to definitely associate the
container image with a version. Experience has shown that a human readable name is very
helpful in recognizing a version and to know what the version is about at first sight. The
\gls{ci} system derives that extra name from the branch name. This tag, consisting out of
the version and extra name, will follow us through the whole pipeline as readable and
unique reference.

The second \gls{ci} step is to run the tests. Figure~\ref{fig:nprt_flow} shows this in
step~\texttt{(5)}. The tests themselves can be split into multiple stages, such as unit,
feature and smoke tests. At this point, though, I will not recall all the details of
automated testing at this point.

The last step of the \gls{ci} system is to send a deploy signal, step~\texttt{(6)} in the
figure. But it depends on the results of the tests, whether to send that deploy signal or
not. The tests can be successful or fail. If the tests fail, the \gls{ci} system does not
send a deploy message, the pipeline stops and the \gls{ci} system may inform the
responsible developer. If the tests are successful in all test stages, the \gls{ci} system
will send the deploy signal to \deployer.

It is reasonable to deploy only specific versions and not every commit. The practice which
is pretty common, is that you develop new features on a separate branch. For those
versions you usually do not send a deploy signal even though the branch's build and tests
are successful. Usually, after the tests, a review is done and the developers decide to
deploy the changes to production. When the decision is made and one developer merges the
change into a specified branch, for instance the master branch, the version will go to
production.

Clarifyingly it has to be mentioned that the \gls{ci} system sends each built image for
every single version to the image repository. This is independent of successful tests or
the decision to go to production. The reason why I want to have every built image in the
repository is to be able to test it in the \gls{ci} system, locally and maybe on a staging
system.

The \gls{ci} system gives the deploy signal when two requirements are fulfilled: the build
and tests are successful and developers decided that the version will go to production.

Until this point, as already mentioned, this is \gls{ci} practice which is commonly used
in software development. I require it for \gls{nprt}. For \gls{nprt} it is especially
important to associate and trace every step through the whole pipeline. For that purpose I
need the \gls{ci} system to tag, as the previous paragraphs explain, the container images
with the exact version reference.

\section{Customizations of Continuous Deployment}

\gls{nprt} is an approach, which I designed to be completely automatable. It is crucial to
not only have \gls{ci}, but to have a customized continuous deployment process as well.

The next component in the pipeline is \deployer. \deployer is a service which realizes
the customized parts of the continuous deployment practice. In the context of this thesis,
I implemented \deployer. Chapter~\ref{chap:detail} describes \deployer in detail. This
chapter demonstrates how \deployer integrates in the pipeline and in the environment among
all the other tools. It is crucial to have full control over the deploy process and as a
consequence it was necessary to implement \deployer.

It would also be possible to implement the logic of the deploy in the \gls{ci} system. But
I had to decide against that, because the deploy needs full access to the production
system and the \gls{ci} system is in our case outsourced to a third party company. I did
not want to give other companies full access to the production cluster. However, this
meant that I had to implement some steps again, which a \gls{ci} server already
implements. \deployer pulls the version from the \gls{vcs} as well as the \gls{ci}. The
difference to the \gls{ci} system is that \deployer's interest lies in the resource
definitions and not the application code. The resource definitions\footnote{see glossary:
  \gls{g:resource}} define the application infrastructure.

\deployer receives the deploy signal from the \gls{ci} system\texttt{(6)}. The deploy
signal again includes the version reference. Now \deployer executes three major steps:
firstly pull the resource definitions~\texttt{(7)}, secondly modify the resource
definitions~\texttt{(8)} and thirdly send the resource definitions to the production
cluster~\texttt{(9)}.

In the first step, \deployer pulls the code from the \gls{vcs}~\texttt{(7)}. The \gls{vcs}
also holds the resource definitions, which describe the application infrastructure. I
wanted to version control\footnote{see glossary: \gls{g:vcs}} the resource definitions in
order to be able to relate the version of the infrastructure, the version of the code and
the version of the artifacts. In Kubernetes those definitions contain different resources,
which chapter~\ref{chap:background} explains with an example.

In the second step, \deployer modifies those resource definitions~\texttt{(8)} in a way
that the production system uses the correct container image which relates to the version
to deploy. The modifications of \deployer also achieve that the running container is aware
of its version. The latter is important to later tag the metrics with the version
reference.

The third step of \deployer is to apply the modified infrastructure definitions to the
production cluster which is shown in the figure as step~\texttt{(9)}. In result a deploy
changes two things: application infrastructure as well as application code changes.

The production cluster receives the modified infrastructure definitions. The production
changes the cluster state according to the definitions. Most important is that Pod
instance of two versions in parallel are in the cluster. The production cluster fetches
the container images in step~\texttt{(10)} from the image repository. The images, which
the \gls{ci} system built in step~\texttt{(3)}. The production cluster is aware of the
specific image identified by the tag.

Running multiple versions in production might not sound optimal. Kubernetes approaches zero downtime
deploys via rolling updates. Rolling updates require it to be able to run two successive versions
side by side. Accordingly, the proposed technique optimally fits to the model of rolling updates.

\section{Metric Collection and Comparison}

Figure~\ref{fig:nprt_flow} illustrates the two differing versions with two colors. Most of
the running instances are in the stable version, red, and only one instance or in practice
few instances are in the new version, orange. These few instances are called canaries.
The load balancer sends traffic to both versions and both versions respond to clients, on
which section~\ref{test_architecture} elaborates.

After a \deployer deployed the canary, a regression might arise. On account to that, 
the regression should affect the least users as possible. This is why there is only few
canaries running in the production cluster. This means for an exemplary ratio of three
instances in the stable versions to one canary in the new version, only 25\% of the 
traffic would hit the canary with the regression. Even if the request of a specific
single user hits the degraded instance, the next request of the same user has the
probability 75\% to hit the stable version. This certainly lowers the risk of failure
for a single user. 

A limitation to the approach is that the new version needs to be able to run side by
side with the stable version. This is why the new version should not have any
functional changes compared to stable version, but only non-functional changes. However, that
means \gls{nprt} cannot test new feature like in an A/B test. Instead, \gls{nprt} is
appropriate to test performance improvements, refactoring or dependency updates.

The production cluster collects the metrics which are necessary for the regression tests.
I adapted those metrics from the four golden metrics of Google's
\gls{sre}~\cite{sre_monitoring}. The metrics are throughput, latency, error rate and
utilization.

The interest lies in the metrics of the two specific versions. Consequently the production
cluster labels those metrics with the version references. This is important since the
metrics of the differing versions will be compared with each other. The production cluster
labels and then sends those metrics to the monitoring system~\texttt{(10)}.

The monitoring system stores all the metrics of the two versions in a time series
database. The monitoring system evaluates those metrics by drawing graphs and comparing
those graphs with each other. One example would be that it draws two graphs for the
latency in one diagram. The first latency graph is the one of the stable version. The
second latency graph is the one of the new version. The monitoring system monitors now
those two graphs for a regression. In the case of latency, if the latency of the new
version is much higher than the latency of the stable version, the monitoring system
detects this as a regression. I explain the specifics of the metric comparison and in the
section~\ref{sec:metric_comparison}.

\section{Rollouts and Rollbacks}

Two possible scenarios arise. The first one would be that the new version runs in
production for a certain amount of time and the monitoring system does not identify any
regression. In this case the monitoring system does nothing. A deploy with the reference
to the new version triggers the full rollout of the new version. A usual deploy message
with the new version reference is sufficient to do so. \deployer receives the deploy
message, deletes the canary and modifies resource definitions to have the new version as
the new stable version and the production cluster proceeds, stops and starts the running
instances accordingly.

The second scenario is that the new version turns out to be a regression compared to the
stable version. In that case the new version in form of the canary should be rolled
back. The monitoring system identifies the regression and sends a message to \deployer, as
figure~\ref{fig:nprt_flow} pictures in step~\texttt{(12)}. Accordingly the test for
regression has failed.

\deployer receives the rollback message, pulls the resource definitions again, modifies the
resource definitions accordingly and applies the modified resource definitions to the
production cluster. To be precise, it is actually a deploy message with the commit
hash of the stable version, which the monitoring system is aware of because of the metrics
it monitors. \deployer modifies the resource definitions similar to step~\texttt{(8)}. The
deploy is idempotent, so that \deployer can repeatedly receive the same deploys. This
can happen in the case a developer as well as the monitoring system trigger the
rollback. The production system itself takes care of deleting the canary instances in the
new version. Since the instances of the stable version are already running in the
production cluster, the production cluster does not modify the running stable instances.

\chapter{Implementation of Deployer and Metric Comparison}
\label{chap:detail}

I implemented \deployer and setup metrics collection and comparison in order to realize
\gls{nprt} with Kubernetes. \deployer receives a deploy message, modifies Kubernetes
resource definitions and applies those modified resources to the production
cluster. \deployer is able to automatically deploy a canary of a new version of an
application which runs in parallel to instances of the stable version. The Kubernetes
cluster is setup to collect the metrics from the instances of both versions. Datadog, the
monitoring system compares the metrics and checks them for regressions.

\section{Deployer Architecture}

Similar to the previous chapter, I demonstrate the implementation with the help of
figures. The following sections will explain all the steps of
figure~\ref{fig:deployer_architecture}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{deployer_architecture}
  \caption{Detailed Deployer Architecture and Steps}
  \label{fig:deployer_architecture}
\end{figure}

As mentioned earlier, \deployer is a web server which runs in Kubernetes itself, is
stateless and can even deploy itself. The production cluster in
figure~\ref{fig:deployer_architecture} is the same Kubernetes cluster in which the
application to test runs in. The yellow box is a Kubernetes Service. The Service is, as
described in chapter~\ref{chap:background}, a load balancer. Clients can reach \deployer
via an http interface. The figure shows that \deployer consists of multiple Pods. These
Pods are replicated and are identical in their behaviour. The Pods are basically
stateless, yet every Pod has its own caching layer which section~\ref{sec:cache} explains
in more detail. \deployer uses approximately 50mb memory and few CPU, so computing
resources are no problem. I designed \deployer to have synchronous communication with a
client. \deployer waits for the deploy process to complete before it responds. This has
the advantage of clients receiving feedback about the deploy. Furthermore it enables
\deployer of being independent of other services such as a queuing or worker
system. \deployer's statelessness leads to easy horizontal scaling of \deployer. It is
possible to add more Pods if needed in order to achieve more concurrent deploys.

\section{Deployer Interface}
\label{sec:interface}

I designed requests to \deployer to be simple and usable. Curl or any other http client
serves as a client. As a result, \deployer can easily integrate with other systems for
instance with a \gls{ci} system. Another way to interact with \deployer is to use the
\depctl command line interface.

The \depctl command line interface wraps http calls and assists developers with automatic
completion of the service to deploy and completion of the version to deploy. With \depctl,
developers can easily skip steps of the whole \gls{nprt} flow. For instance they can skip
the tests which would run on the \gls{ci} system. This makes development more fluent and
developer friendly.

\begin{table}[!htbp]
\begin{verbatim}


   | Endpoint            | depctl command | Parameters  |
   |---------------------+----------------+-------------|
   | GET /               | ls             |             |
   | GET /SERVICE        | show           |             |
   | GET /SERVICE/tags   | tags           |             |
   | PUT /SERVICE/deploy | deploy         | commit, tag |
   | PUT /SERVICE/canary | canary         | commit, tag |
   | GET /version        | version        |             |

\end{verbatim}
\caption{Deployer Interface}
\label{tab:interface}
\end{table}
Examples:
\begin{verbatim}
curl --data commit=025838f \
  https://auth_token:secret@deployer.me.com/app/deploy

depctl deploy
\end{verbatim}

The following will go through the interface of \deployer. \deployer provides different
endpoints: ls, show, tags, deploy, canary and version. The following descriptions uses the
term service in the meaning of a microservice.

The \myemph{ls} or index endpoint returns the configured services and returns the endpoints
for the service, which the configuration defines.

The \myemph{tags} endpoint shows the available tags for that service. \deployer queries the
Docker image registry for all available tags for all images for a service and returns them
to the client.

\myemph{Deploy} and \myemph{canary} are the endpoints for either a regular or a canary
deploy. The requests needs to provide either a version reference, a tag or both. The
request updates the service, this is why the endpoint defines a http put.

The \myemph{version} endpoint simply returns the \deployer version.

\section{Logic and Flow of a Deploy}

\subsection{Authorization}

Next I want to go through the steps involved in the deploy process. Initially a client
sends a http request to \deployer~\texttt{(1)}. The Service resource proxies the request
to \deployer~\texttt{(2)}. After that, \deployer needs to authenticate the client via http
basic authentication. In other words, the client authenticates itself via a username and
password. As I mentioned earlier, the \gls{ci} system is a software as a service solution,
and one of the reasons why I implemented \deployer is the concern about security. The
\gls{ci} system can not have full production cluster access. In the case of \deployer the
auth token authorizes to only deploy a specific version from the repository. For us that
means, that the \gls{ci} system is only able to deploy versions from the \gls{vcs}. The
\gls{ci} system is for instance neither able to deploy other code than ours nor read
credentials from the production cluster.

\subsection{Fetching the Code and Caching It}
\label{sec:cache}

After \deployer authenticated a valid client, it then fetches the code from the
\gls{vcs}~\texttt{(3)}. If Git is the \gls{vcs}, \deployer uses commit hashes and branches
as version reference and to determine the code version. If Subversion is the \gls{vcs},
\deployer uses revision numbers instead. \deployer implements Subversion, which is an
older \gls{vcs}, to be able to do the evaluation with the \gls{din} system.

It is obvious that the process of fetching a repository includes persistence and disk
interaction. \deployer uses the volume just as cache, though. The reason for the existence
of the cache is performance. One of the bottlenecks of a deploy is downloading the
repository. If a repository is large, for instance because of pictures or a long history
of commits, it takes quite an amount of time to download it. If the download rate is
additionally low the duration is even longer. This leads to long running deploys and a bad
development flow experience. Therefore \deployer keeps the already downloaded repositories
on its volume as a cache. The next time \deployer deploys the same repository in a
different version and \deployer only fetches the changes.

As a simplification, every Pod has its own cache and lives as long as the Pod. Thus every
Pod utilizes its own volume as a cache and the need for communication and an extra caching
service does not exist. Since Docker containers are immutable, every time Kubernetes
recreates a Pod, Kubernetes destroys the volume. Thus the cached data is not available
anymore and in other words the cache is empty again.

The version control system contains application code as well as the resource
definitions. In Kubernetes these resource definitions define the application
infrastructure. Resources define for instance Deployments, StatefulSets and
Services. These resource definitions should be stored in the '/kubernetes' directory. This
is a convention of \deployer and it assumes that the '/kubernetes' directory is the
standard location. If application is unable to store the resource definitions in the
'/kubernetes' directory, the configuration of \deployer provides an option to reconfigure
the location.

The configuration of the Kubernetes resource directory enables two different methodologies
of the microservice approach. The first one is the multiple repository methodology. In
this methodology every single service or microservice receives a dedicated repository.
The second methodology is the single monolithic repository. Such a repository contains
multiple microservices.

\subsection{Validations}

After \deployer fetched the specific version reference from the repository, \deployer starts
the deploy procedure. At first \deployer validates the arguments, which the client
sent. The deploy request requires the service and the version to deploy. The client
defines the version by passing either the commit hash, the Docker image tag or both. The
tag includes the branch name, which makes the reference more readable.

\deployer initially validates the arguments of the request. \deployer checks if the service
exists in the configuration. \deployer checks if the commit hash exists in the repository
and \deployer checks if the tag exists for all the images, which are necessary in order to
deploy the service in the new version. \deployer receives the available tags for a Docker
images by communicating with the image repository~\texttt{(4)}.

For simplicity and usability the client sends usually only one argument, either the commit
hash or the tag. One argument is completely sufficient to determine the version, so a
client can provide either one of them. For instance, a developer has to pick only the
commit hash for a manually triggered deploy. This creates a more efficient development
flow. Regardless of the whether \deployer receives multiple or only one argument, \deployer
validates if the deploy of the version is feasible.

The http client and \depctl do not have any validations. Consequently, \deployer is the
single point of defining validations.

\subsection{Modifications of Resource Definitions}

After \deployer validated the deploy request, \deployer takes the previously checked out
resources and modifies them in the next step~\texttt{(5)}. Especially Deployments and
StatefulSets are relevant to those modifications. In contrast to those resources, \deployer
does not modify all other resources, such as Services.

\deployer applies two main modifications: the \myemph{environment} and the image tag. The
former is especially important for the metrics collection. \deployer modifies the
environment in order to enable the instrumentation to differentiate between the stable
version and the canary version. The running Pods can later read the version information
from the environment variables and are able to adjust the instrumentation configuration to
add the version information.

The second important modification is the \myemph{image tag}. The image tag specifies the
container image. The specific container image is import as it holds the specific
application version. \deployer picks the correct image tag belonging to the version
reference from the \gls{vcs}.

\deployer modifies the resource definitions only when the image tag is unspecified. In
case a Deployment resource already specifies an image tag, for instance somebody else
maintains the image, then \deployer does not change the specified tag. The reason is that
the \gls{ci} does not build images, which are maintained by somebody else. Such images do
not have a tag with the specific version reference of the \gls{vcs}. One example is an
additional statsd container running as a sidecar in the application Pod.

The other case would be that the Kubernetes resource defintion leaves the tag
specification open. Then \deployer appends the tag to the container image according to
reference which the client provided.

The next step is to communicate the modifications to the Kubernetes
API~\texttt{(6)}. \deployer sends the modified resources to the Kubernetes API. The
Kubernetes master manages the rollout of the changes. It swaps out one Pod by another and
replaces the old version with the new version. The procedure is called rolling update.

\subsection{Errorhandling}

Sometimes something unexpected happens during the deploy. For example \deployer does not
find the tag to the corresponding commit hash, then the verification fails. Another
example is that the Kubernetes API server returns an error. If this happens, \deployer will
inform the developers. \deployer distinguishes between two different clients.

The first one is a \gls{ci} system, which sent the deploy request. In this case the error
the error reporting system\footnote{Chapter~\ref{chap:background} mentions Bugsnag. The
  application of GapFish uses it for error reporting.} comes into play. \deployer sends the
error to the reporting system ~\texttt{(7)} which collects the errors of multiple
systems. The error reporting system informs the developers via sending notifications to a
specified channel. GapFish has for example a Slack chat channel, to which the person on
call subscribes and receives notifications from.

The other client is a developer sending a deploy request manually. In this case the
developer typically uses \depctl. \deployer differentiates usual curl requests from
requests with \depctl. Instead of utilizing the error reporting system, \deployer answers
the http request with an errorcode and a message in the http body. As mentioned earlier
the communication with \deployer is synchronous.

As a result, developers always receive feedback as fast as possible in case a deploy
fails. The information receives the developer in both cases, whether the \gls{ci} system
triggers a deploy or a developer manually triggers a deploy.

\section{Testing Architecture}
\label{test_architecture}

This section explains the testing architecture inside the production cluster. Few canary
Pods in the new version run next to the stable version. These Pods are able to receive and
respond to production traffic.

\begin{figure}[htbp] \centering \includegraphics[width=\textwidth]{deployer_flow}
  \caption{Canary Testing Architecture}
  \label{fig:testing_flow}
\end{figure}


To deploy a canary, the client sends the deploy canary request to a separate http path,
which section~\ref{sec:interface} previously described. The client has to provide the
same arguments as for the deploy request. Namely those arguments are the service and
either a commit hash, a tag or both. When \deployer receives the canary deploy request, it
proceeds almost identical to the deploy request. The steps are verification, fetching the
version, resource modification and application to the Kubernetes API. The difference to
the deploy lies in the modification step. Instead of deploying a new version, \deployer
creates an additional new canary Deployment resource.

In the modification step \deployer does not only change the container image version, but
it adds another canary Deployment. \deployer names the new Deployment resource with the
name of the original Deployment resource and a canary suffix. Kubernetes differentiates
the canary Deployment from the original Deployment. Kubernetes creates Pods for the
additional canary Deployment in the different version, which \deployer specified.

The Service though, which does the load balancing, selects both. It selects the Pods which
belong to the original Deployment. And it selects the Pods, which belong to the canary
Deployment. Hence the Service selects its Pods in account to the label. The original and
the canary Deployment share a subset of their labels. A shared label would be for instance
'deploy=webserver'. And the two differentiating labels would be 'track=stable' and
respectively 'track=canary'.

I designed the canary deploy to result in fewer canary Pods than stable Pods in order to
reduce risk as the previous chapter describes. \deployer does this simply by scaling the
canary Deployment to a single replica.

\section{Metrics}

\subsection{Metric Collection}

This section focuses on how metrics are being collected during when two versions are in
production. There exist two separate collection mechanisms. Those are hosts, which collect
metrics, and the application instrumentation, which sends metrics.

Firstly the group of metrics, which the host picks up. The host has information about the
Pods' utilization of CPU and memory. Accordingly, on each Kubernetes host, a monitoring
agent runs, which watches the '/proc' directory and the Docker daemon. The agent picks up
the information frequently and then sends it to the monitoring system.

Secondly, the group of metrics, which the application sends. In order to collect the
metrics, the application needs to be instrumented. Examples for those metrics are
throughput, latency and error rate. In practice, I instrumented the applications to use the
statsd protocol and statsd server for that purpose. Simple statsd libraries for most
languages and frameworks exist even though the instrumentation could be better. In case
there is a good instrumentation, developers can use it without much effort. The
instrumented application then sends the data to the statsd server after each request and
the statsd server aggregates the data. From there the statsd server forwards the
aggregated metrics data to the monitoring system.

It is important to correctly label the metrics in order to decide whether a canary or a
stable version sends the metric. The monitoring system is then able to distinguish between
the metrics of the stable version and the metrics of the canary version. For the first
group of metrics collection, the monitoring agent can pick up the label from the labeled
Pod. And for the second group of metrics collection, the instrumentation code of the
application picks up the label from environment variables, which \deployer has set to
either stable or canary.

\subsection{Metric Comparison}
\label{sec:metric_comparison}

The comparison between the canary and the stable version is the part of \gls{nprt}, which
creates the feedback loop.

The monitoring system basically consists out of a time series database, a graphing user
interface and an alarm system. The time series database persists the metrics. And the user
can define graphs from the metrics, which the user interface then displays. Moreover, the
monitoring systems enables developers to define rules which monitor the metrics in the
time series database. In case the metrics violate any rule, the monitoring system sends a
notification.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0 \textwidth]{metric_comparison}
  \caption{Comparing the Stable Version with the Canary Version}
  \label{fig:metrics_comparison}
\end{figure}

Figure~\ref{fig:metric_comparison} pictures an example of a rule on latency. The red
continuous line is the latency of the stable version. The yellow continuous line is the
latency of the canary version. The graph compares them side by side. The red dotted line
depicts a threshold. The threshold depends on the latency of the stable version and is
always 50ms higher than the stable version in the figure. In case the latency of the
canary goes above the threshold, the canary violates the rule and triggers an alarm. The
alarm is in fact a webhook, which would roll back the canary instance.

Next I discuss the metrics to monitor: Latency, utilization, errors and throughput.

Latency defines the duration of successful requests. Latency is a typical performance
metric of a web server. Utilization mostly refers to metrics such as CPU and memory
consumption. The error rate is defined as counted errors in a time period. Applications
sometimes have the problem of reporting too many errors without focus on important
ones. Also throughput of successful requests can be an indicator of the health of an
application.

Deviations between the stable and the canary always exist. The monitoring rules should
identify problematic regressions. This is oftentimes hard to
achieve. Section~\ref{sec:eval_metrics}, in chapter~evaluation, illustrates one such
example of a metric comparison which serves as a test.

\subsection{Metric Parameters}

A comparison includes parameters such as a threshold, an aggregation window and a duration
above the threshold. These parameters need adjustment in order to calibrate the rule and
have a good recall and precision. The following gives insights on the factors, which
affect those parameters.

The ratio of the stable and canary metrics as well as the throughput affect the size of
the \myemph{aggregation window}. Let's assume the application has a very low throughput of
only 1 request per minute and a ratio of 2 stable to 1 canary instance. In case the
aggregation window is 1 minute large, then either the stable Pods would have 1 request or
the canary Pod would have 1 request. This would be an improper aggregation window. Instead
the aggregation window could be rather large with a size of 120 minutes. As result, the
stable Pods would receive 80 requests and the canary Pods would receive 40 requests. This
would be a much more appropriate aggregation window. Consequently the lower the throughput
and the lower the stable canary ratio is, the larger the window should be.

The distribution of throughputs and latencies over the routes of the application affects
which percentile to use. I make an example: If there would be a single route, which has a
low throughput and a very high latency compared to other routes. The average would be an
improper \myemph{aggregation}. The outliers of low latencies would affect the
average. Instead a percentile or the median would represent the latency much better.

The range of throughputs and latencies also affects the \myemph{threshold}. If the
application had a larger range of throughputs and latencies, a proper throughput would
need to a higher. However we can affect the volatility of the metrics by choosing a larger
aggregation window. This allows to lower the throughput.

As a result, application with higher and the more evenly distributed throughputs have
better metrics in order to define proper monitoring rules.

\chapter{Evaluation}
\label{chap:eval}

This chapter evaluates the \gls{nprt} approach in three different aspects, experiments, a
lessons learned section and a related work section. Firstly, the experiments aim to
present an evaluation of the approach's performance in the context of a whole delivery
pipeline and how a sample metric comparison performs. The second part is a qualitative
discussion of the lessons learned of the application use cases at \gls{din} and GapFish.
In the case of GapFish it was an application in production. The third aspect, related work,
compares \gls{nprt} with other approaches in production and references similar tools like
\deployer.

\section{Experiments}

I had the opportunity to do experiments with two different production applications. The
first one is an Apache web server, which is the middleware in front of a Tomcat
application server. This application serves \gls{din}'s
website\footnote{\url{https://www.beuth.de}}. In the case of \gls{din}, I set up a version
control system, a Kubernetes cluster with \deployer, instrumentation and the monitoring
service. Developers working for \gls{din} helped to set up a container image
registry. I also migrated the Apache web server to be able to run on Kubernetes. The
Kubernetes resources for the Apache web server consisted of a Service and a Deployment
with two Pods.

The second application is a Ruby on Rails application which employees of GapFish utilize
as backoffice tools for several panel
platforms\footnote{p.e. \url{https://www.entscheiderclub.de} and
  \url{https://www.spiegel-panel.de}}. As an employee of GapFish I previously migrated the
Rails application to Kubernetes and utilized Codeship as \gls{ci}, Docker Hub as container
image registry and integrated Datadog as monitoring system. The application consists of a
Service and nine Deployments. Eight of the Deployments are different background workers
each with one Pod. One Deployment is for the Ruby on Rails application server with two
Pods.

The experiments for the \gls{din} Apache application were conducted on virtual machines
rented from Atos. The experiments for the GapFish application were conducted on virtual
machines hosted at Google. The appendix describes more about the specific details and the
data used for the plots.

Each of the experiments has three consecutive executions with three different canary
versions in the deploy and rollback experiments. The measurements had to be manually
collected from all the different logs of the involved systems. Those systems were the local http
client, the \gls{ci} system, \deployer and Kubernetes.

\subsection{Deploy}

The first experiment examines the performance of a deploy of canaries. In account to
that an http client triggered a deploy as described in previous
chapters. Figures~\ref{fig:deploy_din} and \ref{fig:deploy_gapfish} show the duration
between the deploy request \texttt{(6)} and the running canary. The blue part of the bar indicates
the time of the \deployer pulling the infrastructure code from the version control system,
the time to check of the version reference and image availability, the time of the resource defintions modification
and the time of the application to the Kubernetes cluster \texttt{(6-8)}. The red part is the duration
of Kubernetes starting the canary \texttt{(9-running)}.

\begin{figure}[htbp]
  \label{fig:deploy}
  \centering
  \begin{subfigure}{.5\textwidth}
    \includegraphics[width=\linewidth]{deploy_din}
    \caption{\gls{din} Apache Middleware}
    \label{fig:deploy_din}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \includegraphics[width=\linewidth]{deploy_gapfish}
    \caption[GapFish]{GapFish Rails Application Server}
    \label{fig:deploy_gapfish}
  \end{subfigure}
  \caption{Deploy}
\end{figure}

Figure~\ref{fig:deploy_din} shows that \gls{din}'s Apache middleware takes between 9 and 12
seconds with a median of 10 seconds. In the third execution, \deployer took 9 seconds and
the start of the canary was in every execution at 1 second.

Figure~\ref{fig:deploy_gapfish} shows that GapFish's Rails application server takes
between 65 and 106 seconds with a median of 93 seconds. The time \deployer took was 18
seconds in median.

In both experiments the second and third execution are faster than the first one. That is
because \deployer caches the code from the version control system and does only need to
download the difference in the consecutive executions. The impact of the cache on \gls{din}'s
Apache web server was not that large, because the repository size is comparably
small. Whereas the cache could save almost half of the time at the GapFish Rails
application.

The start of \gls{din}'s Apache Pods is with 1 second quite fast. This demonstrates the quick
scheduling of Kubernetes and quick startup of Docker containers compared to virtual
machines. The start of GapFish's application is between 45 and 75 seconds with median of
70 seconds.

Accordingly \gls{din}'s Apache application deploy was significantly faster. \deployer is able to
handle multiple Deployments as in the case of GapFish. However, the multiple requests to the
Kubernetes API slightly reduce the deploy duration. An improvement would be a
batched request. \deployer's cache works well for larger repository sizes.

It is important to pay attention with slow application startup durations when looking for
fast deploys. In the case of GapFish's Rails application server, this makes the
greatest impact.

The variation of the second and third executions come from the nature of the cloud
environment and internet services. In particular the requests to Docker Hub, the image container
registry, differ in the duration.

\subsection{Deploy in Pipeline}

This section shows a deploy in the full context of the \gls{cd} pipeline. The steps start
with the commit and push of a new version to the \gls{vcs}, the build of the
Docker image(s), saving the recently built images in the image registry, time spend by
\deployer and the remaining startup of the canary Pods. The testsuite runtime is not
included in this diagram. The focus lies only on the tools the thesis integrated.

At GapFish I could access all the involved systems. This was not the case at \gls{din}. That is
why the experiment was conducted for the GapFish Rails application only.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\linewidth]{whole_pipeline}
  \caption{Deploy in Pipeline}
  \label{fig:whole pipeline}
\end{figure}

The total time of the \gls{cd} pipeline was between 172 and 414 seconds with a median of
192 seconds. Again the caching saves time in the second and the third execution. This time
the \gls{ci} system and the image registry partly cache the layers of the images. They
only have to build and send the differing parts of the container image.

As a result, the diagram shows an overall pipeline duration for a typical canary change at around
3 minutes, including a build and deploy. The makes a continuous deployment of
new canaries absolutely feasible.

\subsection{Rollback}

This section takes a look at the rollback performance of \deployer. The blue part
of the bar again represents the time \deployer is involved that is from the rollback
request including steps \texttt{(7)} and \texttt{(8)} until the Kubernetes API responded. The red bars
represent the remaining time the Pods take to shut down.

Figure~\ref{fig:rollback_din} shows the rollback duration at \gls{din}
is between 4 and 5 seconds. The duration which \deployer takes, is less than in for a deploy.
That is mostly because a delete request to the Kubernetes API is faster.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{.5\textwidth}
    \includegraphics[width=\linewidth]{rollback_din}
    \caption{DIN}
    \label{fig:rollback_din}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \includegraphics[width=\linewidth]{rollback_gapfish}
    \caption{GapFish}
    \label{fig:rollback_gapfish}
  \end{subfigure}
  \caption{Rollback}
\end{figure}

In the case of GapFish's Rails application, we have longer rollback times. All three
executions of the rollback were 63 seconds. This is due to slow shutdowns of the worker
Pods. Kubernetes partly waits for the shutdown process of the Pods. That is why
\deployer spends most of the 48-49 seconds waiting for the Kubernetes API to respond.

Also for the rollback case, \deployer's performance is better with \gls{din}'s Apache system,
because of less Deployment resources and the fast Apache application. Nevertheless,
deploys as well as rollbacks involve zero downtime. On account of zero downtime, it is acceptable to have slightly longer durations.

\subsection{Metric Comparison}
\label{sec:eval_metrics}

This last experiment aims at an evaluation of the feasibility
of a metric comparison and the duration of a broken canary being in production. \gls{din}
provided one hour of production traffic of their website which a Goreplay process
recorded in production. During the experiments, this traffic dump was replayed with the test setup.

Due to the fact the test database state differs to the state of the production database, the
throughput of the requests with http 200 responses was at the low rate of 25 requests per
minute. Requests of in production logged in users mostly had http 404 as responses. The
successful 200 responses consisted of the publicly visible pages of the website.

At first I deployed a canary version with no changes in comparison to the stable version. The
two versions, side by side, calibrated the metric comparison. The first half of the
production traffic determined the size of the aggregation window as well as the threshold
of the comparison.

After the calibration was done, the actual experiment started. The experiment measures the
time between a defect canary being started and the canary being rolled back. Each
of the three executions had its own unique part of the second half hour production traffic.

\begin{verbatim}
ratio =
  apache_GET_200_count.rollup(sum, 30) /
  (2 * apache_canary_GET_200.rollup(sum, 30))
threshold = 3
abs(1 - ratio) > 3
\end{verbatim}

The test compares the throughput of successful responses. The canary responses are weighted by
the factor 2, because there were one canary Pod and two stable Pods. An aggregation window
of 30 seconds and a threshold of 3, which means the canary throughput had to differ 3 times
as much as the throughput of the stable version.

Figure~\ref{fig:metric_comparison} shows that in each of the three executions, the test
has successfully detected the broken canary. The duration of the defect canary in
production was between 129 and 400 seconds with a median of 185 seconds.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\linewidth]{metric_comparison.png}
  \caption{Metric Comparison}
  \label{fig:metric_comparison}
\end{figure}

The experiment demonstrates that the automation of a regression test in production is
feasible and completely automatable.

\section{Lessons Learned}

The GapFish Rails application of the previous experiments as well as two other
GapFish Ruby services utilize the \gls{nprt} approach. This section presents
qualitative feedback which mainly comes from use of \gls{nprt} in production.

On the one hand non-functional regression testing was used to test maintenance of own code,
such as refactorings and performance improvements. On the other hand \gls{nprt} was also
used to maintain dependencies. The previously developed \gemupdater regularly and
automatically opened pull requests with dependency updates. The canary version was tested against the stable version using
error metrics which I explain later on in more detail.

\subsection{Quick Debuggable Deploys}

Deploys of around 1.5 minutes conform to the need of developers of sufficiently quick
deploys. This is good compared to oftentimes long running test suites as it is the case with
GapFish's Rails application which has a current median duration of 12.5 minutes.

Another benefit is the ability to track the version reference from commit to the production
test throughout the whole pipeline. This makes it easy to debug versions
which run or used to be running in production. It is also possible to track the code
change of a stable and a canary version which used to be running side by side.

It has been a good decision to version the Kubernetes resources which are
infrastructure definitions. This made it also possible to track the
infrastructure changes.

Due to the nature of containerized infrastructure and its immutability, rollbacks to
specific versions are simple. The rolling update mechanism provides deploys and rollbacks
without downtime. This fact lead to a certain confidence of the engineering team to deploy
and rollback. As a result, the amount of deploys increased.

\subsection{Engineers Know the Product}

The experiments already showed that a simple side by side comparison of the throughput can
have good results for a test metric with production traffic. Critics could argue that
production metrics can be hard to evaluate and in a test environment it is rather easy to
achieve good results. The problem of overfitting in statistics is well known.

When choosing the wrong metrics for the production regression tests of the Ruby
applications of GapFish, it would not have been feasible to deliver as good results as the
first test showed. The problem lies in the granularity of the metrics. For example a total
throughput or total error rate of a comprehensive application is misleading. GapFish's
Rails application serves not only humans, but also provides interfaces for other
services. This leads on the one hand to sparse usage\footnote{The users of the Rails
  application are only internal employees.} on routes for users and on the other hand
to machines utilizing some routes extensively. The overall throughput was a poor fit.

The much better choice for a metric was using the error aggregation of Bugsnag. Bugsnag
aggregates errors, that it shows specifically newly introduced errors. This has been the
best metric for the regression tests.

% Nevertheless the regression tests have to be maintained as well, just like a
% testsuite. For this purpose it would be better to have the ability to version the
% regression tests.

In conclusion as a developer and engineer of the already have a good intuition on what is
working and what not. This is another evidence for the benefits of the same team building
and running a software product.

\subsection{Continuous and Fully Automatable}

\deployer connects the \gls{ci} system, Kubernetes and the monitoring system. These systems
create the ability to create a fully automatable delivery pipeline and a system for fully
automated rollbacks. Those are the step stones for more possibilities.

\gls{nprt} reduced the efforts of maintenance work on own code. The Engineers were able to
test changes in production quickly and have a small feedback loop.

The engineers working for \gls{din} are still sceptical to have unsupervised deploys. But
in the example of GapFish, the engineers became confident about the automation of the
deploy and regression test in production and start to use unsupervised deploys and tests
more often.

The possibility of dependency update automation with \gemupdater turned out to save effort
at low risk as well. The combination of \gemupdater and \gls{nprt} lead to a continuous
fully automated approach for dependency updates, which used to be an important, but
tedious maintenance work.

\section{Related Work}

There are multiple publications, which use methods to test software production in
production. Tang et~al. demonstrates how Facebook does gradual rollouts with
gatekeeper~\cite{fb_config_management} and tests with high amounts of traffic. But not
only canary or A/B tests are done in production. For example Falé proposes synthetic
monitoring~\cite{synthmonitoring}, which aims at functional tests being made in
production. This enables testing microservices in complex environments. Also Tseitlin
describes how Netflix~\cite{antifragile_org} tests their systems with the simian army,
which produce random defects in a controlled production environment. This practice lays
open unknown issues, which can be fixed in order to gain better reliability.

There are also countless open source tools, which help to implement continuous deployment
similar to \deployer. Vamp\footnote{\url{https://vamp.io}} has the ability to deploy
applications to Kubernetes automatically. Also
spinnaker\footnote{\url{https://www.spinnaker.io/}} can deploy applications to cloud
environments, with Kubernetes being one of them. Spinnaker very recently released a
feature to analyse canaries, just like for \gls{nprt} needed. The difference to \deployer
is that vamp and spinnaker are very complex and exhaustive. They also require other
databases. Whereas \deployer is a very simple and lightweight tool with focus on a single
purpose and consists of only one Docker image. Nevertheless \deployer can be integrated to
several monitoring solutions by the simple http interface.

Schermann et~al. developed bifrost~\cite{bifrost}, an open source tool similar to the
previously mentioned gatekeeper at Facebook. It implements canary testing and other
testing strategies as well. The approach is a complete different, though, to
\deployer. Bifrost is technically an automatically configurable middleware or
load balancer. It does not provide any automation for the deploy itself. \deployer
implements the deploy automation. It could be interesting to combine the two tools to have
more testing strategies available.

I used Datadog in the evaluation as monitoring system. However, there also exist other
monitoring systems, such as the open source monitoring system Prometheus, which provides
very similar features. Google has a similar monitoring system in there internal
infrastructure~\cite{sre_monitoring}.

Lastly, I am going to mention publications, which deal with metric analysis. Zimmerman
motivates the need for data scientists software teams~\cite{data_science_role2} and
identifies the multiple roles~\cite{data_science_role}. Farshchi et~al. evaluates a
regression based approach in order to detect errors during rolling
upgrades~\cite{anomaly_detection}. They demonstrate how to identify the set of metrics,
which have the highest chance to detect the errors. Their approach has a high precision
and recall. In comparison to that Bakshy and Frachtenberg developed a statistical model to
analyse errors of a distributed service in an A/B
experiment~\cite{error_analysis_of_distr_system}. They provide guidelines for the design
of error analysis models.

\chapter{Conclusion}
\label{chap:conclusion}

The thesis illustrated that \gls{nprt} extends the \gls{cd} pipeline, supports developers
not only until the deploy, but well into the operation of their
software product. The monitoring system evaluates the test without the supervision of the
developers at a very low risk. The change goes through the whole pipeline including tests
in the production environment. If test is successful, developers will be completely free
of work. Potential changes of bots would be robust enough to be able to act fully
automated and unsupervised.

\section{Summary}

The thesis proposed \gls{nprt} to support maintenance work. Chapter~\ref{chap:problem}
examined the problem of maintenance work. It illustrated that maintenance responsibility
shifts from the operations teams to developers due to DevOps and microservice
practices. The simplicity of containerization enables and manifests this shift as
well. The maintenance work consists of two main aspects: maintenance of the own code and
maintenance of dependencies. Many flexible microservices and many dependencies result in a
high amount of maintenance work. To address the issue of maintenance work, the chapter
suggests the value of automation which is time saving and reliable. It proposed \gls{nprt}
in order to extend the continuous delivery pipeline and to automate non-funtional
regression tests in production.

Chapter~\ref{chap:macro} demonstrated a macro view of \gls{nprt} and how it integrates
with all the involved systems. The chapter went through all the steps of the pipeline
including the steps: a developer commits a new version to the \gls{vcs}, the \gls{ci}
system builds and tests the new version, \deployer deploys the new version to production
in form of a canary, the monitoring system collects and compares the metrics of the side
by side running versions and in case of a regression \deployer rolls back the canary. It
is important to tag each step including the metrics with the version reference in order to
by able to debug a specific version. \gls{nprt} creates with those steps an automated
feedback loop between development and production.

I implemented \deployer in order to enable \gls{nprt} with Kubernetes and used Datadog for
the metric comparison. Chapter~\ref{chap:detail} firstly demonstrates the simplicity and
horizontal scalability of \deployer's architecture. Deployer is implemented in order to
run in Kubernetes and to deploy itself. The chapter went through \deployer's http
interface and its endpoints ls, show, tags, deploy, canary and version. Simple http
clients or depctl can interact with \deployer. Moreover, the chapter explains a deploy of
a canary. Therefore \deployer first authenticates the client, validates the given version
references, fetches and modifies the Kubernetes resource definitions and applies them to
the Kubernetes API. In case of an error, \deployer informs the developers as quick as
possible by sending an appropriate return message to the client and by sending the error
to a specified channel. The chapter describes how I designed the architecture of tow
versions running in Kubernetes and how Kubernetes does the load balancing between
them. The metrics of the application's instances are collected in two different ways. A
monitoring agent picks some of the metrics from the host and the application's
instrumentation sends the other metrics to the monitoring system. Finally,
chapter~\ref{chap:detail} elaborates on the metric comparison itself and important
parameters aggregation window, the aggregation and the threshold.

Chapter~\ref{chap:eval} evaluated \gls{nprt} in three different aspects: experiments,
lessons learned and related work. For the experiments I tested \deployer and the metric
comparison with two different production applications. The first one was an Apache web
server which \gls{din} uses as a middleware. The second application is a Rails web server
which serves as backoffice tool for GapFish. The experiments showed that \deployer's
duration of a deploy varies between 9 and 106 seconds, the duration of a rollback between
4 and 63 seconds. The long deploy of 106 and the long rollback of 63 seconds were
determined by the long start and stop times of GapFish's Rails application. In contrast to
the long start and stop, \deployer's performance was faster. The experiments proved that a
metric comparison in production is fully automatable. Also the usage of \gls{nprt} at
GapFish in production, proved that the metric comparison is feasible because developers
usually have a good understanding of their software product and are able to write suitable
tests. Also the deploy durations were efficient to use \deployer in production. The
chapter also demonstrated that GemUpdater, a tool to automatically update dependencies,
successfully committed version changes which were then automatically tested in production
via \gls{nprt}.

\section{Outlook and Future Work}

\gls{nprt} is a good approach to continuously automate the maintenance of a software
product. Nevertheless there are possible improvements. Goreplay has proven to be a good tool for the evaluation. In order to save even more risks,
Goreplay could serve to clone the traffic and send it to the canary. In this case, it would be
possible to compare 100\% of the production traffic and make the metric comparison much simpler and
more precise. Furthermore, the responses of the cloned traffic would not receive the users. This
would result in a riskless testing approach. But there is one open problem with this approach:
there must be a mechanism to imitate the interaction with the database.

A more realistic extension to \gls{nprt} are completely unsupervised updates. An automatic approach such as
\gemupdater works well. A developer resultantly has to do manual work only in case of a regression.
More automatic services of such kind are needed. One obvious use
case is the automation of updating Docker base images. A Docker base image updater would
help to maintain continuously update the operating systems and software, on which the
applications' Docker images are based. Furthermore there is a similar need for update
mechanism for other languages such as Java or Python, which come with their own dependency
managers.

Another aspect is the lack of good open source instrumentation. On the one hand good
instrumentation exists. On the other hand the instrumentation is proprietary and it cannot
be utilized outside of the proprietary ecosystem. We should invest more in more fine
grained open source instrumentation for popular application frameworks and web servers.

In order to be able to better maintain the metric comparisons, these would need to go into
version control as well. With Datadog, however, it was not possible to version those tests.
With other tools for instance Prometheus this may be possible.

% extend nprt for functional changes by including a load balancer
