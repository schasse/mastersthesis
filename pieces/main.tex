\chapter{Introduction}
\section{Post Release Testing supports developers to efficiently do operations work}

Many people talk about DevOps as well as there are multiple definitions and interpretations of the term DevOps. DevOps is referred as a philosophy, a culture, practices and specific tools. For my research, I will focus on two different aspects of the term DevOps:

The first one is the perspective of operation teams. Operation teams traditionally modeled infrastructure by installing physical hardware and by manually installing software components. With the rise of virtual machines and the cloud, it became possible to model infrastructure in software\footnote{``Infrastructure as Code'' describes different dynamic infrastructure types~\cite[p. 30]{infra_as_code} and how to model those by code~\cite[p. 42]{infra_as_code}.}. Modelling via software enables operation teams to use tools and practices\footnote{In the chapter ``Software Engineering Practices for Infrastrucure''~\cite[p. 179-194]{infra_as_code} practices like version controlling, continuous integration are described.} as seen in software engineering. Infrastructure code is version controlled, tested and can be automatically deployed.

The other aspect of DevOps\footnote{The book ``DevOps''~\cite{devops} is written in the view of a developer running a system.} is the perspective of developer teams. Previously developer teams were only responsible for developing new features. Software engineering practices got established and proven. One of those practices is the continuous delivery pipeline\footnote{For theoretical details on the continuous delivery pipeline read Part II of ``Continuous Delivery''~\cite[p. 103-140]{continuous_delivery} or a more practical approach by Wolff~\cite{continuous_delivery2}.}. The last step of the continuous delivery pipeline is the deployment. Formerly operation teams were responsible for deploying new features. The deployment as last step of the continuous delivery pipeline shifts a responsibility from operation to development. This shows that developer teams are becoming more and more responsible for running the software, they built.

\chapter{Continuous Delivery only covers practices until release}
\section{Continous Delivery disregards security and operations topics}
\section{Fast time to market is crucial}
\section{Continuous monitoring is hard}
monitoring change and trying to predict the future from data
\section{Simple day to day work must be automated}

\section{How to read this masterthesis}

\chapter{State of the art technologies and practices are the foundations for NPRT}
\section{The Continuous Delivery Pipeline consists of commit, automated testing and deployment}
\section{Docker packages applications}
\section{Kubernetes is a cluster operating system}
\section{Monitoring a highly dynamic infrastructure is role centric}

\chapter{Nonfunctional Production Regression Testing extends the Continuous Delivery Pipeline}
\section{NPRT evolves from continuous deployment and the CD Pipeline}

The environment we choose is based on common continous delivery practices. To summarize it, it consists of a version control system, a continous integration system with 3 stages, which are build, test, deploy an artifact repository, a production cluster and a monitoring system. To grasp the concept of nonfunctional production regression testing, it is necessary to have a general idea of how the whole delivery pipeline looks like. The delivery pipeline is a requirement to gain most profit from the concept. In the following I will go through the whole pipeline step by step.

The version control system contains the plan of the every part of the production system. This means that it does not only contain application code, but also infrastructure code. This is very important to the later deployment process. So in the beginning there is a change committed to the version control system, this change is saved and has a so called commit hash, which is our reference of the version throughout our whole pipeline. There can also be subversion with revisions in a sequencial order instead of commit hashes. After the version control system has received a change, it sends a message to the conitnuous integration system in step 2.

The continous integration system has three major steps: build, test, deploy. The build step produces code artifacts, these can be compiled code, compiled assets or most important for our case docker images. These docker images get a tag. As it was mentioned before the commit hash is crucial throughout the whole pipeline as a reference for a specific version, every produced artifact is tagged with the commit hash. In our case it made sense to have additional information in the tag for easier recognition of the image, and we went for the a format including the git branch name as well as the commit hash. There could also be multiple builds for a single version, so one could also include the build number. In our case we decided against that, because a build is reproducable and deterministic and gives for every repition the same result.

The next step of the cointinous integration system, is the testing. Testing is usually divided into several stages, but for the purpose of this thesis it's not necessary to extend that topic. We just would like to mention that the principle of ``fail fast'' should be considered. In the last step of the continous integration system is the deploy. After each step has a succesfull outcome, a message to the deployer is sent.

Until this point, we reached the end of the continuous delivery pipeline. Now the process of the nonfunctional production regression testing begins.

The deployer uses the infrastructure definitions, changes those accordingly to the specific deploy and applies the infratstructure definitions to the production cluster. And it now deploys the new version to test to the cluster. Changes to the infrastructure definitions include to change the versions of the build images, which the continous integration system created in advance and pushed to the image repository. Here it is important to have full control over the changes in the infrastructure definitions. With full control over that deployment process, we will be able to implement the deployment, which is needed to nonfunctional production regression testing. We will go more into detail in the following chapters.

After the changed infrastructure definition the production cluster, will take care of the changes itself. The definitions are declarative and cluster itself will recognize the difference between the current state of the production cluster and the newly applied definitions. It will then change the state to the desired state. We will have a closer look at this process later. The new production infrastructure is now tagged with the new version. Just like in the steps before we have the commit hash as the version. As already said earlier this version is necessary to know through the whole pipeline.

Now the production cluster is serving the traffic from the internet with the new version of code applied. The production cluster is monitored during the whole time via a monitoring system. This monitoring system consists of processes to collect different monitoring data. For the thesis the nonfunctional monitoring data is the only relevant. This data is collected inside the cluster and then sent to a timeseries database and is also tagged with the specific version, which is passed through the whole pipeline. The monitoring data itself is monitored by an alarm system, in which we can define rules to trigger an alarm.

The alarm itself is a feedback loop to the deployment system. When a test rule fails it sends a message to the deployment system, so that it can take care about it and roll back the malicious version in production. This is again done by applying changes to the infrastructure definitions in the production cluster. The cluster will then take care and roll back the the desired previous stable version.

The test is successful after a certain amount of time, if there is no regression detected. Then a full rollout to the new version is done. This version is now not only tested in a separate test environment, but also tested in the production environment. This is one more step to ensure the stability of the production system.

This is the general overview of the setup to run nonfunctional production regression tests. To conclude, we will summarize the steps again:

1. commit to version control system
2. continuous integration system runs build and pushes artifacts to the artifact repository
3. continuous integration system runs the test and sends the deploy message
4. deploy system changes infrastructure definitions to the specific version and apply them to production
5. production cluster will change to the new state
6. monitoring data of production traffic is collected
7. monitoring data is evaluated and in case of regression a rollback is triggered
8. in case no regression is detected the full rollout of the new version is continued

\section{A new version is validated with production traffic}



\section{Nonfunctional tests in NPRT focus on latency, utilization and errorrate}
\section{NPRT tests a new version for regression in its characteristics}
\section{NPRT enables unattended continuous updates}

\chapter{Deployer implements and automates NPRT}
\section{Version centric testing via commit hashes}
build, test, deploy, only 2 versions in production. undeploy a canary.
\section{A canary and its testing metrics know about themselves}
\section{Controlling deploys in the pipeline and manually}
\section{Comparison of versions triggers webhooks for further actions}
monitoring validation, fail

\chapter{Evaluation}
\section{How NPRT changes the behaviour of development teams}
\subsection{Deploys}
\subsection{Cycletime}
\subsection{Change}
\subsection{True/False Positives/Negatives}
\section{NPRT compared to other in production testing strategies}
\subsection{Netflix Simian Army to intensify NPRT}
\subsection{Synthetic Monitoring is functional post release testing}

\chapter{Conclusion}
\section{Resume}
\section{Outlook and future work}

% \part{Foundations}

% \chapter{Introduction: Developer and operation teams converge and both use software engineering practices}

% \chapter{Developers use the Continuous Delivery Pipeline}

% \section{The Continuous Delivery Pipeline consists of commitment, continuous integration and deployment}
% \section{Software Deployment approaches evolved from manual to automated}
% \subsection{Blue-Green Deployment allows Zero Downtime releases}

% The approach of blue green deployment involves two environments. One is the environment which serves production traffic, the other environment is in standby. You deploy to the other environment the new version. You then switch routing from the environment, which runs the old version, to the environment with the new version.

% This has the major disadvantage that it is a waste of resources. Just one environment doing work with serving production traffic as the other environment is just idling. Even if you use the idling environment as a staging it would be oversized and still wasting resources.

% \subsection{Automation leads to resource saving Phoenix Deployment and Rolling Deployments}

% In times of dynamic resource allocation, automation and virtual machines, it became easy to automatically spawn new servers. In for a deploy of a new version you would not change the servers, but automatically create new ones with the version to deploy, then switch the traffic from the old servers to the new servers, and in the end just destroy the old servers. This procedure is called phoenix replacement.

% \subsection{Canaries test releases with a small amount of traffic}


% Canary releasing is a way to test new versions of the application in production. But testing in production is risky, because when there is an error which leads to a defect or failure the users will get affected. And this costs money in any way.

% There comes canary releasing into play. A single canary can't do much harm and it's not a big catastrophe if the small canary is malicious. Let me explain it at the example of a typically scaled web application. Usually you do not have just a single web server, but multiple servers. So when releasing in the canary style, you change just a small proportion of the twenty webservers to the new version. Now two versions of the webservers are running at the same time.

% Usually you want exactly the same version of webservers in production with the exact same configuration. This makes systems easier to debug in case of an error. The maximum count of different versions during canary releasing is two. But why go for two different versions in production with the canary releasing technique, when it makes debugging in general harder.

% It's because with canary releasing you can achieve different goals. The first one is truly when an error occurs. Just because less users are affected by the error. The majority of webservers is still in the old version, so just a small portion of all the users have a poor experience with the erroneous small canary version fraction.

% In case of success, when everything works as expected, it is proven that there is less risk of errors, even under real production conditions.


% \subsection{continuous deployment is not continuous delivery}

% \chapter{Operators turned into Site Reliability Engineers}

% Companies, which run software on their servers, usually have a specialized operations team. A team of developers programs the software and after the team is finished, the software is handed to another team, which runs the software and keeps it running. Those teams are usually referred as operations teams. So what are their duties and responsibilities exactly?

% At first the already developed software needs to be deployed on an infrastructure. Therefore the operations team must provide an infrastructure. Infrastructure generally consists of hardware like racks, servers and networking devices. But in most cases, software depends on other software components and services, which are for example programming languages, dns or databases. Software usually changes. Most importantly there are security patches, but also software updates with new features are mandatory to install. Hence the operations team is responsible for installing, configuring and updating hardware and software components.

% To provide a certain quality of service, infrastructure as well as the software must be maintained. That means identifying any problems and moreover fixing the identified problems. To identify problems in the first place, there must be any kind of monitoring. A good monitoring system\cite[p. 127/8]{devops} finds failures and resulting faults, recognizes performance problems, usual workloads and it detects intruders. More obvious problems can be detected early, even before the problem leads to failures. For example a disk will soon be full or utilization of resources like cpu, ram, network or disk i/o is too high. Other failures can not be detected in advance and lead to a defective system.

% In case monitoring identifies a problem, the operations team needs to take care of the problem. Operations teams need to be on call and manage incidents. A new Incident must be rated how severe it is. In advance detected problems potentially leading to failures like a disk going to be full in few days, must not be handled immediately. Other incidents like an unavailable database service can be business critical and must be fixed as rapidly as possible. But often the cause of a failure is not obvious and the system must be debugged with specific tools. Critical failures mostly affect users, so another part of incident management is to inform the affected parties.

% Google states that the traditional approach of an operations team\footnote{Google actually calls it the sysadmins approach} is not efficient. It is expensive, it does not scale very well and it creates tension in interests\cite[p. 3/4]{site_reliability}.

% A growing system means there will be more hardware, more software components and a more complex configuration. More people are needed to install, monitor and maintain the infrastructure and services. The traditional operations team approach has the disadvantage, that it does not scale very well and produces more costs, the bigger the system becomes. Another disadvantage is conflicting interests of the developer team and operation team. Operations teams are payed to create stability and provide a certain quality of service. But change is the biggest source of failure\cite[p.10]{site_reliability}. That's why operations teams work under the motto of 'Never change a running system'. On the other hand, a good service changes to the needs of its users. Developers get paid to deliver as much features as possible, so they want frequent change. Those are conflicting interests and they harm the whole product. For Google it's clear, that the different assumptions and tension in interests of operations and developer teams produce a lot of hidden costs.

% \section{Site Reliability Engineers maintain systems like software engineers}

% Operations teams developed a lot of practices to both integrate the fundamentally conflicting interests of change and stability, but also scale the operations team approach. This reduces obvious and hidden costs and makes the whole work more efficient. A lot of those practices became possible, because the infrastructure turned from pure hardware into a dynamic software defined infrastructure. Google refers to this approach as the site reliability engineering approach. It is operations how an software engineer would do it.

% New technologies made it possible to define all infrastructure in code. One of the first who provided an dynamic infrastructure is amazon with ec2. The technology of virtualization made provisioning and configuration to software tasks instead of hardware tasks. This is what it made automate-able. Also Docker made it easier to package software and to deploy and configure the services\footnote{See Chapter 2 'Platforms' in \cite{infra_as_code} for those technologies}.

% With the possibility to automate the tasks an operations team has to do, there will come many benefits. The obvious value is that is scalabillity. When a task is automated and can be done by a machine, it is very cheap and easy to execute the task a lot more often. But there is a much bigger advance, which automation gives. When a task is automated it is a well defined process, which will consistently performed, where a human can easily make a mistake by manually executing the task. It can also be extended, measured and done at inconvenient times for humans\footnote{See Chapter 'Value of Automation' in \cite{site_reliability} for complete discussion of the advantages.}.

% The before one off tasks of provisioning, deploying and configuring have now well defined practices and processes. Those tasks are now reproducible because the definition or configuration of the infrastructure can now be stored in a version control system. And with automate-able deployment processes like zero downtime releases, phoenix deployment or rolling updates, those processes can be done continuously.

% Operations teams have collected great knowledge on monitoring. What are the key indicators for monitoring a system. What does a good monitoring infrastructure provide. Those questions I am going to examine in detail in the upcoming sections. But very important is that monitoring can notify a human or even better trigger automated tasks, that humans do not need to be involved at all, when a problem occurs.

% Also incident management can partly be automated. Distributed systems like databases have nowadays automatic failovers. Issues which have been serious, major incidents before and a humans had to manually intervene can, are now automatically triggered and the systems heal themselves. Disaster recovery is now easily done, because you can automatically reprovision the whole infrastructure as mentioned before.

% With those practices it means that in the end this means that the conflicting interests of change and stability can nowadays be more and more integrated.

% \section{Monitoring to identify Problems}
% \subsection{Health checks measure availability}
% \subsection{Measuring Latency, Traffic, Errors and Saturation identifies failures and performance problems}
% \subsection{Incident Management (/Notifications) for appropriate and fast actions in case of Problems}

% \chapter{Metrics measure team efficiency and software quality}
% \section{Velocity and cycletime are efficiency metrics for an agile team}
% cycle time measures quality of delivery engine
% \subsection{Deploys/Week indirectly measures velocity}
% and it measures the effect of a quality delivery engine
% we want many deploys per week
% \subsection{Deploy Duration is import for cycle time}
% \section{MTTR and Failurerate measure the quality of a software}
% we want low risk per deploy to achieve MTTR and low Failure Rate
% \subsection{LOCS/Deploy indirectly measures the risk per deploy}


% \part{New Practices}

% \chapter{Post Release Testing extends the Continuous Delivery Pipeline to support maintaining a system}

% The three main steps in the continuous delivery pipeline are: commitment to version control, continuous integration and release. After releasing, the new version will be operated. This includes monitoring, logging, security aspects and incident management\footnote{In ``Site Reliability Engineering'' monitoring and notifying principles~\cite[p. 55-63]{site_reliability} are well described.}. To enable developers to take more responsible in running the software, it is necessary to extend the practices of the continuous delivery pipeline.

% In my masterthesis I want to optimize the software engineering practices in order to empower developers in their bigger responsibilities. I want to focus on the process of deployment and enhance the continuous delivery pipeline. To achieve this, I want to examine operation practices like monitoring and incident management. And then extend the deployment process by three consecutive steps.

% The first step is to completely automate deployments of software features and achieve continuous deployment. The next step is to sensibly monitor new releases to give automatically feedback. Third and lastly, identify incidents to automatically trigger rollbacks to self-heal the software system. I'm going to call these last two steps continuous post release testing.

% \section{Post Release Testing leads to lower time to market}
% cycle time
% \section{It makes Releases consistent, measurable, fast and scalable}
% mttr, automation/automatic discussion
% \section{It is a new opportunity for risk management}
% identify test before release is a mttr of zero, after release still fast. easier to test in production (complexity of system)
% \section{Companies are already post release testing their software systems}
% \subsection{Netflix uses Simian Army to live test their systems}
% \subsection{Synthetic Monitoring tests a complex distributed system}
% \section{Post Release Testing with Canaries is appropriate for testing non change}
% ErrorRate as monitoring measure for automation
% problems in error rate measure defect and failure
% solution a secific heuristic
% \subsection{Black-Box monitoring is only one part and monitoring change is difficult}
% \subsection{Canary testing is important for maintenance but not feature deploys}
% \subsection{Continuous Delivery is a requirement}
% \subsection{Notifications in case a canary behaves different}
% \subsection{Automated Rollbacks for a automatic self healing system}

% \chapter{Implementing Canary Post Release Testing}
% \section{New technologies drive new techniques}
% \subsection{Kubernetes is a Cluster OS}
% \subsubsection{Resource Management in Kubernetes is made for high available services}
% \subsubsection{Deployments implement Rolling Updates}

% The extend to the canary releasing technique is a rolling update. It unifies phoenix replacement and canary releasing and extends it to a full deployment. You start with one server in the new version and spawn it automatically. Now it becomes part of the whole pool of servers and serves traffic as the server is ready. Now two different versions are serving traffic, just like with a canary release. But after the first step the deployment goes on and then destroys a server instance in the old version as you would do in the phoenix replacement. Then the procedure will be done multiple times until all servers of the old version are replaced by the new version.

% \subsection{DataDog is a Cluster Monitoring Systems as a sevice}
% alternativen Graphana, Prometheus
% \subsubsection{The main components are: Datacollection, Timeseriesdatabase, Graphing and Alerting}
% \subsubsection{DataDog integrates well in Kubernetes}
% \section{Deployer is a Service for Continuous Deployment and enables Canary Post Release testing}
% \subsection{Deployer integrates into the Continuous Delivery Pipeline}
% \subsection{Deployer integrates into Kubernetes and deploys itself}
% \subsection{Deployer integrates into Monitoring and enables Canary testing}
% \subsection{The main Deployer API features are Deployments and Canary deployments}
% depctl, curl
% \subsubsection{Deployments deploy a whole repository}
% \subsubsection{One Canary per Replicated Pods can be deployed and monitored}
% \subsubsection{Immediate Notifications in case of failure}
% difference depctl and curl
% \subsubsection{Automatic Rollbacks in case defect Canary}
% via datadog and triggers deployer
% future: staging deploys

% % \subsection{bugs + debugging}
% % \subsection{ilities}
% % \subsection{security}
% % \subsection{monitoring}


% \part{Evaluation}

% \chapter{GapFish is the company to evaluate Deployer}
% \section{GapFish's services are complex and highly available}
% overview of Gapfish's services
% \subsection{GapFish's Operation Service is used by internal Staff and Customers}
% operation service == operation.gapfish.com
% \section{GapFish uses tools for continuous deployment}
% \subsection{GapFish differentiates between development and operation}
% how is the deployment tradditionally done
% \subsection{Kubernetes enables GapFish to have development and operations in one team}
% which services are migrated to kubernetes

% \chapter{Metrics are implemented as logging output}
% \section{Deploys/Week}
% \section{Deploy Duration}
% \section{LOCS/Deploy}

% \chapter{Results}
% \section{Metrics: traditional vs. new approach}
% \section{theoretical/practical conclusion for deployer and cd}
% \section{for Gapfish}
% \section{Lessons learned and future}

% \chapter{resume}
