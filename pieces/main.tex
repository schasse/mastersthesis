\part{Foundations}

\chapter{Introduction: Developer and operation teams converge and both use software engineering practices}

Many people talk about DevOps as well as there are multiple definitions and interpretations of the term DevOps. DevOps is referred as a philosophy, a culture, practices and specific tools. For my research, I will focus on two different aspects of the term DevOps:

The first one is the perspective of operation teams. Operation teams traditionally modeled infrastructure by installing physical hardware and by manually installing software components. With the rise of virtual machines and the cloud, it became possible to model infrastructure in software\footnote{``Infrastructure as Code'' describes different dynamic infrastructure types~\cite[p. 30]{infra_as_code} and how to model those by code~\cite[p. 42]{infra_as_code}.}. Modelling via software enables operation teams to use tools and practices\footnote{In the chapter ``Software Engineering Practices for Infrastrucure''~\cite[p. 179-194]{infra_as_code} practices like version controlling, continuous integration are described.} as seen in software engineering. Infrastructure code is version controlled, tested and can be automatically deployed.

The other aspect of DevOps\footnote{The book ``DevOps''~\cite{devops} is written in the view of a developer running a system.} is the perspective of developer teams. Previously developer teams were only responsible for developing new features. Software engineering practices got established and proven. One of those practices is the continuous delivery pipeline\footnote{For theoretical details on the continuous delivery pipeline read Part II of ``Continuous Delivery''~\cite[p. 103-140]{continuous_delivery} or a more practical approach by Wolff~\cite{continuous_delivery2}.}. The last step of the continuous delivery pipeline is the deployment. Formerly operation teams were responsible for deploying new features. The deployment as last step of the continuous delivery pipeline shifts a responsibility from operation to development. This shows that developer teams are becoming more and more responsible for running the software, they built.

\chapter{Developers use the Continuous Delivery Pipeline}

\section{The Continuous Delivery Pipeline consists of commitment, continuous integration and deployment}
\section{Software Deployment approaches evolved from manual to automated}
\subsection{Blue-Green Deployment allows Zero Downtime releases}

The approach of blue green deployment involves two environments. One is the environment which serves production traffic, the other environment is in standby. You deploy to the other environment the new version. You then switch routing from the environment, which runs the old version, to the environment with the new version.

This has the major disadvantage that it is a waste of resources. Just one environment doing work with serving production traffic as the other environment is just idling. Even if you use the idling environment as a staging it would be oversized and still wasting resources.

\subsection{Automation leads to resource saving Phoenix Deployment and Rolling Deployments}

In times of dynamic resource allocation, automation and virtual machines, it became easy to automatically spawn new servers. In for a deploy of a new version you would not change the servers, but automatically create new ones with the version to deploy, then switch the traffic from the old servers to the new servers, and in the end just destroy the old servers. This procedure is called phoenix replacement.

\subsection{Canaries test releases with a small amount of traffic}


Canary releasing is a way to test new versions of the application in production. But testing in production is risky, because when there is an error which leads to a defect or failure the users will get affected. And this costs money in any way.

There comes canary releasing into play. A single canary can't do much harm and it's not a big catastrophe if the small canary is malicious. Let me explain it at the example of a typically scaled web application. Usually you do not have just a single web server, but multiple servers. So when releasing in the canary style, you change just a small proportion of the twenty webservers to the new version. Now two versions of the webservers are running at the same time.

Usually you want exactly the same version of webservers in production with the exact same configuration. This makes systems easier to debug in case of an error. The maximum count of different versions during canary releasing is two. But why go for two different versions in production with the canary releasing technique, when it makes debugging in general harder.

It's because with canary releasing you can achieve different goals. The first one is truly when an error occurs. Just because less users are affected by the error. The majority of webservers is still in the old version, so just a small portion of all the users have a poor experience with the erroneous small canary version fraction.

In case of success, when everything works as expected, it is proven that there is less risk of errors, even under real production conditions.


\subsection{continuous deployment is not continuous delivery}

\chapter{Operators turned into Site Reliability Engineers}

Traditionally a developer team developed the software and after they were finished, another team runs the software and keeps the software running. Those teams are usually referred as operation teams. So what are their duties and responsibilities exactly?

The already developed software needs to be deployed in the first place. Therefore operators are responsible to install the infrastructure. The infrastructure consists of real hardware servers in racks and a network for the communication. The software usually doesn't without any operating system and programming libraries. After these exist the actual software can be installed.

<-software must be configured

To provide a certain quality of service, the whole system must be maintained. That means to first identify any problems and second to fix the identified problems. And to identify problems in the first place, there must be a kind of monitoring. The key insights of monitoring\cite[p. 127/8]{devops} are identifying failures resulting faults, identifying performance problems and usual workloads and lastly to detect intruders.

The software usually changes. Most importantly there are security fixes to defend intruders. But it is possible that there are also new features for the software. So another aspect of maintaining the system, is to have regular updates for the system.

In case there is any problem identified, the operation team needs to fix the problem. The easier and more obvious problems can be detected in advance before the problem actually occures. For example the disk is will be soon full or the utilization of the resources cpu, ram, network and disk i/o is very high. Then the operation team has to scale the system to satisfy the users.

Other problems can not be detected in advance and lead to a failing system. The operation team needs to be on call and react in the case of a failure. Commonly hardware fails and then must be replaced. Or another example is that a configuration change leads to a failure and must then be fixed. In more precise words: the operation team must manage incidents.

Google states that the operation team approach is not efficient\cite[p. 3/4]{site_reliability}. It is expensive, it does not scale very well and tension in interests because of wrong incentives.

A growing system means there will be more hardware, more software components and a more complex configuration. With the traditional operation team approach there is the disadvantage, that the growing system needs a bigger operation team. A bigger operation team produces more costs and that means it does not scale very well.

Another disadvantage is conflicting interests of the developer team and operation team. The operation team is incentived to provide a certain quality of service and therefore highly stable system is preferred. And since the greatest source of failure is change\cite[p.10]{site_reliability}, the operation team does not want to have change. Never run a changing system. On the other hand developers want to change their software very frequently, because they get paid for deliver as many features as fast as possible. This combination leads to different assumptions and a tension in interests, which produces even more costs than the scalability issue.

\section{Site Reliability Engineers maintain applications like software engineers}

automation value

\section{Monitoring to identify Problems}
\subsection{Health checks measure availability}
\subsection{Measuring Latency, Traffic, Errors and Saturation identifies failures and performance problems}
\subsection{Incident Management (/Notifications) for appropriate and fast actions in case of Problems}

\chapter{Metrics can indirectly measure team efficiency and software quality}
\section{Velocity and cycletime are efficiency metrics for an agile team}
cycle time measures quality of delivery engine
\subsection{Deploys/Week indirectly measures velocity}
and it measures the effect of a quality delivery engine
we want many deploys per week
\subsection{Deploy Duration is import for cycle time}
\section{MTTR and Failurerate measures the quality of a software}
we want low risk per deploy to achieve MTTR and low Failure Rate
\subsection{LOCS/Deploy indirectly measures the risk per Deploy}


\part{New Practices}

\chapter{Post Release Testing extends the Continuous Delivery Pipeline to support maintaining a system}

The three main steps in the continuous delivery pipeline are: commitment to version control, continuous integration and release. After releasing, the new version will be operated. This includes monitoring, logging, security aspects and incident management\footnote{In ``Site Reliability Engineering'' monitoring and notifying principles~\cite[p. 55-63]{site_reliability} are well described.}. To enable developers to take more responsible in running the software, it is necessary to extend the practices of the continuous delivery pipeline.

In my masterthesis I want to optimize the software engineering practices in order to empower developers in their bigger responsibilities. I want to focus on the process of deployment and enhance the continuous delivery pipeline. To achieve this, I want to examine operation practices like monitoring and incident management. And then extend the deployment process by three consecutive steps.

The first step is to completely automate deployments of software features and achieve continuous deployment. The next step is to sensibly monitor new releases to give automatically feedback. Third and lastly, identify incidents to automatically trigger rollbacks to self-heal the software system. I'm going to call these last two steps continuous post release testing.

\section{Post Release Testing leads to lower time to market}
cycle time
\section{It makes Releases consistent, measurable, fast and scalable}
mttr, automation/automatic discussion
\section{It is a new opportunity for risk management}
identify test before release is a mttr of zero, after release still fast. easier to test in production (complexity of system)
\section{Companies are already post release testing their software systems}
\subsection{Netflix uses Simian Army to live test their systems}
\subsection{Synthetic Monitoring tests a complex distributed system}
\section{Post Release Testing with Canaries is appropriate for testing non change}
ErrorRate as monitoring measure for automation
problems in error rate measure defect and failure
solution a secific heuristic
\subsection{Black-Box monitoring is only one part and monitoring change is difficult}
\subsection{Canary testing is important for maintenance but not feature deploys}
\subsection{Continuous Delivery is a requirement}
\subsection{Notifications in case a canary behaves different}
\subsection{Automated Rollbacks for a automatic self healing system}

\chapter{Implementing Canary Post Release Testing}
\section{New technologies drive new techniques}
\subsection{Kubernetes is a Cluster OS}
\subsubsection{Resource Management in Kubernetes is made for high available services}
\subsubsection{Deployments implement Rolling Updates}

The extend to the canary releasing technique is a rolling update. It unifies phoenix replacement and canary releasing and extends it to a full deployment. You start with one server in the new version and spawn it automatically. Now it becomes part of the whole pool of servers and serves traffic as the server is ready. Now two different versions are serving traffic, just like with a canary release. But after the first step the deployment goes on and then destroys a server instance in the old version as you would do in the phoenix replacement. Then the procedure will be done multiple times until all servers of the old version are replaced by the new version.

\subsection{DataDog is a Cluster Monitoring Systems as SaaS}
alternativen Graphana, Prometheus
\subsubsection{The main components are: Datacollection, Timeseriesdatabase, Graphing and Alerting}
\subsubsection{DataDog integrates well in Kubernetes}
\section{Deployer is a Service for Continuous Deployment and enables Canary Post Release testing}
\subsection{Deployer integrates into the Continuous Delivery Pipeline}
\subsection{Deployer integrates into Kubernetes and deploys itself}
\subsection{Deployer integrates into Monitoring and enables Canary testing}
\subsection{The main Deployer API features are Deployments and Canary deployments}
depctl, curl
\subsubsection{Deployments deploy a whole repository}
\subsubsection{One Canary per Replicated Pods can be deployed and monitored}
\subsubsection{Immediate Notifications in case of failure}
difference depctl and curl
\subsubsection{Automatic Rollbacks in case defect Canary}
via datadog and triggers deployer
future: staging deploys

% \subsection{bugs + debugging}
% \subsection{ilities}
% \subsection{security}
% \subsection{monitoring}


\part{Evaluation}

\chapter{GapFish is the company to evaluate Deployer}
\section{GapFish's services are complex and highly available}
overview of Gapfish's services
\subsection{GapFish's Operation Service is used by internal Staff and Customers}
operation service == operation.gapfish.com
\section{GapFish uses tools for continuous deployment}
\subsection{GapFish differentiates between development and operation}
how is the deployment tradditionally done
\subsection{Kubernetes enables GapFish to have a development and operation in one team}
which services are migrated to kubernetes

\chapter{Implementation of Metrics: traditional vs new}
\section{Deploys/Week}
\section{Deploy Duration}
\section{LOCS/Deploy}

\chapter{Results}
\section{Metrics: traditional vs. new}
\section{theoretical/practical conclusion for deployer and cd}
\section{for Gapfish}
\section{Lessons learned and future}

\chapter{resume}
