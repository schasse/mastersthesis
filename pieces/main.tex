\chapter{Introduction}
\section{Introduction}

DevOps~\cite{devops_definition} is a often discussed topic and there are multiple
definitions and interpretations of the term DevOps. DevOps is referred to a philosophy, a
culture, practices and specific tools. For my research, I will focus on two different
aspects of the term DevOps:

The first aspect is the perspective of operation teams. Operation teams traditionally
modeled infrastructure by installing physical hardware and by manually installing software
components. With the rise of virtual machines and the cloud, it became possible to model
infrastructure in software~\cite{infra_as_code_platforms}. Modelling via software enables
operation teams to use tools and practices used in software engineering. Infrastructure
code is version controlled, tested and can be automatically deployed.

The other aspect of DevOps is the perspective of developer teams. Previously, developer
teams were only responsible for developing new features. Software engineering practices
got established and proven. One of those practices is the continuous delivery
pipeline~\cite{cd_humble_pipeline}. The last step of the continuous delivery pipeline is
the deployment. Formerly operation teams were responsible for deploying new features. The
deployment as the last step of the continuous delivery pipeline shifts a responsibility
from operation to development. This shows that developer teams are becoming more and more
responsible for running the software they built.

\section{Thesis outline}

In the following outlines the structure of the thesis. We discuss every chapter briefly
and talk about its contents.

In the first chapter we will go through the foundations. The chapter provides references
the practices and technologies, which are crucial to understand the thesis. The references
are properly selected to understand the details in case those are not known. In summary
those practices and technologies are kubernetes, continuous delivery, continuous
deployment and techniques from infrastructure as code and site reliability engineering.

The second chapter is a conceptual macro view to the method nonfunctional production
regression testing. The text goes through components of the whole environment and
discusses the most important concepts and how they communicate with each other. The
explanation of the communication between the components clarifies how the pipeline embeds
the methodologies of nonfunctional production regression testing and how those extend the
pipeline.

In the third chapter we will get to the concrete design of nonfunctional production
regression testing. To enable nonfunctional production regression testing, I wrote the
software deployer.  The chapter explains the details of the design concept and design
decisions. After reading this chapter it is clear how to use the software deployer.

Chapter four is about the evaluation of the new approach. It evaluates the usage of
deployer and the technique in two different companies. The first company is Gapfish, a
four year old startup, and the software department of DIN, a company established for a
hundred years. We are going to evaluate positive outcomes, still problematic concerns and
their improvements. Another part of the evaluation is the comparison to other techniques
which other companies and groups developed and tested. We differentiate in their features,
advantages and disadvantages.

In the last chapter, the conclusion, the whole thesis is summarized and all the chapters
are resumed. Important is the second part of the conclusion, in which we have an outlook
to further improvements and how the technique can be extended to have further upgrades to
delivery pipelines.

\chapter{Problem Definition}


\section{Maintenance responsibility shifts}

% devops/microservices aproach successful development and operation
The DevOps TODO approach and the microservice culture TODO create practices to solve the
issues of the conflicting tension\cite{sre_intro} between a development and operation
team. On the one hand the aim to change the software product quickly and on the other hand
to provide a stable and reliable software product of high quality. The CTO of amazon said
''You build it, you run it` TODO, which creates the awareness of the teams to not only be
responsible for the development and deliver features whereas they are also responsible to
operate their product in production. This is because operation is an essential part of a
successful software product. One team is consequently responsible of the development,
operation and maintenance their own code.

% responsibility of deps shifts from ops to dev
Furthermore the responsibility of software dependencies shifts. In the past, development
teams delivered code and depended on installed software dependencies. Operation teams have
been responsible of maintaining those dependencies. Virtualization made a
change. Operation teams operating a virtual infrastructure operate and maintain the
hardware and software of this infrastructure. They do not maintain the specific
dependencies of the code, which the development team develops. The development team can
choose which dependencies they want to have. Accordingly there is a clear isolation
between the dependencies of the virtual machine infrastructure and the development team.

% contra arg vms operated by ops team
You could argue that virtual machines have still the same functionality like bare metal
servers and need to be maintained in the same way by the same operation
teams. Nevertheless it becomes easier to maintain dynamic virtual
infrastructure\cite{infra_as_code_platforms} compared to bare metal infrastructure. And
the easier it becomes, the more it enables developers to operate their software product
themselves.

Containerization manifests the shift of the responsibility. It is a further step of
isolation. The operating system is able to isolate cpu and memory for every process. Yet
with containerization the operating system can also isolate the disk between processes,
which means it can isolate the installed dependencies for every process. Containerization
techniques even potentiate the ease of managing dependencies. So the responsibility of the
software dependencies shifts resultantly towards the team, which builds and runs the
software product.


\section{Lack of focus on maintenance}

% is risky
If we did not do maintenance properly, there is a risk of depreciating the quality of the
software product. For instance there can be security vulnerabilities. Those can be very
harmful to the company of the software product. If we do not maintain the software
products properly more issues arise, such as bugs or performance problems. On account to
that, maintaining a software product comes with responsibility. If we disregard
maintenance and operations work, the quality of the software product suffers.

% operate and maintaining software means: config, monitoring, incident management
What does it mean to operate and maintain a software product in production? It is all the
work related to running the software product. It includes the configuration of the
servers. It includes monitoring of performance, errors and security issues. And it
includes the handling of incidents. For example there are any issues, you would need to
react to those issues and to make the software product for instance scale or apply
security patches.

% (maintenance is a lot of work and) is easily disregarded in favor of velocity
Agile and continuous practices focus primarily on software development. Those practices
altered software development to deliver features faster into production. Fitzgerald asses
that the focus of current research is primarily on the delivery process. And Fowler argues
velocity kills agility TODO. Focusing on development velocity only has a major
drawback. Operating software in production and maintaining it needs to be done properly to
ensure the quality of a software product. Companies easily disregard the importance of
operation and maintenance of the software product.


\section{Underestimated amount of maintenance work}

% maintenance of the written code itself is a major source of changes
The software written by the development team needs to be maintained as well. As we
illustrated previously, different issues occur in production. For instance performance and
scaling issues or simply bugs. The developers need to refactor the software without
changing the actual functionality. Those changes are mostly invisible or do not alter any
function visible to users. At facebook TODO those invisible changes are more than 50\% of
the total changes.

% dependencies are becoming more
Moreover Software products have a high amount of dependencies. This totally makes sense,
because when programming a software you do not want to do it from scratch. Firstly the
dependencies are different abstraction levels, such as a processor, a programming language
and for instance a programming framework. And secondly the dependencies are modules which
encapsulate a certain functionality. Those are very common functionalities for
applications, for instance authentication of a user. The abstraction as well as the
reusable software modules make it easier and faster for developers to reach their goals.

% many versions in open source
The open source communities change and release their software continuously. Sometimes
those changes consist of new feature, often times just refactorings and every now and then
security patches. This results in releases and new versions.

% almost every software product relies on open source
In the web, there is rarely a software product, which does not depend on open source
software. The most web servers run linux and there are many popular open source
frameworks, which make up the basis of the running software products.

% RedHat/Ubuntu companies provide bundling and update service
Companies like RedHat or Ubuntu provide enterprise support to those open source
projects. Those companies bundle those open source projects, care about the bugs and
security issues and provide tools to upgrade from one version to another easily. As a
result companies with a software product can outsource the maintenance work of the
dependencies. There is still the need though, to integrate those changes to the own
software product.

% hard to upgrade at once
One problem is that it is not always easy to upgrade from one version to
another. Refactorings result in breaking changes and the depending software needs to adapt
to those changes. As a result software such as windows xp is still out in the wild. Due to
the fact that companies are not able to adapt to those major changes at once. However the
very same applies to open source frameworks such as django or ruby on rails.

% microservices create ability to continuously upgrade one part by another
% it is much work though
One reason for the introduction of microservice is the flexibility to update only a small
specific part of the whole software product. With a microservice architecture we are able
to update a single service without having to upgrade the whole product at once. We can
choose the microservice for the update because it is more important. Or we can update one
microservice by another without the major problem of upgrading the whole software product
at once. Consequently this means we need to update more often and the amount work of
updates itself is more.

% compensation by automation instead of more humans
There are several things you can do to keep up with the amount of work. Facebook TODO for
example hires more developers. This is expensive though. A much cheaper solution is
automation. We should automate some of the maintenance work.

\section{Disregard of automation for maintenance work}

% practices exist which connect dev and ops
You could argue that DevOps practices already exist and connect development and operations
including maintenance of a software product. However the focus lies mostly on testing the
quality of a change in a lean fashion and to reduce risk of a change in production TODO
(devops defintion). It is true that those practices help to deliver changes faster to
production and help to detect issues at low risk. Nevertheless monitoring and reacting to
errors requires manual work.

% testing in production is already done
TODO There exist practices, which already enable production testing. For example A/B
testing, where you compare two different versions in production. Or canary releasing, with
which you change only a few replicated parts of the production environment to reduce
risk. There is also shadow releasing, where users are not affected by the change, which
goes into production. Netflix tests its services with the simian army in production to
detect unforeseen events in a controlled environment.

% containerization takes out risk for updates
Containerization takes out the risk of updating a service. Morris describes phoenix
replacement in immutable infrastructure TODO. Kubernetes has the approach of rolling
updates, which implement the same practices. A containerized ecosystem is typically
immutable. Rather than changing a running container, we would create a new container based
on an updated image including the changes to deploy. After the new container was started,
we can kill the old container. Usually an image repository holds every built container
image version. Those saved images allow us to easily rollback to a previous version. Hence
updates are less risky in a containerized immutable infrastructure.

TODO (author) illustrates the value of automation~\cite{sre_automation}. Software
reliability engineers automate tasks to firstly save the valuable human resources and
secondly take risk out of the tasks because of human errors. This is a good approach and
focuses the quality of the software product.

Immutability takes out some of the risks and automation brings value. Combining those two
arguments, there is an obvious need of an automated process for updating services.

% cd provides good automation, but has lack of maintenance focus
There is a lack of truly automatable practices for maintaining a software
product. Continuous delivery brought up good automatable practices to continuously deliver
changes and features into production. It has the lack of good practices though to maintain
a software product.

Continuous delivery does not focus on the maintenance work and the DevOps approach does
not focus on automating the maintenance work as previously examined. We conclude that
there is a need for practices, which continuously support and automate maintenance work.

\section{Contribution of this thesis}

In my masterthesis I want to optimize the software engineering practices in order to
empower developers in their bigger responsibilities. I want to focus on the process of
deployment and enhance the continuous delivery pipeline. To achieve this, I want to
examine operation practices like monitoring and incident management. And then extend the
deployment process by three consecutive steps.

The contribution of this masterthesis is a practice, which can fully be automated for
maintaining a software product. It extends the continuous delivery and continuous
deployment pipeline as those go until release and no further. Nonfunctional production
regression testing extends the pipeline to the production environment, where we need to
operate and maintain a system. The thesis suggests a practice to deploy a change to
production, monitor the change at low risk and in case of a regression automatically roll
back. The thesis provides an exemplary implementation of this practice and evaluates the
practice in a nonproduction test as well as in a production environment of a software
product.

\chapter{Background}

Some background is required to understand the masterthesis. In the following I will not
explain all the practices and technologies. Instead I will just list them and provide well
chosen references to be able to glean description of the prerequisite practices and
technologies. The practices are at least well known in software engineering as well as the
concepts of most of the technologies.

\section{Prerequisite Practices and Technologies}

\subsection{Practices}

\begin{itemize}

\item At first common practices from software engineering. Initially
  version control with a \emph{version control system}. Humble explains what is important in
  version control~\cite{cd_humble_config}.

\item Next the reader should know about continuous integration as Fowler
  propagates~\cite{ci_fowler} and what a \emph{continuous
    integration system} is.

\item I refer to \emph{DevOps}, which Bass et.~al defines~\cite{devops_definition}.

\item The reader should know what \emph{continuous delivery} is and how continuous
  delivery relates to DevOps~\cite{cd_wolff_devops}.

\item Important practices from continuous delivery are prerequisite: \emph{canary
    releasing}, \emph{zero downtime deploys} and \emph{continuous deployments}, which
  Humble illustrates~\cite{cd_humble_deploy}.

\item I require the reader to have an idea of monitoring in a distributed system as
  Ewaschuk describes in ``Site Reliability Engineering''~\cite{sre_monitoring} and how
  Bass et.~al illustrates the design of a \emph{monitoring
    system}~\cite{devops_monitoring}.

\item I presume the reader to know the concepts of dynamic infrastructures and
  infrastructures defined as code\cite{infra_as_code_platforms}. Morris explains how to
  common software engineering practices for
  infrastructure~\cite{infra_as_code_se_practices}.

\item The concepts of cluster management and how Google manages its clusters are
  important~\cite{borg}. Hence those concepts heavily influenced the design of
  \emph{kubernetes}~\cite{borg_kubernetes}.

\end{itemize}

\subsection{Technologies}

\begin{itemize}

\item The next references are practical introductions to the concepts of the technologies,
  which I extensively use in this masterthesis. It is necessary to know what docker
  \emph{image} and \emph{containers}
  are~\cite{docker_orientation}\cite{docker_orientation2}.

\item It helps to have an idea of the architecture of kubernetes. Beda gives a good
  overview~\cite{kubernetes_architecture2}.

\item Absolutely relevant for the next chapters are the kubernetes resources. Before
  reading chapter 4 and 5 the reader should know what \emph{pods}~\cite{pod},
  \emph{deployments} (the new replicationcontroller)~\cite{replicationcontroller} and
  \emph{services}~\cite{service} are.

\end{itemize}


\section{Software Dependencies}

Here I would like to mention in short the dependencies, which I used to implement
deployer. Deployer is implemented in ruby\cite{ruby} and since it is a webserver,
sinatra\cite{sinatra} is it's framework. Deployer uses git\cite{git} and
subversion\cite{svn} to communicate with the either git or subversion as the control
version system. Moreover deployer has a plugin which sends messages to
bugsnag\cite{bugsnag} and slack\cite{slack}, to inform the developer in case it identifies
a problem. Deployer uses kubectl to interact with kubernetes\cite{kubernetes}
master. Deployer itself is containerized with docker\cite{docker} and its natural hosting
solution would be kubernetes. We published deployer on github\cite{deployer} under
GPL-3.0, a free software license.

The monitoring system, which we used in our evaluation, is datadog\cite{datadog}, a
commercial 2service. Nevertheless prometheus\cite{prometheus} is also a suitable open
source solution as monitoring system. The continuous integration systems we use for the
evaluation are codeship\cite{codeship} and jenkins\cite{jenkins}. As an image repo, we use
docker hub\cite{dockerhub}.


\section{Typical 3 Tier Webapp in Kubernetes}

I'll explain how you would implement a typical 3 tier webapp in kubernetes as it helps a
lot to understand the next chapters. You would implement the application tier with a
deployment, which monitors the presence of a specified number of stateless pods. A service
acts as a loadbalancer and selects the pods and forwards the request. The data tier can be
implemented with a statefulset, which monitors the presence of the stateful pods. Those
stateful pods keep their host name and mount the same volumes when they are being
restarted.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0 \textwidth]{3tierwebapp}
  \caption[3tierwebapp]{Typical 3 tier webapp in kuberentes.}
  \label{fig:3tierwebapp}
\end{figure}

So the service receives a request from the client. It selects a pod via round robin and
proxies the request to the pod. The pod probably communicates with the database and sends
the request back to the client, where the loadbalancer acts again as a proxy.

\chapter{Approach}

\section{In Short}

In nonfunctional production regression testing we compare a new version with the current
stable version side by side in production. We test in production if the nonfunctional
metrics show a regression. This is nonfunctional production regression testing in two
sentences.  Nevertheless we will go through the term in more detail in the following. The
approach is a novelty and we name the approach in the context of this thesis.

Nonfunctional refers to the metrics, which we evaluate in a test. They are nonfunctional
and as a consequence generically applicable to multiple applications. The next word,
production, refers to the environment, because we monitor the production application and
collect the metrics from it. Finally the word regression refers to the testing
strategy. The test compares the metrics of the two different versions, a stable version
and a new version. We are create a canary from the new version in the production
environment. We test the canary version for a regression, concretely a decline of the
monitored metrics. If the test identifies a regression, we roll back to the stable
version.

The approach provides some further features, which are not included in the term. Indeed
the testing approach is completely automatable and you can continuously apply it to the
new versions. The approach is designed in respect to failing as fast as possible and
inform developers.

This testing approach naturally evolves from common practices such as continuous
integration, continuous delivery and continuous deployment and extends those
practices. The already established practices support developers before and until the
software deployment. In contrast to that, nonfunctional production regression testing,
supports developers during and after the deployment. In other words it supports the
developers to run applications in production, which formerly has been a business of
operations teams.

\section{Pipeline Overview}

\begin{figure}[htbp]
  \includegraphics[width=0.97\textheight,angle=90]{nprt_overview}
  \caption[nprtflow]{Overview of the NPRT flow.}
  \label{fig:nprt_flow}
\end{figure}

To understand the testing approach in a whole, it is necessary to show a complete overview
of the whole pipeline and environment. Figure \ref{fig:nprt_flow} pictures this
overview. In the following we will go through the steps of the pipeline and discuss them.

\begin{verbatim}
(1) push new version
(2) inform about new version
(3) fetch code
(4) build
(5) push container image(s)
(6) run tests
(7) deploy message
(8) fetch code
(9) modify resource definitions
(10) apply to production cluster
(11) pull container image(s)
(12) send metrics
(13) trigger full rollout or rollback
\end{verbatim}

\subsection{Continuous Integration is a Requirement}

The first parts of the pipeline are commonly known and established practices: continuous
integration, delivery and deployment. It is necessary, though, to touch on them and
integrate them in the whole picture. It will illustrate important design characteristics
for the new testing approach.

First the developer changes some code on his local machine and creates a new version. He
then pushes the new version to the version control system as shown in (1).

After the push of the new version, the second step is a message (2) to the continuous
integration system. This message holds the reference of the new version and the continuous
integration server pulls the new version from the version control system (3). Now the
continuous integration system has three major jobs. Firstly it starts a build process
(4)(5), secondly it runs the tests (6) and the thirdly gives the deploy signal (7).

In step (4), namely the build, the continuous integration system typically compiles
binaries, renders assets and may create further artifacts. For our purposes it is
especially necessary to build at least one or multiple container images. The continuous
integration system then pushes the ready built container image to an image repository in
(5). Later the image repository serves the built images.

One import thing is that we need to associate every built container image with a specific
version. Therefore we use the version reference, which was created by the version control
system. It is important to be able to trace the version through every step in the
pipeline. With this thought in mind, the continuous integration system tags the container
image with the version reference and an extra name. Just to mention that the extra name is
not absolutely necessary to definitely associate the container image with a version. But
it turned out that a human readable name is very helpful to recognize a version and to
know what the version is about at first sight. The continuous integration system derives
that extra name from the branch name. This tag, consisting out of the version and extra
name, will follow us through the whole pipeline as readable and unique reference.

The second continuous integration step is to run the tests. This is shown as step (6) in
figure \ref{fig:nprt_flow}. The tests themselves can be split into multiple stages, such
as unit, feature and smoke tests. Yet we do net need to recall all the details of
automated testing at this point.

The last step of the continuous integration system is to send a deploy signal, step (7) in
the figure. But it depends on the results of the tests, whether to send that deploy signal
or not. The tests can be successful or fail. If the tests fail, the continuous integration
system does not send a deploy message, the pipeline stops and the continuous integration
system may inform the responsible developer. If the tests are successful in all test
stages, the continuous integration system will send the deploy signal to the deployer.

It makes sense to deploy only specific versions and not every commit. The practice which
is pretty common, is that you develop new features in a seperate branch. For those
versions you usually don not send a deploy signal even though the branch build and tests
are successful. Usually after there has been a review and a decision to deploy the changes
to production, even though it is a very small change. But when the decision is made and
merged into a specified branch, for instance the master branch, this version will go to
production.

However, just to clarify, the continuous integration system sends each built image for
every single version to the image repository. This is independent of successful tests or
the intention to go to production. The reason why we want to have every built image in the
repository is to be able to test it in the continuous integration system, locally and a
staging system. But this is just a side note.

So the deploy signal is given when two requirements are fulfilled: the build and tests are
successful and it is a version which is planned to go to production.

Until this point, as we already mentioned, it is continuous integration practice, which is
commonly used in software development. We require it for our approach and again, it is
important to nonfunctional production regression testing to associate and trace every step
through the whole pipeline. For that purpose we need to especially tag the container
images.

\subsection{Customizations of Continuous Deployment}

Nonfunctional production regression testing is an approach, which we designed to be
completely automatable. It is crucial to not only have continuous integration, but to have
a customized continuous deployment process as well.

The next component in the pipeline is the deployer. Deployer is a service, which realizes
the customized parts of the continuous deployment practice. In the context of this thesis,
we implemented the software. We describe deployer in detail in the next chapter. This
chapter demonstrates how deployer integrates in the pipeline and in the environment among
all the other tools. It is crucial to have full control over the whole deployment process
and as a consequence it was necessary to implement the software and have it customizable.

We could also implement the logic of the deploy deploy in the continuous integration
system. But we had to decide against that, because the deploy needs full access to the
production system and the continuous integration system is in our case outsourced to a
third party company. We do not want to give other companies full access to another
company. However this meant, that we had to implement some steps again, which a continuous
integration server already implements.  the continuous integration system pulls the
version from the version control system as well as the deployer. (2 different things: one
application code, 2 infrastructure code. But good to have them both in a single
repository, to have the relation.

Deployer receives the deploy signal from the continuous integration system (7). It again
includes the version reference. Now deployer executes three major steps: firstly pull
infrastructure code (8), secondly modify the infrastructure code (9) and thirdly send the
infrastructure code to the production cluster (10).

So in the first step deployer pulls the code from the version control system (8). The
version control system also holds the files, which describe our infrastructure. We want to
version control the infrastructure definitions of in order to be able to relate the
version of the infrastructure to the version of the code and the version of the
artifacts. In kubernetes those infrastructure definitions are made up of different
resources, which were already mentioned in the background chapter.

In the second step deployer modifies those infrastructure (9) defintions in a way, that
the production system uses the related container image. The running container needs to be
aware of the its version. The modifications of deployer achieve that as well. The latter
is important to later tag the metrics with running version.

The third step of deployer is to apply the modified infrastructure definitions to the
production cluster, which is shown in the figure as step (10). We note that two things are
changed: infrastructure changes as well as application code changes.

The production cluster receives the modified infrastructure definitions. The production
changes the cluster state according to the definitions. Most important is that two
versions in parallel are in the cluster. The production cluster fetches the container
image in step (11) from the image repository. The image, which the continuous integration
system built in step (4). The production cluster is aware of the specific image identified
by the tag, which we described before.

\subsection{Metrics Collection and Comparison}

The figure illustrates the two different versions with two differnt colors. Most of the
running instances are in the stable version (red) and only one instance or in practice few
instances are in the new version (orange). The loadbalancer sends traffic to both versions
and both versions respond to clients, which is not shown in the figure.

The production cluster collects the data, in which we are interested for the regression
tests.  The collected metrics are nonfunctional metrics. We adapted those metrics from the
four golden metrics of google's sre. The metrics are throughput, latency, errorrate and
utilization.

We are interested in the monitoring data of specific versions. Consequently the production
cluster label the metrics with the specific version. This is important, since we want to
compare the metrics of the different version.  The production cluster sends those labeled
metrics to the monitoring system (11).

The monitoring system stores all the metrics of the two different versions in a
timeseriesdatabase. The monitoring system evaluates those metrics by drawing different
graphs and diagrams and comparing those graphs with each other. One example would be that
it draws two graphs for the latency in one diagram. The first latency graph is the one of
the stable version. The second latency graph is the one of the new version. The monitoring
system monitors now those two graphs for a regression. That means in the case of latency,
that if the latency of the new version is much bigger than the latency of the stable
version, the monitoring system detects this as a regression. In other words we would have
a deviation of a monitored metric, which is above a certain threshold. We discuss the
specific implementations of the different metrics and their comparisons in the following
chapter.

\subsection{Rollouts and Rollbacks}

Now there are two different scenarios. The first one would be, that the new version runs
in production for a certain amount time and the monitoring system does not identify any
regression. In this case the monitoring system does nothing. A scheduled job triggers the
full rollout of the new version. This job sends a usual deploy message of the new version
to deployer in case the new version still exists in production and was not rolled back
beforehand. Deployer receives this deploy message, deletes the canary and modifies
infrastructure definitions to have the new version as the new stable version and the
production system proceeds and stops and starts the running instances accordingly.

The other scenario is that the new version turns out to be a regression compared to the
stable version. In that case we want to rollback the new version. Our monitoring system
identifies the regression and it sends a message to deployer, as shown in step (13) in the
figure. Accordingly our test for regression is failed.

Deployer receives the rollback message and sends a deploy to the production system. Just
to be precise, it is actually a deploy message with the commit hash of the stable version,
which the monitoring system is aware about because of the metrics it monitors. Deployer
now modifies the infrastructure definitions similar to step (9). The important thing is
that there must not be instances of the new version anymore. The production system itself
takes care of deleting instances in the new version. Since instances of the stable version
are already running in the production cluster, the production cluster does not touch the
other instances.

\chapter{In Practice}

This chapter discusses deployer. We implemented deployer in order to realize nonfunctional
production regression testing with kubernetes. Deployer receives deploy messages, modifies
kubernetes resources and applies those modified resources to the production cluster. With
deployer you can automatically deploy another version of an application, which runs in
parallel.


\section{Deployer Architecture}

Similar to the previous chapter, we demonstrate the implementation with the help of some
figures. And we are going through all the parts of the figure.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{deployer_architecture}
  \caption[nprtflow]{Detailed Deployer Architecture and Steps.}
  \label{fig:deployer_architecture}
\end{figure}

\begin{verbatim}
(1) deploy request
(2) loadbalance request to pod and authenticate
(3) validate availability of container image(s)
(4) fetch resource definitions from version control
    system
(5) modify resource definitions (image version and
    environment)
(6) apply resource definitions to kubernetes API
(7) eventually report errors
\end{verbatim}

As mentioned earlier, deployer is a webserver, which runs in kubernetes itself and is
stateless. The production cluster in in the figure is the same kubernetes cluster the app
runs in, which we want to test via nonfunctional production regression testing. The yellow
box is a kubernetes service. The service is, as we already know, a loadbalancer and
clients can reach deployer via an http interface. The figure shows us that deployer
consists of multiple pods. The pods are replicated and identical in their behaviour. The
pods are basically stateless, yet every pod has its own caching layer, which we will
discuss in a later section. The statelessness leads to easy horizontal scaling of
deployer. We can just add more pods if needed. Along the way, deployer uses approximately
50mb ram and very few cpu, so computing resources should not be a problem. A deploy takes
about 10 seconds. This is just an orientation. Nevertheless this mostly depends on the
size of the repository and the download rate from the version control system. A request to
deployer is blocking and waits for the deployment process to complete. Certainly we do not
need a queuing system here and can scale deployer to the concurrent deploys we need.


\section{interface}

Requests to deployer are very simple. We can use curl request or any other http client and
execute it for instance from our continuous integration system. Another way to interact
with deployer is to use the depctl command line interface.

Depctl command line interface wraps http calls and assists developers with automatic
completion of the repository to deploy and completion of the version to deploy. With
depctl developers can easily skip steps of the whole nonfunctional production regression
testing flow. For instance they can skip the tests, which would run on the continuous
integration system. This makes development more fluently and developer friendly.

In the following we will go through the interface of deployer. Deployer provides different
endpoints: ls, show, tags, deploy, canary and version.

The \emph{ls} or index endpoint returns the configured services and the show endpoint for a
specific service shows the current configuration for that service.

The \emph{tags} endpoint shows
the available tags for that service. Deployer queries the docker image registry for all
available tags for all images in a service and returns them to the client.

You need to call \emph{deploy} and \emph{canary} on a specific repository and you
need to provide either a version reference, a tag or both. The request updates a service, so
this is why we are using a http put call. An example curl request would be

The \emph{version}
endpoint simply returns the deployer version.

\begin{table}[!htbp]
  \begin{tabular}{ l|l|l }
    Endpoint & \texttt{depctl} command & Parameters \\
    \hline \hline
    \textbf{GET /} & ls & \\
    \textbf{GET /SERVICE} & show & \\
    \textbf{GET /SERVICE/tags} & tags & \\
    \textbf{PUT /SERVICE/deploy} & deploy & commit, tag \\
    \textbf{PUT /SERVICE/canary} & canary & commit, tag \\
    \textbf{GET /version} & version & \\
    \multicolumn{3}{l}{} % empty line
  \end{tabular}

  Examples:

\begin{verbatim}
 curl --data commit=025838f \
    https://auth_token:secret@deployer.company.com/gapfish/deploy

 depctl deploy
\end{verbatim}
\end{table}

\section{Logic and Flow of a Deploy}

\subsection{Authorization}

Next we want to go through the steps involved in the deployment process. Initially a
client sends a http request to deployer (1). The service proxies the request to deployer
(2). After that deployer needs to authenticate the client via http basic
authentication. In other words, the client authenticates itself via a username and
password. As we mentioned earlier, our continuous integration system is a software as a
service solution, and one of the reasons why we could not implement a deploy logic inside
the continuous integration system, were security concerns. The continuous integration
system cannot have full production cluster access. In this case the token authorizes to
only deploy a specific version from the repository. For us that means, that the client is
not able to do everything to the kubernetes api consequently not everything to our whole
production cluster. It is for instance not able to deploy other code than ours or read
credentials from the production cluster.

\subsection{Validations}

After deployer authenticated a valid user, deployer starts the deploy procedure. At first
deployer validates the arguments given by the client. The deploy request requires the
service to deploy and a version of the service. The client needs to specify either the
commit hash or the tag name or both. The tag would additionally include the branch name,
which makes the reference more readable.

Deployer initially validates the arguments of the request. To be specific, it checks if
the service exists in the configuration. Furthermore deployer checks if the commit exists
in the repository and deployer checks if a the tag, which references the version, exists
for all the images, which are necessary in order to deploy the service. Deployer executes
the latter validation by communicating with the docker registry (3). The client does in
contrast not have any validations\footnote{We can use any http client such as simple curl
or the more comfortable depctl.}. The reason for that is that we want to have a central
definition for the validations.

For simplicity and usability the client has the options to either
\begin{itemize}
    \item exclusively give a version reference
    \item or exclusively give a tag
\end{itemize}
because it makes things a lot simpler when a developer needs to deploy
manually. This makes the development flow more efficient. The version reference is totally
sufficient to determine the version, so the clients usually provides the version reference
only.

\subsection{Fetching the Code and Caching it}

After deployer validated the deploy request, it then fetches the code from the version
control system (4). When there is git as the version control system, deployer uses commit
hashes and branches as version reference and to determine the code version. When there is
subversion, deployer uses revision numbers instead. We implemented subversion, which is
more of an ancient technology, to be able to do the evaluation with the DIN system.

As we know, the process of fetching a repository includes persistence and disk
interaction. That is true. Deployer uses its volume just as a cache, though. The reason
why there is a cache at all is of course performance. One of the bottlenecks of a deploy
is to download the repository. If a repository is big, for instance because of images or
it is a repository with a long history of commits, it lasts quite an amount of time to
download it. If the download rate is additionally very low the duration is even
longer. This leads to long running deployments and a bad development flow
experience. Therefore deployer keeps the already downloaded repositories on its volume as
a cache. The next time it deploys the same repository in a different version deployer only
fetches the changes.

As a simplification, every pod has its own cache and lives as long as the pod. So every
pod utilizes its own volume as a cache and there is no need for communication to an extra
caching service. Since docker containers are immutable, every time kubernetes recreates a
pod, kubernetes destroys the volume. Thus the cached data is not available anymore and in
other words the cache is empty again.

The version control system contains not only application code, moreover it contains the
infrastructure code as well. In kubernetes especially these infrastructure definitions are
different resource definitions. These resources are for instance deployments and
statefulsets and so on. These resource definitions should be stored in the 'kubernetes'
directory. This is a convention and deployer assumes that this as the location. If an
application is not able to locate it in that directory or does not want to locate it there
for any reason, the configuration of deployer provides an option to reconfigure this
location.

The configuration of the kubernetes resource path enables two different methodologies in
the microservice approach. The first one is the multiple repository methodology. In that
methodology for every single service or microservice we would have a dedicated repository.
The second methodology is the single monolithic repository, which contains multiple
microservices.

\subsection{Modifications of Resource Definitions}

After having the specific version reference of the repository fetched and checked out,
deployer takes the resources and modifies them in a next step (5). Especially deployments
and statefulsets are relevant to those modifications. In contrast to those resources,
deployer does not modify all other resources, such as services.

Deployer applies two main modifications: the image version and the environment. The latter
is especially important for the metrics collection. Deployer modifies the environment, so
that the stable version and the canary version know about themselves as such. The running
service can later read the version information from environment variables. The pods
receive an according label, so that other applications, which may consume the kubernetes
api, can also distuinguish between the stable and the canary version.

The other important modification is the image version. The image tag specifies the version
of the docker image. Deployer modifies the resource only when the image tag is
unspecified. In case the deployment already specifies a tag, such as 'debian:wheezy',
deployer does not change that tag. The reason is there may be containers, which the
continuous integration system does not build and tag with the specific version
reference. Commonly another party maintains these docker images. One example is an
additional statsd container running as a sidecar in the application pod.

The other case would be that the tag is not specified in the kubernetes resource. Then
deployer appends the tag to the container image according to reference the client provided.

The next step is to communicate the modifications to the kubernetes api (6). Deployer
sends the modified resources to the kubernetes api. The kubernetes master manages the
rollout of the changes. It swaps out one pod by another and replaces the old version with
the new version. The procedure is called rolling update.

\subsection{Errorhandling}

Sometimes something unexpected is happening during the deployment. This can be for example
that a verification of deployer fails, such as deployer does not find the tag
corresponding to the commit. Or maybe kubernetes api server returns an error for any
reason. If something like the described happens, deployer will inform the developers. We
distinguish between two different clients. One is, a continuous integration system
requests the deploy. In that case our error report system\footnote{In technologies
  section, we mention Bugsnag. This is the error report system we use.} come to
play. Deployer raises an error and the error is sends it to the error report system (7),
which collects all the errors of any system. The error report system informs the
developers via sending notifications to a specified channel. We have for example a slack
chat channel, on which the person on call subscribes and receives notifications from.

The other client is a developer sending a deploy request manually. In that case the
developer typically uses depctl. Deployer discriminates usual curl requests from requests
with depctl. Instead of utilizing the error report system, deployer answers the http
request with an errorcode and a message in the http body. As mentioned earlier the
communication with deployer is synchronous.

\section{Testing Architecture}

In this section we look at the testing architecture inside the production cluster. A few
canary pods in the new version run next to the stable version. These pods are able to
receive and respond to production traffic.

\begin{figure}[htbp] \centering \includegraphics[width=\textwidth]{deployer_flow}
  \caption[nprtflow]{Testing Flow Implementation.}
  \label{fig:testing_flow}
\end{figure}


To deploy a canary, the client sends the deploy canary request to a separate http path ,
which is different from the usual deploy and which deployer defines as well. The client
needs to provide the same arguments as for the deploy request. Namely those arguments are
the service, which is required, and either a commit hash, a tag or both. When deployer
receives the canary request, it proceeds almost identical to the deploy request. The steps
are verification, fetching the version, resource modification and application to the
kubernetes api. The difference to the deploy lies in the modification step. Deployer
creates a new canary deployment resource.

In the modification step deployer does not only change the container image version, but it
also changes the name of the deployment, which would then represent another
deployment. Deployer names this new deployment resource with the suffix canary. Since
deployer changes the name, kubernetes does not identify the canary deployment as the
original deployment. Instead it treads the canary deployment as a separate resource and
creates pods for that deployment in the different version, which deployer specified.

The service though, which does our loadbalancing, selects both. It selects the pods which
the original deployment created. And it selects the pods, which the canary deployment
created. Hence the service selects its pods on account of the label and the original and
the canary deployment share some of their labels. An example would be a shared label like
'deploy=webserver' and tow different labels like 'track=stable' respectively
'track=canary'.

As it was mentioned in the previous chapter, we want to have fewer pods in the canary
version than in the stable version. Deploy does this simply by scaling the canary
deployment to only one replica.

\section{Metrics}

\subsection{Metrics Collection}

Now we want to focus on how we collect the metrics during the two versions are in
production. We differentiate between two different collection mechanisms. Metrics
collected from hosts and metrics from applications.

Firstly the group of metrics, which are picked up from the host. The host has information
about the pods' utilization of cpu and memory. So on each kubernetes host, there is a
monitoring agent running, which watches the \verb!/proc! directory and the docker
daemon. The agent picks up the information frequently and then sends it to our monitoring
system.

Secondly the group of metrics, which the application sends. We need to have the
application instrumented in order to collect metrics like throughput, latency and
errorrate. In practice we use the statsd protocol and statsd server for that
purpose. There are statsd libraries for the most languages and frameworks and you get the
instrumentation without much effort. The instrumented application sends the data to the
statsd server after each request and the statsd server aggregates the data. From there the
statsd server forwards the aggregated metrics data to our monitoring system.

It is important to correctly label the metrics independently from which version we collect
the metric. The monitoring system is then able to distinguish between the metrics of the
stable version and the metrics of the canary version. For the first group of metrics
collection, the monitoring agent can pick up the label from the labeled pod. And for the
second group of metrics collection the instrumentation code of the application picks up
the label from environment variables, which have been set by deployer as either stable or
canary.
\subsection{Metrics Comparison}

Another part of nonfunctional production regression testing is the comparison within the
monitoring system.

The monitoring system consists basically out of a timeseriesdatabase, a graphing user
interface and an alarm system. The timeseriesdatabase persists the metrics. And the user
can define graphs from those metrics, which the user interface displays. Moreover you can
define rules in the alarm system, which monitor the metrics in the timeseriesdatabase.
In case the metrics violate any rule, the monitoring system sends a notification.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0 \textwidth]{metric_comparison}
  \caption[metric_comparison]{Comparing the stable version with the canary version.}
  \label{fig:3tierwebapp}
\end{figure}

Let's have a closer look on those rules. The figure shows the example of a rule on the
latency. The red continuous line is the latency of the stable version. The yellow
continuous line is the latency of the canary version. The graph compares them side by
side. Then we have a threshold, which is the red dotted line. The threshold depends on the
latency of the stable version and is in our figure always 50ms higher. In case the latency
of the canary goes above the threshold, the graph would violate the rule and trigger an
alarm. The alarm would trigger a webhook, which would roll back the canary instance.

Next we have a look at the metrics, we want to monitor:

\paragraph{Latency} of successful requests. This is a performance metric. And the
monitoring rule should identify serious regressions. To achieve that, we use median or
other percentiles so that outliers do not trigger a rollback, but only serious
regressions do.

\paragraph{Utilization} refers to the metrics CPU and Memory consumption. We can have the
same aggregation window and aggregation method like we have it with Latency.

\paragraph{Errors} We simply count errors. Best is an application, where no errors
occur. Then we can trigger a rollback of the canary instance in case it raises an
error. Nevertheless most applications are not free of errors. On account to this, we
rollback in case the canary raises a new error, which the application never has raised.

Throughput is not a metric we define a rule on. The version of the canary does not have
any influence on throughput. The production environment determines throughput. Throughput
is relevant for the parameters of the other metrics, though.

\subsection{Metric Parameters for Latency and Utilization Metrics}

The ratio of stable and canary instances as well as the throughput affect the size of the
\emph{aggregation window}. What would we do in case we had a very low throughput of only 1
request per minute and a ratio of 2 stable instances and 1 canary instance? If we had
chosen a small window of 1 minute, we would have had either 1 request on the stable or 1
request on the canary instance. That would not be a good comparison. Instead we choose a
rather big window of 120 minutes. Then we would have 80 requests on the stable instance
and 40 on the canary instance. This is a definitely better comparison. Consequently the
lower the throughput and the lower the stable canary ratio is, the bigger needs the window
to be.

The distribution of different throughputs and different latencies over all the routes of
the application affects which \emph{percentile} to use. I make an example: If we had one
single route which has a very high throughput and a very low latency compared to all other
routes, then we would not choose the median. Instead we would choose the 95th percentile
to well represent the routes with a high latency.

The range of the different throughputs and the different latencies also affects the
\emph{threshold}. If we had a bigger range of throughputs and latencies, we would need to
choose a higher threshold. However we can affect the volatility of the metrics by choosing
a bigger aggregation window. This allows as to lower the throughput.

We see that the higher the throughput is and the distribution is more evenly, the better
will the regression testing work.

The monitoring system we were using in our evaluation, was datadog. But there is also the
opens source monitoring system prometheus, which provides very similar features. Also
google has a similar monitoring system in there internal infrastructure.

\chapter{Evaluation}

We have different possibilities to compare those version. One possibility is, that we
compare current and historical data. For instance to compare the metrics of the current
production system with the metrics of the production system of the day before or even the
week before and compare the different versions of those times.

We are following a different approach, because when we are comparing the current
production system with the production system of last week, we have lots of different
changes. The current traffic must not be the same traffic as last week, the load of the
production system must not be the same load and other system with which the application is
interacting with must not be the same.

That is why we decided to compare two different versions which run in the production
system concurrently. This brings not only the advantage, that you have the very same
traffic, but also the advantage, that there is less risk involved. We illustrate the
advantage of less risk now by demonstrating the process of deploying the second version
and comparing it to the old version.

Ok, if you compare the two versions with each other, you will do it as follows. Deployer
create another deployment resource from the one that already exists. Deployer calls this
other deployment resource canary deployment. The creation of the canary deployment
resource has the effect, that not only pods of version I are running in production, but
there is pods running in version II as well. Similar to the regular deployment, the canary
deployment defines how many pods in which version are supposed to be running.

We want to test, if there is a regression respectively a degradation between the two
versions. On account to the fact, that a regression is possible and when introducing
change, a regression is very likely, we at least want to affect as little users. So what
do we do for that? In our example there three pods running in version I and only one pod
in version II. This is a ratio of three to one and due to the fact that the loadbalancer
uses round robin as the scheduling algorithm, only one in four requests, so 25\% of the
total traffic is sent to the pods in version II, which is to test.

This certainly lowers the risk of failure and that users are affected by a
regression. Even if the request of specific single user hits the degraded pod, the next
request of the same user has the probability 75\% to hit the old stable version.

A limitation to this technique is that the new version II needs to be able to run side by
side with the old version I. In most cases, that means that the new version needs to be
semantically almost identical to the old version. So version II should not provide
functional changes compared to version I, but only nonfunctional changes. However that
means we cannot test new feature like in an A/B test. Instead we can test performance
improvements, refactoring or updates.

They call this technique canary releasing. Again, you change would only change a part of
the production system, the canary instead of the whole. Devops TODO examines this
technique in more detail.
% TODO: difference to canary testing

Assuming we would want to test features in production, the current implementation of the
technique is not suitable. If we wanted to do that, we would need to include the
loadbalancer. The loadbalancer would need to remember which user is proxied to which
version, so that the next request of that user goes to the same version, thus the user
sees the same set of features as before. The design of the database could potentially be
affected as well and could be needed to be loadbalanced for the users. The technique we
just described is usually called an A/B test. The disadvantage of the A/B test is that the
same user will hit on the same potentially degraded service and it is not that simple to
automatically provide a stable service to the user. Due to simplification, we did decide
to not include the implementation of the loadbalancing.

We want to state that it is suboptimal to run multiple versions in the cluster like also
mentioned in devops TODO. Rolling updates require it to be able to have two versions in
production, though. And kubernetes utilizes rolling updates as a technique to provide zero
downtime deployments. Accordingly our proposed technique does not introduce a worsening to
that. But as in devops mentioned, you should avoid to run more than two versions at the
same time in production. Deployer ensures that by either updating a deployment, creating a
canary deployment, or creating a deployment in a new version, just before it deleted the
canary deployment.

Especially to test the latter, security updates, is absolutely appealing, since we can
fully automate the procedure of updating the dependencies of our application in a fully
automated and in a way, which would have a very low risk. We could have a job, which
checks frequently for any new version, pushes the updates to the version control system,
the continous integration system runs the pre deploy tests, deployer deploys the update
and even in production we check the update for an regression. We could save a lot of
developer time, who would usually need to take care of the whole updating procedure.

And even if there is a degradation in production, a small amount of requests is affected,
because we send only a reasonable amount of traffic, which arrives at the same time, to
the potentially degraded version. Further more we limit the time the degraded version is
in production, because we automate detection of the degradation and the rollback to the
old stable version.


We let this running for a specified time in production. We need to decide on how long we
want to compare the versions. That depends on how much traffic is in production, because
when we would few traffic in production, we wanted to compare for a longer time. We
suggest to have a well balanced test scenario in terms of load. The time depends on how
much traffic there is in production and how often a team wants to deploy its
application. A team which is working with a monolithic application has the disadvantage,
that every change in every part of the software causes a deploy and deploys are more
frequent. This limits the time in production for the canary. Instead if we have a
microservice environment, the deploy affects only a specific service, hence little part of
the whole application. As a result deploys are less frequent and we have more time for the
canary in production.

We do not need to generate the test traffic, we do not need to weight traffic and we do
not need to think about edge cases. These are all advantages, that we get for free from
the production traffic. We save time and work, because the users generate the test data,
instead of us.

The users create more requests and with that test data for parts of the application, which
are more important. Consequently the users reasonably weight the test data. And lastly the
longer we run the comparison in production, users will produce more of those edge cases,
which would be hard to make up.

We are aware of that the two compared versions do not receive the very same
requests. Hence the comparison is not perfect. In future work we could extend the
technique to achieve that.

We could simply clone the requests, send the original request to the stable version and
send a cloned request to the canary. The loadbalancer could then differentiate between the
two responses of the two versions. We would reject the response of the canary. And we
would forward the response of the stable version.

As a result we even lower the risk, because the potentially degraded version does not even
respond to real users. Ergo we do not have any risk of a degradation of our production
service which we cause by testing the new version.

\chapter{Conclusion}
\section{Resume}

We illustrated that we are able to automate the whole testing procedure, which is the
advantage of nonfunctional production regression testing. We can extend the continuous
delivery pipeline in a natural way and support developers not only until the deploy,
furthermore we automated a part of the developer's job during run time. The pipeline is
now advanced in a way, that we can change application code and test it without the
supervision of the developers at a very low risk. The change goes through the whole
pipeline including tests in a testing environment, an automated deploy and tests in the
production environment. If the nonfunctional production regression test and automated
deploy is successful, developers will be completely free of work. Potential changes of
bots would be robust enough to be able to act fully automated and unsupervised. However we
still test the changes they make and can be sure to not have a regression in our
production system.

Finally we want to summarize, what we examined so far: The whole process of nonfunctional
production regression testing is fully automatable, every step is determined and traceable
throughout and even reproducible until the deploy on the basis of the version
reference. We save time not only by automization, but also by not having to write load and
integration tests with edge cases, which occur in production. And ultimately the amount of
work and time saved comes at a low risk.



Finally in this chapter, we summarize what we have discussed so far. We went through the
infrastructure of deployer, we saw, that deployer scales horizontally. We have seen that
deployer has simple interface, with the two most important commands deploy and canary. We
went through a canary deploy and saw that deployer validates the given arguments and how
it notifies developers if an error occurs. We discussed how and what modificactions
deployer does to the kubernetes resources. We demonstrated the different test metrics and
how the production system sends those metrics to the test system. We went briefly through
the deployer interface and which commands are all provided. We had a look on the
monitoring system and the tests itself and how we rollback the canary in case the test
fails.

Another nice thing to mention about deployer, is that it deploys itself, which fits to the
declarative model and recursion. That means we develop deployer itself with the continuous
deployment flow and can apply nonfunctional production regression testing to deployer.



\section{Outlook and future work}

% \part{Foundations}

% \chapter{Introduction: Developer and operation teams converge and both use software
% engineering practices}

% \chapter{Developers use the Continuous Delivery Pipeline}


% \section{Monitoring to identify Problems}
% \subsection{Health checks measure availability}
% \subsection{Measuring Latency, Traffic, Errors and Saturation identifies failures and
% performance problems}
% \subsection{Incident Management (/Notifications) for appropriate and fast actions in
% case of Problems}

% \chapter{Metrics measure team efficiency and software quality}
% \section{Velocity and cycletime are efficiency metrics for an agile team}
% cycle time measures quality of delivery engine
% \subsection{Deploys/Week indirectly measures velocity}
% and it measures the effect of a quality delivery engine we want many deploys per week
% \subsection{Deploy Duration is import for cycle time}
% \section{MTTR and Failurerate measure the quality of a software}
% we want low risk per deploy to achieve MTTR and low Failure Rate
% \subsection{LOCS/Deploy indirectly measures the risk per deploy}


% \part{New Practices}

% \chapter{Post Release Testing extends the Continuous Delivery Pipeline to support
% maintaining a system}

% The three main steps in the continuous delivery pipeline are: commitment to version
% control, continuous integration and release. After releasing, the new version will be
% operated. This includes monitoring, logging, security aspects and incident
% management\footnote{In ``Site Reliability Engineering'' monitoring and notifying
% principles~\cite[p. 55-63]{site_reliability} are well described.}. To enable developers
% to take more responsible in running the software, it is necessary to extend the
% practices of the continuous delivery pipeline.

% In my masterthesis I want to optimize the software engineering practices in order to
% empower developers in their bigger responsibilities. I want to focus on the process of
% deployment and enhance the continuous delivery pipeline. To achieve this, I want to
% examine operation practices like monitoring and incident management. And then extend the
% deployment process by three consecutive steps.

% The first step is to completely automate deployments of software features and achieve
% continuous deployment. The next step is to sensibly monitor new releases to give
% automatically feedback. Third and lastly, identify incidents to automatically trigger
% rollbacks to self-heal the software system. I'm going to call these last two steps
% continuous post release testing.

% \section{Post Release Testing leads to lower time to market}
% cycle time
% \section{It makes Releases consistent, measurable, fast and scalable}
% mttr, automation/automatic discussion
% \section{It is a new opportunity for risk management}
% identify test before release is a mttr of zero, after release still fast. easier to test
% in production (complexity of system)
% \section{Companies are already post release testing their software systems}
% \subsection{Netflix uses Simian Army to live test their systems}
% \subsection{Synthetic Monitoring tests a complex distributed system}
% \section{Post Release Testing with Canaries is appropriate for testing non change}
% ErrorRate as monitoring measure for automation problems in error rate measure defect and
% failure solution a secific heuristic
% \subsection{Black-Box monitoring is only one part and monitoring change is difficult}
% \subsection{Canary testing is important for maintenance but not feature deploys}
% \subsection{Continuous Delivery is a requirement}
% \subsection{Notifications in case a canary behaves different}
% \subsection{Automated Rollbacks for a automatic self healing system}

% \chapter{Implementing Canary Post Release Testing}
% \section{New technologies drive new techniques}
% \subsection{Kubernetes is a Cluster OS}
% \subsubsection{Resource Management in Kubernetes is made for high available services}
% \subsubsection{Deployments implement Rolling Updates}

% The extend to the canary releasing technique is a rolling update. It unifies phoenix
% replacement and canary releasing and extends it to a full deployment. You start with one
% server in the new version and spawn it automatically. Now it becomes part of the whole
% pool of servers and serves traffic as the server is ready. Now two different versions
% are serving traffic, just like with a canary release. But after the first step the
% deployment goes on and then destroys a server instance in the old version as you would
% do in the phoenix replacement. Then the procedure will be done multiple times until all
% servers of the old version are replaced by the new version.

% \subsection{DataDog is a Cluster Monitoring Systems as a sevice}
% alternativen Graphana, Prometheus
% \subsubsection{The main components are: Datacollection, Timeseriesdatabase, Graphing and
% Alerting}
% \subsubsection{DataDog integrates well in Kubernetes}
% \section{Deployer is a Service for Continuous Deployment and enables Canary Post Release
% testing}
% \subsection{Deployer integrates into the Continuous Delivery Pipeline}
% \subsection{Deployer integrates into Kubernetes and deploys itself}
% \subsection{Deployer integrates into Monitoring and enables Canary testing}
% \subsection{The main Deployer API features are Deployments and Canary deployments}
% depctl, curl
% \subsubsection{Deployments deploy a whole repository}
% \subsubsection{One Canary per Replicated Pods can be deployed and monitored}
% \subsubsection{Immediate Notifications in case of failure}
% difference depctl and curl
% \subsubsection{Automatic Rollbacks in case defect Canary}
% via datadog and triggers deployer future: staging deploys

% % \subsection{bugs + debugging}
% % \subsection{ilities}
% % \subsection{security}
% % \subsection{monitoring}


% \part{Evaluation}

% \chapter{GapFish is the company to evaluate Deployer}
% \section{GapFish's services are complex and highly available}
% overview of Gapfish's services
% \subsection{GapFish's Operation Service is used by internal Staff and Customers}
% operation service == operation.gapfish.com
% \section{GapFish uses tools for continuous deployment}
% \subsection{GapFish differentiates between development and operation}
% how is the deployment tradditionally done
% \subsection{Kubernetes enables GapFish to have development and operations in one team}
% which services are migrated to kubernetes

% \chapter{Metrics are implemented as logging output}
% \section{Deploys/Week}
% \section{Deploy Duration}
% \section{LOCS/Deploy}

% \chapter{Results}
% \section{Metrics: traditional vs. new approach}
% \section{theoretical/practical conclusion for deployer and cd}
% \section{for Gapfish}
% \section{Lessons learned and future}

% \chapter{resume}
