\chapter{Introduction}
\section{Introduction}

Many people talk about DevOps as well as there are multiple definitions and
interpretations of the term DevOps. DevOps is referred as a philosophy, a culture,
practices and specific tools. For my research, I will focus on two different aspects of
the term DevOps:

The first one is the perspective of operation teams. Operation teams traditionally modeled
infrastructure by installing physical hardware and by manually installing software
components. With the rise of virtual machines and the cloud, it became possible to model
infrastructure in software\footnote{``Infrastructure as Code'' describes different dynamic
  infrastructure types~\cite[p. 30]{infra_as_code} and how to model those by
  code~\cite[p. 42]{infra_as_code}.}. Modelling via software enables operation teams to
use tools and practices\footnote{In the chapter ``Software Engineering Practices for
  Infrastrucure''~\cite[p. 179-194]{infra_as_code} practices like version controlling,
  continuous integration are described.} as seen in software engineering. Infrastructure
code is version controlled, tested and can be automatically deployed.

The other aspect of DevOps\footnote{The book ``DevOps''~\cite{devops} is written in the
  view of a developer running a system.} is the perspective of developer teams. Previously
developer teams were only responsible for developing new features. Software engineering
practices got established and proven. One of those practices is the continuous delivery
pipeline\footnote{For theoretical details on the continuous delivery pipeline read Part II
  of ``Continuous Delivery''~\cite[p. 103-140]{continuous_delivery} or a more practical
  approach by Wolff~\cite{continuous_delivery2}.}. The last step of the continuous
delivery pipeline is the deployment. Formerly operation teams were responsible for
deploying new features. The deployment as last step of the continuous delivery pipeline
shifts a responsibility from operation to development. This shows that developer teams are
becoming more and more responsible for running the software, they built.

\section{Thesis outline}

In the following the structure of the thesis is outlined. Every chapter is briefly
discussed, what it is about.

In the first chapter we will walk through the foundations. The chapter mentions
technologies, which are used for the thesis and gives references. Furthermore it gives
references to the practices which are used and are crucial for the thesis. The references
are properly selected, to understand the details if they are not known and to understand
what the technologies and techniques are used for. In summary those are kubernetes,
continuous delivery, continuous deployment and techniques from infrastructure as code and
site reliability engineering.

The second chapter is conceptual macro view to the technique nonfunction production
regression testing. The text walks through the general environment and discusses the most
important concepts and how they communicate with each other. The most important steps of
the continous delivery pipeline are discussed and it explains how the pipeline is extended
in order to have the technique of nonfunctional production regression testing. The text
argues how the methodologies of nonfunctional production are embedded in the pipeline and
how the pipeline must be extended.

In the third chapter we will get to the concrete implementation of the nonfunctional
production regression testing. The chapter will go into the details of how the concept is
implemented. Concretely the software deployer is described, which was implemented in the
context of this thesis.

Chapter four is about the evaluation of the new approach. We will investigate the use of
the technique and customized software in two different companies. The first company is
Gapfish, a four year old startup, and the software department of DIN, a company which is
established for a hundred years. We are going to evaluate positive outcomes, still
problematic concerns and their improvements. Another part of the evaluation is the
comparison to other techniques which other companies and groups developed and tested. We
differentiate in their features, advantages and disadvantages.

In the last chapter, the conclusion, the whole thesis is summarized and all the chapters
are resumed. Important is the second part of the conclusion, in which we have an outlook
to further improvements and how the technique can be extended to have further upgrades to
delivery pipelines.

\chapter{Background}

\chapter{The Technique: Nonfunctional Production Regression Testing}

\subsubsection{the term}
Nonfunctional Production Regression Testing is the topic of the masterthesis and we will
discuss the core of it in this chapter. The term itself describes precisely what the
technique and what the new practice is about. Therefore we will go shortly through every
single term in the following.

Nonfunctional refers to the metrics, which we're evaluating in the test; These metrics are
only nonfunctional and as a consequence generically applicable to multiple
applications. The term production refers to the environment. The metrics, which are
collected are collected in the production environment. And finally the term regression
refering to the testing strategy. The metrics will be compared between two different
versions and the latter version is tested for a regression, concretely a decline of the
monitored metrics.

\subsubsection{features not defined by the term: automatable, conintuous, deterministic, informative}
The testing technique provides some further features, which are not included in the
designation. Indeed the testing technique is completely automatable and you can
continuously apply it to the new versions. The testing technique is designed in respect to
failing as fast as possible and inform developers.

\subsubsection{evolved from cd, but supports in prod}
When it comes to automation and continuous, the thoughtful reader is now probably reminded
of continuous integration, delivery and deployment. This testing techniques evolved
naturally from those practices and extends those. Those already established practices
support the developers until the software is deployed. In contrast to that, nonfunctional
production regression testing, supports the developers during and after the deploy and
while the software runs in production.

\subsection{going through the steps: the whole picture}
To understand the testing procedure in a whole and completely, it is necessary to show a
complete overview of the whole testing and production environment. We will go through the
steps of the pipeline and discuss it in a nutshell as you can see in the figures.

The steps in the very beginning are known from the established practices continuous
delivery and deployment respectively. But it is necessary to touch them and integrate them
to the whole picture and outline the special characteristics for the new testing
technique.

\subsubsection{the developer commits code to a vcs}
At the very beginning, there is a developer, who changed the code locally on his working
machine. There is also a version control system. And in the first step the developer
commits the code to the version control system. The main focus is on git. Subversion is
possible, too though. In the following description we will stick to the terms and notions
of git.

\subsubsection{ci: build, test, deploy}
Next there is a continuous integration system. After the commit happened, the second step
is a message to the continuous integration system. The message holds the information that
a new commit exists and the continuous integration server clones the code from the version
control system and checks out the specified version. Now the continuous integration system
has three major jobs. The first one is to start a build process, the second is to run the
tests and the third is to give the deploy signal.

\subsubsection{build}
In step four, namely running the build, it is typical to compile binaries, render assets
and further artifacts. For our purposes it is especially necessary to build at least one
or multiple docker images.

\subsubsection{importance of commit hash}
The import thing about this is, that we need to identify every docker image to a specific
build. Therefore we use the commit hash, the version created by the version control
system. This commit hash will follow us through the whole pipeline. This is important to
be able to trace every step in the pipeline for a specific version. With this thought in
mind, the docker image is tagged with the commit hash of this version and the name of the
branch. Along the way it is mentioned, that the branch name is not absolutely necessary to
definitely determine the version. The branch name is included for better readability for
the developer and approximately recognize what the image version is about.

\subsubsection{push image}
The continuous integration server then pushes the ready build docker image to an image
registry, such as docker hub. Nevertheless this can be a private registry as well. This
registry serves later as an artifact repository.

\subsubsection{test}
The second continuous integration step, or in total step six, the continuous integration
server runs the tests. There can be multiple stages, such as unit, feature or smoke
tests. Yet we do net need to recall all the details here.

\subsubsection{deploy signal: green tests}
The last step of the continuous integration system is to send a deploy signal. In the
shown figure this is step seven. When you look at the tests, the result could on the one
hand be a failure or on the other hand be successful. If the tests have failed, the
remaining pipeline will be cancelled and the developer will be informed. Just as you know
it from a typical continuous integration system. If the tests have been successful and
accordingly the build including all test stages have been successful, the continuous
integration system sends the signal to deploy.

\subsubsection{deploy signal: branch}
It makes sense to deploy only specific versions and not every commit. The practice which
is pretty common, is that you develop new features in a seperate branch. For those version
it is common to not send a deploy signal even though the branch build and tests are
successful. Usually after there has been a review and a decision to deploy the changes to
production, even though it is a very small change. But when the decision is made and
merged into a specified branch, for instance the master branch, this version will go to
production.

\subsubsection{image build: independent from green tests and branch}
However, just to clarify, each built image for every single version is sent, independently
of successful tests and independently of the intention to go to production, to the image
repository. The reason could be a staging system and even running the tests inside the
build image. But this is just a side note.

\subsubsection{until here cd}
So the deploy signal is given when two requirements are fulfilled: the build and tests are
successful and it is a version which is planned to go to production.

Until this point, as it was already mentioned, it is just a usual continuous delivery or
deployment pipeline, which is commonly used in the development process. But since
nonfunctional production regression testing is a technique, which is supposed to be
completely automated, such a prior described delivery pipeline including automated
deployments is precondition. From now on it is becoming interesting, hence the testing
technique supports the developers post deployment in production instead of the old
practices before the deploy.

\subsubsection{deployer is customized}
The next unit is the deployer. It is the software, which is particularly implemented for
this masterthesis. In the next chapter the deployer is described in detail. This chapter
demonstrates how the deployer is embedded in the pipeline or in other words in the
environment amongst all other tools. In the meantime tools exist, which have a similar
purpose. It is crucial to have full control over the whole deployment process and as a
consequence it was necessary to implement the software and have it customizable.

\subsubsection{why deployer is implemented as a service instead in ci}
We could also implement the logic of the deploy deploy in the continuous integration
system. But we had to decide against that, because the deploy needs full access to the
production system and the continuous integration system is in our case outsourced to a
third party company. We don not want to give other companies full access to another
company. However this meant, that we had to implement some steps again, which a continuous
integration server already implements.

\subsubsection{commit hash identifies infrastructure definitions, deployer fetches code}
The deploy message, which the continuous integration system sends to the deployer,
includes the commit hash again. And again we use the commit hash to identify the
version. Now the deployer executes three major steps: In the first step the deployer
fetches the code from the version control system. This is the same thing, which the
continuous integration system does. The repository is necessary, because it holds the
files which describe our infrastructure. We want to version control the definitions of our
infrastructure in order to be able to relate the version of the infrastructure to the
version of the code and the version of the artifacts. At kubernetes those infrastructure
definitions are made up of different resources, which were already mentioned in the
background chapter.

\subsubsection{deployer modifies infrastructure definitions}
In the second step deployer modifies those infrastructure resources. The infrastructure
should use the related docker image and be aware of its own version.

\subsubsection{send infrastructure definitions to k8s}
And the third step or the ninth step in the figure, deployer applies the modified
resources to the production system, which is shown in the figure as well. We note that not
only application code changes are deployed, but also infrastructure changes. We not that
we deploy the infrastructure changes in a continuous fashion as well.

\subsubsection{production system holds a 3 tier webapp}
Next we go on with the process of how the production system updates itself. The production
system is a kubernetes cluster, which the previous chapter foundations describes. For now,
we assume, that the production system runs a typical three tier webapp. The webapp is made
up out of a loadbalancer, multiple stateless webservers and a stateful database. The
loadbalancer balances the requests between the webservers. And the webservers communicate
with the database. And the database consists of a replicated cluster.

\subsubsection{three tier webapp in kubernetes}
We explain firmly how the typical three tier webapp is implemented in kubernetes. For more
details refer to the references given in the foundations chapter. With kubernetes we
define the metnioned elements with the kubernetes resources: service, deployment and its
pods and statefulset. For simplification, we imagine the service as a loadbalancer. Then
there is a deployment, which manages the existence of the identical pods. And lastly the
statefulset, which manages the stateful pods, with their unique name and disk.

\subsubsection{loadbalancing in kubernetes}
So the loadbalancer receives a request from the client. The loadbalancer selects a pod via
round robin and proxies the request to the pod. The pod probably communicates with the
database and sends the request back to the client, where the loadbalancer acts again as
a proxy.

\subsubsection{deployments are focus}
We now look at the change, which happens to the production system. Earlier, deployer made
changes to the definitions of the kubernetes resources in the deployers memory and
communicates those changes to the production system. We are interested in particular in
the deployments. As an illustration we only look at what happens to the deployment and the
pods.

\subsubsection{deployments are getting rolling update}
The deployer talks to the kubernetes master api and sends the changes. The master manages
the concrete changes. It swaps out one pod by another by stoping the pods in the old
version and starting the pods in the new version. The procedure is called rolling
update. Now the pods are all swapped out and run in the new version.

\subsubsection{four metrics picked up by monitoring agents}
Another part of the cluster are the monitoring agents. They pick up monitoring metrics in
different ways. They collect the data, we are interested in, for the regression test in
production, as well. We are talking about the nonfunctional metrics. We selected the
metrics, defined by the four golden metrics of google's sre (identified). The metrics are
throughput, latency, errorrate and utilization.

\subsubsection{monitoring agents on hosts}
With utilization we are lucky, because kubernetes already implements a collection of cpu
and memory. But we need to instrument the application to collect the other metrics,
throughput, latency and errorrate. So the pods send the instrumentation data to the
monitoring agent. The monitoring agent is an statsd server, which collection the data and
aggregates the data and forwards it to the monitoring system.

\subsubsection{monitoring system structure}
The monitoring system consists basically out of a timeseriesdatabase, a graphing user
interface and a alarm system. The timeseriesdatabase persists the metrics. And the user
can define graphs from those metrics, which the user interface presents visually. You can
define rules in the alarm system, which monitor the metrics in the timeseriesdatabase and
then, in case the rule is violated, sends notifications.

\subsubsection{commit hash is in the label of the metrics }
We are interested in the monitoring data of specific versions. Consequently the monitoring
agents needs to send the monitoring labeled with the specific version to the monitoring
system. This is important, since we want to compare the metrics of the different version.

\subsubsection{possibility: comparing historical}
We have different possibilities to compare those version. One possibility is, that we
compare current and historical data. For instance to compare the metrics of the current
production system with the metrics of the production system of the day before or even the
week before and compare the different versions of those times.

\subsubsection{comparing historical data has the disadvantage of different environment and
different requests}
We are following a different approach, because when we are comparing the current
production system with the production system of last week, we have lots of different
changes. The current traffic must not be the same traffic as last week, the load of the
production system must not be the same load and other system with which the application is
interacting with must not be the same.

\subsubsection{comparing same traffic is better}
That is why we decided to compare two different versions which run in the production
system concurrently. This brings not only the advantage, that you have the very same
traffic, but also the advantage, that there is less risk involved. We illustrate the
advantage of less risk now by demonstrating the process of deploying the second version
and comparing it to the old version.

\subsubsection{how deployer deploys 2 different versions: canary}
Ok, if you compare the two versions with each other, you will do it as follows. Deployer
create another deployment resource from the one that already exists. Deployer calls this
other deployment resource canary deployment. The creation of the canary deployment
resource has the effect, that not only pods of version I are running in production, but
there is pods running in version II as well. Similar to the regular deployment, the canary
deployment defines how many pods in which version are supposed to be running.

\subsubsection{low risk because small amount of traffic in new version}
We want to test, if there is a regression respectively a degradation between the two
versions. On account to the fact, that a regression is possible and when introducing
change, a regression is very likely, we at least want to affect as little users. So what
do we do for that? In our example there three pods running in version I and only one pod
in version II. This is a ratio of three to one and due to the fact that the loadbalancer
uses round robin as the scheduling algorithm, only one in four requests, so 25\% of the
total traffic is sent to the pods in version II, which is to test.

This certainly lowers the risk of failure and that users are affected by a
regression. Even if the request of specific single user hits the degraded pod, the next
request of the same user has the probability 75\% to hit the old stable version.

\subsubsection{limitation: versions must be semantically identical}
A limitation to this technique is that the new version II needs to be able to run side by
side with the old version I. In most cases, that means that the new version needs to be
semantically almost identical to the old version. So version II should not provide
functional changes compared to version I, but only nonfunctional changes. However that
means we cannot test new feature like in an A/B test. Instead we can test performance
improvements, refactoring or updates.

\subsubsection{usually known as canary testing}
They call this technique canary releasing. Again, you change would only change a part of
the production system, the canary instead of the whole. Devops TODO examines this
technique in more detail.
% TODO: difference to canary testing

\subsubsection{difference to a/b testing features}
Assuming we would want to test features in production, the current implementation of the
technique is not suitable. If we wanted to do that, we would need to include the
loadbalancer. The loadbalancer would need to remember which user is proxied to which
version, so that the next request of that user goes to the same version, thus the user
sees the same set of features as before. The design of the database could potentially be
affected as well and could be needed to be loadbalanced for the users. The technique we
just described is usually called an A/B test. The disadvantage of the A/B test is that the
same user will hit on the same potentially degraded service and it is not that simple to
automatically provide a stable service to the user. Due to simplification, we did decide
to not include the implementation of the loadbalancing.

\subsubsection{2 versions in production is common and unproblematic}
We want to state that it is suboptimal to run multiple versions in the cluster like also
mentioned in devops TODO. Rolling updates require it to be able to have two versions in
production, though. And kubernetes utilizes rolling updates as a technique to provide zero
downtime deployments. Accordingly our proposed technique does not introduce a worsening to
that. But as in devops mentioned, you should avoid to run more than two versions at the
same time in production. Deployer ensures that by either updating a deployment, creating a
canary deployment, or creating a deployment in a new version, just before it deleted the
canary deployment.

\subsubsection{automatic deployment and testing of updates saves time}
Especially to test the latter, security updates, is absolutely appealing, since we can
fully automate the procedure of updating the dependencies of our application in a fully
automated and in a way, which would have a very low risk. We could have a job, which
checks frequently for any new version, pushes the updates to the version control system,
the continous integration system runs the pre deploy tests, deployer deploys the update
and even in production we check the update for an regression. We could save a lot of
developer time, who would usually need to take care of the whole updating procedure.

\subsubsection{lower risk: regression triggers automatic rollback}
And even if there is a degradation in production, a small amount of requests is affected,
because we send only a reasonable amount of traffic, which arrives at the same time, to
the potentially degraded version. Further more we limit the time the degraded version is
in production, because we automate detection of the degradation and the rollback to the
old stable version.

\subsubsection{compare two graphs}
Now version I and version II send metrics via the monitoring agent to the monitoring
system. We tag the metrics with the specific version as well. The monitoring system stores
the two comparable metrics of the two different versions in the timeseriesdatabase. Now
you can define a graph to the metrics of version I and you can, too, define a graph of the
metrics in version II. We then compare the two graphs by for instance substracting one
from the other and monitor the result.

\subsubsection{time of comparison}
We let this running for a specified time in production. We need to decide on how long we
want to compare the versions. That depends on how much traffic is in production, because
when we would few traffic in production, we wanted to compare for a longer time. We
suggest to have a well balanced test scenario in terms of load. The time depends on how
much traffic there is in production and how often a team wants to deploy its
application. A team which is working with a monolithic application has the disadvantage,
that every change in every part of the software causes a deploy and deploys are more
frequent. This limits the time in production for the canary. Instead if we have a
microservice environment, the deploy affects only a specific service, hence little part of
the whole application. As a result deploys are less frequent and we have more time for the
canary in production.

\subsubsection{advantage: no need to write a test, which is good}
We do not need to generate the test traffic, we do not need to weight traffic and we do
not need to think about edge cases. These are all advantages, that we get for free from
the production traffic. We save time and work, because the users generate the test data,
instead of us.

\subsubsection{advantage: test truely reflects production}
The users create more requests and with that test data for parts of the application, which
are more important. Consequently the users reasonably weight the test data. And lastly the
longer we run the comparison in production, users will produce more of those edge cases,
which would be hard to make up.

\subsubsection{comparison not totally perfect: different prod traffic}
We are aware of that the two compared versions do not receive the very same
requests. Hence the comparison is not perfect. In future work we could extend the
technique to achieve that.

\subsubsection{alternative: cloning requests}
We could simply clone the requests, send the original request to the stable version and
send a cloned request to the canary. The loadbalancer could then differentiate between the
two responses of the two versions. We would reject the response of the canary. And we
would forward the response of the stable version.

\subsubsection{cloning requests advantage: no risk of degredation}
As a result we even lower the risk, because the potentially degraded version does not even
respond to real users. Ergo we do not have any risk of a degradation of our production
service which we cause by testing the new version.

\subsubsection{no regression happening: normal deploy}
Now there are two different scenarios. The first one would be, that the canary runs in
production for a certain amount time and the monitoring system does not identify any
regression. Afterwards the monitoring system triggers the full rollout of the new
version. It sends a deploy message of the new version to deployer, and deployer deletes
the canary and modifies the other deployment to have the new version and the production
system proceeds and stops and starts the pods accordingly.

\subsubsection{regression happening: rollback}
The other scenario is that the new version turns out to be a regression to the old
version, or in other words, that we have a deviation, which is above a certain threshold
and which is regression instead of an improvement. Then we want to rollback the
canary. Our monitoring system identifies the deviation and it sends a message to deployer,
as shown in step 12 of the figure. Accordingly our test for regression is failed.

\subsubsection{deployer receives request and applies to production}
Deployer receives the rollback message and sends a deploy to the production system. Just
to be precise, it is actually a deploy message with the commit hash of the stable version,
which the monitoring system knows about because of the metrics it monitors. Deployer now
modifies the infrastructure definitions that there is no canary deployment anymore and the
production system itself takes care of deleting the canary. Since the version to deploy is
already in production, the production system will not touch the other deployment and its
pods.

\subsubsection{advantages: natural to cd, automated, low risk, timesaver, bots can trigger updates}
We illustrated that we are able to automate the whole testing procedure, which is the
advantage of nonfunctional production regression testing. So we can extend the continuous
delivery pipeline in a natural way and we support the developers not only until the
deploy, furthermore we support the developers during run time. The pipeline is now
advanced in a way, that even jobs, which for instance are able to update dependencies of
an application at a very low risk. The update would go through the whole pipeline
including tests in a testing environment, an automated deploy and tests in the production
environment. If the nonfunctional production regression test and automated update is
successful, developers would be completely free of work. The changes of bots would be
robust enough to be able to act fully automated and unguarded, but we would still test the
changes they make and can be sure to not have a major degradation to our production
system.

\subsubsection{automatable, traceable, reproducible, load and integration testing for
  free, low risk}
Finally we want to summarize, what we examined so far: The whole process of nonfunctional
production regression testing is fully automatable, every step is determined and traceable
throughout and even reproducible until the deploy on the basis of the commit hash. We can
save time not only by automization, but also by not having to write load and integration
tests with edge cases, which occur in production. And ultimately the amount of work and
time saved comes at a low risk.

\chapter{Deployer implements and automates NPRT}

This chapter discusses the software deployer. Deployer is the implementation in account to
the new testing procedure nonfunctional production regression testing. The chapter does
not go into implementation details, but describes how deployer works and how to use
deployer from a user's perspective. The chapter does not discuss the surrounding
applications, such as implementations of version control systems, continuous integration
systems. Whereas we discuss the monitoring system, since the testing rules for the metrics
are implemented with it. And we will go into more detail how deployer is integrated with
kubernetes, since this is an important part.

Initially we want to mention in short the technologies we used to implement deployer. It
is implemented in ruby and since it is a webserver, sinatra is it's framework. Deployer
uses git and svn to communicate with the control version system. Moreover deployer has a
plugin which sends messages to bugsnag and slack, to inform the developer in case it
identifies a problem. Also to mention is that deployer uses kubectl to interact with
kubernetes master. Deployer itself is containerized with docker and its natural hosting
solution would be kubernetes. Deployer is published on github under GPL-3.0, an free
software license.

The monitoring system, which we used in our evaluation, is datadog, a commercial
service. Nevertheless prometheus is also a suitable open source solution as monitoring
system.

Similar to the previous chapter, we demonstrate the implementation with the help of
a figure, which shows the whole picture of the implementation. And we are going through
the parts of the figure.

As mentioned earlier, deployer is a webserver, which runs in kubernetes itself. The ... in
the figure is the kuberntes cluster, which is also our production environment. There is a
kubernetes resource service. The service is, as we already know, a loadbalancer and
clients can reach deployer via an http interface. The figure show us that deployer is
composed of multiple pods. The pods are replicated and identical in their behaviour. The
pods are basically stateless, yet every pod has its own caching layer, which we will
discuss in a later section. We assert, that because of the statelessness, we can scale
deployer horizontally and can just add more pods. Along the way, deployer uses
approximately 50 mega bytes ram and very few cpu, so resources shouldn't be a problem. A
deploy takes about 10 seconds. This depends on the size of the repository and the download
rate from the version control system. A request is blocking and waits for the deployment
process to complete. Certainly we do not need a queuing system here and can scale deployer
to the concurrent deploys we need.

Next we want to go through the steps involved in the deployment process. Initially a
client sends a http request to deployer. Therefore the client needs to authenticate via
http basic authentication. In other words, the client authenticates via a username and
password. As we mentioned earlier, our continuous integration system is a software as a
service solution, and one of the reasons why we could not implement a deploy logic inside
the continuous integration system, were security concerns. In this case the token
authorizes to only deploy a specific version from the repository. What that means for us,
is that the client is not able to do everything to our production system. It is for
example not able to deploy other code than ours or read credentials from the production
cluster.

So the loadbalancer proxies the request to a pod and after deployer authenticated a valid
user, deployer starts the deploy procedure itself. At first deployer validates the given
arguments by the client. The deploy request requires the service to deploy and a version
of the service. The client needs to specify either the commit hash or the tag name or
both. The tag would additionally include the branch name, which is used for readability
reasons.

Deployer initially validates the arguments from the request. To be specific, it checks if
the service exists in the configuration. Furthermore Deployer checks if the commit exists
in the repository and deployer checks checks if a the tag, which is associated to the
commit, exists for all the images, which are necessary in order to deploy the
service. Deployer executes the latter validation by communicating with the docker
registry. The client, which can be depctl or curl, do in contrast not have any
validations. The reason for that is that we want to have a single definition for the
validations.

We designed the deployer deploy interface to have the option to only give either a commit
hash or a tag, because it makes things a lot easier when a developer needs to deploy
manually. It makes the development flow more efficient. The commit is totally sufficient
to determine the version, so the clients usually pass the commit hash only.

After deployer validated the deploy request, it then fetches the code in the version
control system lies. When there is git as the version control system, we are using commit
hashes and branches to determine the application version. And when there is subversion, we
are using revision numbers. We implemented subversion, which is more of an ancient
technology, to be able to do the evaluation with DIN legacy system.

As we know, the process of cloning a repository includes persistence and disk
interaction. That is true, but deployer uses its volume just as cache. The reason why
there is a cache at all is of course performance. One of the bottlenecks of the deploy is
to download the repository. If a repository is big, for instance because of images or it
is a repository with a long history of commits, it lasts quite an amount of time to
download it. And if the download rate, the throughput, is very low the duration is even
longer, which is bad. Therefore deployer keeps the already downloaded repositories on
volume, as a cache, and the next time it deploys the same repository, but in a different
version, it only downloads the changes.

As a simplification, every pod has its own cache and it is not very long living. So every
pod utilizes its own volume as a cache, so the cache is integrated in the pod and there is
no extra instance or communication to a caching service. Since docker containers are
immutable, every time kubernetes recreates a pod, docker destroys the volume. Thus the old
data is not available anymore and in other words the cache is empty again.

The version control system contains not only application code, moreover it contains the
infrastructure code as well. In kubernetes especially these infrastructure definitions are
different resource definitions. These resources are deployments and statefulsets and so
on. These resource defintions should be stored in the 'kubernetes' directory. This is a
convention and deployer assumes that this is the location. If an application is not able
to locate it there or does not want to locate it there for any reason, the configuration
of deployer provides an option to reconfigure this location.

The configuration of deployer is very little, because we designed deployer to have
conventions. This is the design, which we know from the rails community as well. It has
the advantage of good defaults instead and there is not the need for everyone to invent a
good configuration.

The configuration of the kubernetes resource path enables two different methodologies in
the microservice approach. The first one is the multiple repository methodology. In that
methodology we have for every single service or microservice a dedicated repository.
Deployers configuration would have for each service the repository url, which would be
sufficient. Second methodology is the single repository, which contains multiple
microservices. There we would have multiple services defined with the identical repository
url. However deployer can differentiate between services, due to the fact that the
configuration for each service has a different location for the kubernetes resource
definitions. An example for these different configurations are shown below TODO.

After having the specific commit hash of the repository checked out, deployer takes the
resources and modifies them in a next step. Especially deployments and statefulsets are
relevant to that modification the deployer makes. In contrast to those resources, deployer
does not modify all other resources, such as services.

Deployer applies two modifications. The most important is the image version. The version
of a docker image is specified by the docker tag. And if the deployment already specifies
a tag, such as 'debian:wheezy', deployer does not change that tag. The reason for that is,
that there may be containers, which the continuous integration system does not build and
tag with the specific commit hash. Commonly another party maintains that docker image. An
example is an additional statsd container running in pod side by side to the application
container.

The other case would be that the tag is not specified in the kubernetes resource. Then
deployer appends the tag to the container image according to the either given commit hash
or given tag.

Sometimes something unexpected is happening during the deployment. This can be for example
that a verification of deployer fails, such as deployer does not find the tag
corresponding to the commit. Or maybe kubernetes api server returns an error for any
reason. If something like the described happens, deployer will inform the developers. We
distinguish between two scenarios. One is, when the continuous integration system
requested the deploy. There our already mentioned bugsnag plugin takes hold. Deployer
raises an error and the error is send to bugsnag, which collects all the errors of
systems. Bugsnag informs the developers via sending a message to a specified slack
channel. Slack notifies the developer who is on call about the bugsnag message and the
developer knows about the problem. Another possibility would be, that the one who created
the commit and triggered the deploy is informed.

The other scenario is that a developer manually sends a deploy request. In that case
the developer typically uses depctl. Deployer discriminates usual curl requests from
requests with depctl. Instead of utilizing bugsnag, deployer answers with an error in the
http response. We can do this due to the fact that the http requests are synchronous.

The sixth step is the communication with kubernetes. Deployer now sends the modified
resources to the kubernetes api, which the last chapter already described. In this chapter
we focus on the comparison between the stable version and new version and how deployer
deploys the canary for the new version.

To deploy the canary, the client sends the deploy canary request to a separate http path ,
which is different from the usual deploy and which deployer defines as well. The client
needs to provide the same arguments as for the deploy request. Namely those arguments are
the service, which is required, and either a commit hash, a tag or both. When deployer
received the canary request, it proceeds almost identical to the deploy request. The steps
are verification, version checkout, modification and application to the kubernetes
api. The difference to the deploy lies in the modification step. Deployer create a new
canary deployment resource.

In the modification step deployer does not only change the container image version, but it
also changes the name of the deployment, which would then represent another
deployment. Deployer names this new deployment resource with the suffix canary. Since
deployer changes the name, kubernetes does not identify the canary deployment as the
original deployment. Instead it treads the canary deployment as a separate resource and
creates pods for that deployment in the different version, which deployer specified.

The service though, which does our loadbalancing, selects both. It selects the pods which
were created from the original deployment. And it selects the pods, which were created
from the canary deployment. Hence the service selects its pods on account of the
label. And the original and the canary deployment share some of their labels, but not all
the labels. In our example the shared label is 'deploy=webserver' and the different labels
are 'track=stable' respectively 'track=canary'.

As it was mentioned in the previous chapter, we want to have fewer pods in the canary
version than in the stable version. Deploy does this simply by scaling the canary
deployment to only one replica.

Now we want to focus on how we collect the metrics during the two versions are in
production. We differentiate between two different collection mechanisms. There is the
group of metrics, which are picked up on the host. The host has information about the
pods' utilization of cpu and memory. So on each kubernetes host, there is a monitoring
agent running, which watches the proc directory and the docker daemon, picks up the
information frequently and then sends it to our monitoring system.

The other group is the metrics, which the application sends. We need to have the
application instrumented in order to collect metrics like throughput, latency and
errorrate. In practice we use the statsd protocol and statsd server for that
purpose. There are statsd libraries for the most languages and frameworks and we get the
instrumentation out of the box. To summarize the process of the data collection shortly,
the instrumented application sends the data to the statsd server in each request and the
statsd server aggregates the data. From there the statsd server forwards the aggregated
metrics data to our monitoring system.

It is important to correctly label the metrics independently from which version we collect
the metric. The monitoring system is then able to distinguish between the metrics of the
stable version and the metrics of the canary version. For the first group of metrics
collection, the monitoring agent can pick up the label from the labeled pod. And for the
second group of metrics collection the instrumentation code of the application picks up
the label from environment variables, which have been set by deployer, either stable or
canary.

Requests to deployer are very simple. We can easily create a curl request and execute it
from our continuous integration system. Another way to interact with deployer is to use
the depctl command line interface. Developers use this tool to manually deploy as well as
during development and to stage versions. Depctl is a very helpful tool, because it
provides shortcuts to deploy a version to a staging system, without having the need to
wait for the whole delivery pipeline. With depctl we can easily skip steps like
tests, which would run on the continuous integration system. This makes the development
flow more fluently.

In the following we will go through the interface of deployer. Deployer provides different
endpoints: ls, show, tags, deploy, canary and version. The design of the deployer api
feels more like remote procedure calls instead of a restful api. This simplifies the calls
with curl and approaches a more intuitive way of deploying a service. Nevertheless the
endpoints are very similar to a restful api. With index, show and tags you can read the
information services, whereas deploy and canary update the services to a specific
version. Deployer does not provide create or delete, though. To add or remove services
from deployer, you edit the configuration.

A client speaks to deployer by either http calls or by simply using depctl command line
interface, which wraps the http calls and assists the developer by automatically
completing information such as the repository to deploy and the version to deploy. You
find the http call and the corresponding depctl command in the following tables.

As mentioned earlier you need to call deploy and canary on a specific repository and you
need to provide either a commit hash, a tag or both. This call updates the service, so
this is why we are using a http put call. An example curl request would be

\begin{table}[!htbp]
  \begin{tabular}{ l|l|l }
    Endpoint & \texttt{depctl} command & Parameters \\
    \hline \hline
    \textbf{GET /} & ls & \\
    \textbf{GET /SERVICE} & show & \\
    \textbf{GET /SERVICE/tags} & tags & \\
    \textbf{PUT /SERVICE/deploy} & deploy & commit, tag \\
    \textbf{PUT /SERVICE/canary} & canary & commit, tag \\
    \textbf{GET /version} & version & \\
    \multicolumn{3}{l}{} % empty line
  \end{tabular}

  Examples:

\begin{verbatim}
 curl --data commit=025838f23a70f5d1df5851c31fa6e36dc0d58b9b \
    https://auth_token:secret@deployer.company.com/gapfish/deploy

 depctl deploy
\end{verbatim}
\end{table}

The ls or index endpoint returns the configured services and the show endpoint for a
specific service shows the current configuration for that service. The tags endpoint show
the available tags for that service. Deployer queries the docker image registry for all
available tags for all images in a service and returns them to the client. The version
endpoint simply returns the deployer version.

The monitoring system provides a tool, which is simply called monitor. And those monitors
continuously evaluate metrics by given rules. So when we have the metrics in the
timeseriesdatabase, the monitors draw continuously graphs from those timeseriesdatabase
metrics. As an example we have a graph for our errorate. In the example we have first have
a graph for the stable version's errorate. The errorate is very low as you expect it. The
other graph is the graph of the canary version and this graph has some higher errorates.

Next we define a composite graph. That would be the graph of the canary version and then
subtract the graph of the stable version. So if the resulting graph goes over the
threshold of 10 errors per second, a alarm is triggered. The monitoring sends the an alert
to deployer, which is a deploy command with the commit hash of the stable version.

The monitoring system we were using in our evaluation, was datadog. But there is also the
opens source monitoring system prometheus, which provides very similar features. Also
google has a similar monitoring system in there internal infrastructure.

Finally in this chapter, we summarize what we have discussed so far. We went through the
infrastructure of deployer, we saw, that deployer scales horizontally. We have seen that
deployer has simple interface, with the two most important commands deploy and canary. We
went through a canary deploy and saw that deployer validates the given arguments and how
it notifies developers if an error occurs. We discussed how and what modificactions
deployer does to the kubernetes resources. We demonstrated the different test metrics and
how the production system sends those metrics to the test system. We went briefly through
the deployer interface and which commands are all provided. We had a look on the
monitoring system and the tests itself and how we rollback the canary in case the test
fails.

Another nice thing to mention about deployer, is that it deploys itself, which fits to the
declarative model and recursion. That means we develop deployer itself with the continuous
deployment flow and can apply nonfunctional production regression testing to deployer.

\chapter{Evaluation}

\chapter{Conclusion}
\section{Resume}
\section{Outlook and future work}

% \part{Foundations}

% \chapter{Introduction: Developer and operation teams converge and both use software
% engineering practices}

% \chapter{Developers use the Continuous Delivery Pipeline}

% \section{The Continuous Delivery Pipeline consists of commitment, continuous integration
% and deployment}
% \section{Software Deployment approaches evolved from manual to automated}
% \subsection{Blue-Green Deployment allows Zero Downtime releases}

% The approach of blue green deployment involves two environments. One is the environment
% which serves production traffic, the other environment is in standby. You deploy to the
% other environment the new version. You then switch routing from the environment, which
% runs the old version, to the environment with the new version.

% This has the major disadvantage that it is a waste of resources. Just one environment
% doing work with serving production traffic as the other environment is just idling. Even
% if you use the idling environment as a staging it would be oversized and still wasting
% resources.

% \subsection{Automation leads to resource saving Phoenix Deployment and Rolling
% Deployments}

% In times of dynamic resource allocation, automation and virtual machines, it became easy
% to automatically spawn new servers. In for a deploy of a new version you would not
% change the servers, but automatically create new ones with the version to deploy, then
% switch the traffic from the old servers to the new servers, and in the end just destroy
% the old servers. This procedure is called phoenix replacement.

% \subsection{Canaries test releases with a small amount of traffic}


% Canary releasing is a way to test new versions of the application in production. But
% testing in production is risky, because when there is an error which leads to a defect
% or failure the users will get affected. And this costs money in any way.

% There comes canary releasing into play. A single canary can't do much harm and it's not
% a big catastrophe if the small canary is malicious. Let me explain it at the example of
% a typically scaled web application. Usually you do not have just a single web server,
% but multiple servers. So when releasing in the canary style, you change just a small
% proportion of the twenty webservers to the new version. Now two versions of the
% webservers are running at the same time.

% Usually you want exactly the same version of webservers in production with the exact
% same configuration. This makes systems easier to debug in case of an error. The maximum
% count of different versions during canary releasing is two. But why go for two different
% versions in production with the canary releasing technique, when it makes debugging in
% general harder.

% It's because with canary releasing you can achieve different goals. The first one is
% truly when an error occurs. Just because less users are affected by the error. The
% majority of webservers is still in the old version, so just a small portion of all the
% users have a poor experience with the erroneous small canary version fraction.

% In case of success, when everything works as expected, it is proven that there is less
% risk of errors, even under real production conditions.


% \subsection{continuous deployment is not continuous delivery}

% \chapter{Operators turned into Site Reliability Engineers}

% Companies, which run software on their servers, usually have a specialized operations
% team. A team of developers programs the software and after the team is finished, the
% software is handed to another team, which runs the software and keeps it running. Those
% teams are usually referred as operations teams. So what are their duties and
% responsibilities exactly?

% At first the already developed software needs to be deployed on an
% infrastructure. Therefore the operations team must provide an
% infrastructure. Infrastructure generally consists of hardware like racks, servers and
% networking devices. But in most cases, software depends on other software components and
% services, which are for example programming languages, dns or databases. Software
% usually changes. Most importantly there are security patches, but also software updates
% with new features are mandatory to install. Hence the operations team is responsible for
% installing, configuring and updating hardware and software components.

% To provide a certain quality of service, infrastructure as well as the software must be
% maintained. That means identifying any problems and moreover fixing the identified
% problems. To identify problems in the first place, there must be any kind of
% monitoring. A good monitoring system\cite[p. 127/8]{devops} finds failures and resulting
% faults, recognizes performance problems, usual workloads and it detects intruders. More
% obvious problems can be detected early, even before the problem leads to failures. For
% example a disk will soon be full or utilization of resources like cpu, ram, network or
% disk i/o is too high. Other failures can not be detected in advance and lead to a
% defective system.

% In case monitoring identifies a problem, the operations team needs to take care of the
% problem. Operations teams need to be on call and manage incidents. A new Incident must
% be rated how severe it is. In advance detected problems potentially leading to failures
% like a disk going to be full in few days, must not be handled immediately. Other
% incidents like an unavailable database service can be business critical and must be
% fixed as rapidly as possible. But often the cause of a failure is not obvious and the
% system must be debugged with specific tools. Critical failures mostly affect users, so
% another part of incident management is to inform the affected parties.

% Google states that the traditional approach of an operations team\footnote{Google
% actually calls it the sysadmins approach} is not efficient. It is expensive, it does not
% scale very well and it creates tension in interests\cite[p. 3/4]{site_reliability}.

% A growing system means there will be more hardware, more software components and a more
% complex configuration. More people are needed to install, monitor and maintain the
% infrastructure and services. The traditional operations team approach has the
% disadvantage, that it does not scale very well and produces more costs, the bigger the
% system becomes. Another disadvantage is conflicting interests of the developer team and
% operation team. Operations teams are payed to create stability and provide a certain
% quality of service. But change is the biggest source of
% failure\cite[p.10]{site_reliability}. That's why operations teams work under the motto
% of 'Never change a running system'. On the other hand, a good service changes to the
% needs of its users. Developers get paid to deliver as much features as possible, so they
% want frequent change. Those are conflicting interests and they harm the whole
% product. For Google it's clear, that the different assumptions and tension in interests
% of operations and developer teams produce a lot of hidden costs.

% \section{Site Reliability Engineers maintain systems like software engineers}

% Operations teams developed a lot of practices to both integrate the fundamentally
% conflicting interests of change and stability, but also scale the operations team
% approach. This reduces obvious and hidden costs and makes the whole work more
% efficient. A lot of those practices became possible, because the infrastructure turned
% from pure hardware into a dynamic software defined infrastructure. Google refers to this
% approach as the site reliability engineering approach. It is operations how an software
% engineer would do it.

% New technologies made it possible to define all infrastructure in code. One of the first
% who provided an dynamic infrastructure is amazon with ec2. The technology of
% virtualization made provisioning and configuration to software tasks instead of hardware
% tasks. This is what it made automate-able. Also Docker made it easier to package
% software and to deploy and configure the services\footnote{See Chapter 2 'Platforms' in
% \cite{infra_as_code} for those technologies}.

% With the possibility to automate the tasks an operations team has to do, there will come
% many benefits. The obvious value is that is scalabillity. When a task is automated and
% can be done by a machine, it is very cheap and easy to execute the task a lot more
% often. But there is a much bigger advance, which automation gives. When a task is
% automated it is a well defined process, which will consistently performed, where a human
% can easily make a mistake by manually executing the task. It can also be extended,
% measured and done at inconvenient times for humans\footnote{See Chapter 'Value of
% Automation' in \cite{site_reliability} for complete discussion of the advantages.}.

% The before one off tasks of provisioning, deploying and configuring have now well
% defined practices and processes. Those tasks are now reproducible because the definition
% or configuration of the infrastructure can now be stored in a version control
% system. And with automate-able deployment processes like zero downtime releases, phoenix
% deployment or rolling updates, those processes can be done continuously.

% Operations teams have collected great knowledge on monitoring. What are the key
% indicators for monitoring a system. What does a good monitoring infrastructure
% provide. Those questions I am going to examine in detail in the upcoming sections. But
% very important is that monitoring can notify a human or even better trigger automated
% tasks, that humans do not need to be involved at all, when a problem occurs.

% Also incident management can partly be automated. Distributed systems like databases
% have nowadays automatic failovers. Issues which have been serious, major incidents
% before and a humans had to manually intervene can, are now automatically triggered and
% the systems heal themselves. Disaster recovery is now easily done, because you can
% automatically reprovision the whole infrastructure as mentioned before.

% With those practices it means that in the end this means that the conflicting interests
% of change and stability can nowadays be more and more integrated.

% \section{Monitoring to identify Problems}
% \subsection{Health checks measure availability}
% \subsection{Measuring Latency, Traffic, Errors and Saturation identifies failures and
% performance problems}
% \subsection{Incident Management (/Notifications) for appropriate and fast actions in
% case of Problems}

% \chapter{Metrics measure team efficiency and software quality}
% \section{Velocity and cycletime are efficiency metrics for an agile team}
% cycle time measures quality of delivery engine
% \subsection{Deploys/Week indirectly measures velocity}
% and it measures the effect of a quality delivery engine we want many deploys per week
% \subsection{Deploy Duration is import for cycle time}
% \section{MTTR and Failurerate measure the quality of a software}
% we want low risk per deploy to achieve MTTR and low Failure Rate
% \subsection{LOCS/Deploy indirectly measures the risk per deploy}


% \part{New Practices}

% \chapter{Post Release Testing extends the Continuous Delivery Pipeline to support
% maintaining a system}

% The three main steps in the continuous delivery pipeline are: commitment to version
% control, continuous integration and release. After releasing, the new version will be
% operated. This includes monitoring, logging, security aspects and incident
% management\footnote{In ``Site Reliability Engineering'' monitoring and notifying
% principles~\cite[p. 55-63]{site_reliability} are well described.}. To enable developers
% to take more responsible in running the software, it is necessary to extend the
% practices of the continuous delivery pipeline.

% In my masterthesis I want to optimize the software engineering practices in order to
% empower developers in their bigger responsibilities. I want to focus on the process of
% deployment and enhance the continuous delivery pipeline. To achieve this, I want to
% examine operation practices like monitoring and incident management. And then extend the
% deployment process by three consecutive steps.

% The first step is to completely automate deployments of software features and achieve
% continuous deployment. The next step is to sensibly monitor new releases to give
% automatically feedback. Third and lastly, identify incidents to automatically trigger
% rollbacks to self-heal the software system. I'm going to call these last two steps
% continuous post release testing.

% \section{Post Release Testing leads to lower time to market}
% cycle time
% \section{It makes Releases consistent, measurable, fast and scalable}
% mttr, automation/automatic discussion
% \section{It is a new opportunity for risk management}
% identify test before release is a mttr of zero, after release still fast. easier to test
% in production (complexity of system)
% \section{Companies are already post release testing their software systems}
% \subsection{Netflix uses Simian Army to live test their systems}
% \subsection{Synthetic Monitoring tests a complex distributed system}
% \section{Post Release Testing with Canaries is appropriate for testing non change}
% ErrorRate as monitoring measure for automation problems in error rate measure defect and
% failure solution a secific heuristic
% \subsection{Black-Box monitoring is only one part and monitoring change is difficult}
% \subsection{Canary testing is important for maintenance but not feature deploys}
% \subsection{Continuous Delivery is a requirement}
% \subsection{Notifications in case a canary behaves different}
% \subsection{Automated Rollbacks for a automatic self healing system}

% \chapter{Implementing Canary Post Release Testing}
% \section{New technologies drive new techniques}
% \subsection{Kubernetes is a Cluster OS}
% \subsubsection{Resource Management in Kubernetes is made for high available services}
% \subsubsection{Deployments implement Rolling Updates}

% The extend to the canary releasing technique is a rolling update. It unifies phoenix
% replacement and canary releasing and extends it to a full deployment. You start with one
% server in the new version and spawn it automatically. Now it becomes part of the whole
% pool of servers and serves traffic as the server is ready. Now two different versions
% are serving traffic, just like with a canary release. But after the first step the
% deployment goes on and then destroys a server instance in the old version as you would
% do in the phoenix replacement. Then the procedure will be done multiple times until all
% servers of the old version are replaced by the new version.

% \subsection{DataDog is a Cluster Monitoring Systems as a sevice}
% alternativen Graphana, Prometheus
% \subsubsection{The main components are: Datacollection, Timeseriesdatabase, Graphing and
% Alerting}
% \subsubsection{DataDog integrates well in Kubernetes}
% \section{Deployer is a Service for Continuous Deployment and enables Canary Post Release
% testing}
% \subsection{Deployer integrates into the Continuous Delivery Pipeline}
% \subsection{Deployer integrates into Kubernetes and deploys itself}
% \subsection{Deployer integrates into Monitoring and enables Canary testing}
% \subsection{The main Deployer API features are Deployments and Canary deployments}
% depctl, curl
% \subsubsection{Deployments deploy a whole repository}
% \subsubsection{One Canary per Replicated Pods can be deployed and monitored}
% \subsubsection{Immediate Notifications in case of failure}
% difference depctl and curl
% \subsubsection{Automatic Rollbacks in case defect Canary}
% via datadog and triggers deployer future: staging deploys

% % \subsection{bugs + debugging}
% % \subsection{ilities}
% % \subsection{security}
% % \subsection{monitoring}


% \part{Evaluation}

% \chapter{GapFish is the company to evaluate Deployer}
% \section{GapFish's services are complex and highly available}
% overview of Gapfish's services
% \subsection{GapFish's Operation Service is used by internal Staff and Customers}
% operation service == operation.gapfish.com
% \section{GapFish uses tools for continuous deployment}
% \subsection{GapFish differentiates between development and operation}
% how is the deployment tradditionally done
% \subsection{Kubernetes enables GapFish to have development and operations in one team}
% which services are migrated to kubernetes

% \chapter{Metrics are implemented as logging output}
% \section{Deploys/Week}
% \section{Deploy Duration}
% \section{LOCS/Deploy}

% \chapter{Results}
% \section{Metrics: traditional vs. new approach}
% \section{theoretical/practical conclusion for deployer and cd}
% \section{for Gapfish}
% \section{Lessons learned and future}

% \chapter{resume}
