\chapter{Introduction}
\label{chap:intro}
\section{Automation to Support Maintenance Work}

% TODO: microservices case study von klems
Devops~\cite{devops_definition} and microservices~\cite{microservices_fowler} are
practices or even cultures, which break down the silos between the development and
operation teams and their conflicting goals. For instance Amazon and Facebook successfully
use those practices to build and run their software products~\cite{build_run,dev_at_fb}.

However the responsibility of maintaining a software product changed. A single team is
responsible in two aspects: their own code and the dependencies they are using and relying
on. The need for maintenance of own code exists, for instance with nonfunctional changes
concerning performance, scalability or bug fixes. Code written by others needs to be
maintained as well via adapting and updating to new versions.

Devops practices interconnect operations including those maintenance necessities with
development work. Nevertheless there is room for improvement in terms of continuous
automatable processes regarding maintenance. The \gls{cd}
pipeline~\cite{cd_humble_pipeline} is such a continuous automatable practice. But it
covers the process only up to the deploy whereas maintenance involves more than just the
delivery.

Instead of employing more engineers, Google combats workforce problems with
automation~\cite{sre_automation}. They recognized that automation does not only save
valuable programmer time, it furthermore adds reliability to the process. Google explains
how valuable automation is and successfully illustrates it with their site reliability
engineering approach~\cite{sre_intro}.

This thesis proposes with \gls{nprt} a practice which addresses the amount of maintenance
work. The practice is designed for teams, which have the responsibility of developing and
operating their own code as well as code dependencies. \gls{nprt} extends the \gls{cd}
pipeline and automates continuous testing of nonfunctional changes in production. I
implemented \gls{nprt} at two companies, \gls{din} and GapFish, in order to make an
evaluation.

\section{Thesis Outline}

\paragraph{\ref{chap:problem} \nameref{chap:problem}}
Chapter~\ref{chap:problem} elaborates on the problem of the amount of maintenance work and
demonstrates how the thesis contributes to solve the issue.

\paragraph{\ref{chap:background} \nameref{chap:background}}
The glossary explains the practices and techniques required in order to understand the
problem in depth. In contrast to this, chapter~\ref{chap:background} briefly explains the
major kubernetes resources on the example of a 3 tier webapp and names the software
dependencies.

\paragraph{\ref{chap:macro} \nameref{chap:macro}}
Chapter~\ref{chap:macro} describes the conceptual macro view of \gls{nprt}. The text goes
through all the steps of the pipeline and demonstrates the components and how they
communicate with each other.

\paragraph{\ref{chap:detail} \nameref{chap:detail}}
Chapter~\ref{chap:detail} goes more into detail of the thesis contributions. It
demonstrates the design of Deployer, the software I wrote in account to enable \gls{nprt}
in practice. The chapter also elaborates on the collection and comparison of the metrics.

\paragraph{\ref{chap:eval} \nameref{chap:eval}}
Chapter~\ref{chap:eval} evaluates \gls{nprt} in three aspects. Firstly, it discusses the
performance with the help of experiments. Secondly, it provides a qualitative evaluation
of \gls{nprt} based on the experiences at \gls{din} and GapFish. And lastly, it relates
\gls{nprt} to other practices, tools and sophisticated metric analysis.

\paragraph{\ref{chap:conclusion} \nameref{chap:conclusion}}
Chapter~\ref{chap:conclusion} summarizes the thesis and suggests more automation
opportunities and extensions to the implementation.

\chapter{Problem Definition}
\label{chap:problem}

\section{Maintenance Responsibility Shift}

% devops/microservices aproach successful development and operation
The devops~\cite{devops_definition} approach and the
microservice~\cite{microservices_fowler} culture create practices to solve the issues of
the conflicting tension~\cite{sre_intro} between a development and operation team. On the
one hand the aim is to change the software product quickly and on the other hand to
provide a stable and reliable software product of high quality. Vogels, the chief
technology officer of amazon, once said in an interview ”You build it, you run
it”~\cite{build_run}. This quote represents the awareness of teams to not only be
responsible for the development and deliver features. Moreover the team is also
responsible of operating their product in production, because operation is an essential
part of a successful software product. One team is consequently responsible of the
development, operation and maintenance of their own code.

% responsibility of deps shifts from ops to dev
Furthermore the responsibility of software dependencies shifts. In the past, development
teams delivered code and depended on the previously installed software
dependencies. Operation teams were responsible of maintaining those installed
dependencies. Virtualization changed this. Teams operating virtual infrastructure operate
and maintain the hardware and software of this infrastructure. They do not maintain the
specific dependencies of the code, which the development team develops. The development
team can choose which dependencies they want to use. Accordingly there is a clear
isolation between the dependencies of the virtual machine infrastructure and the
development team.

% contra arg vms operated by ops team
You could argue that virtual machines have still the same functionality like bare metal
servers and need to be maintained in the same way by the same operation
teams. Nevertheless it becomes easier to maintain dynamic virtual
infrastructure~\cite{infra_as_code_platforms} in comparison to bare metal
infrastructure. And the easier it becomes, the more it enables developers to operate their
software product themselves.

Containerization manifests the responsibility shift. It is a further step of
isolation. The operating system is able to isolate cpu and memory for every process. Yet
with containerization the operating system can also isolate the disk between processes,
which means it can isolate the installed dependencies for every process. Containerization
techniques even potentiate the ease of managing dependencies. So the responsibility of the
software dependencies shifts resultantly towards the team, which builds and runs the
software product.


\section{Lack of Focus on Maintenance}

% is risky
If developers did not do maintenance properly, there is a risk of decreasing the quality
of the software product. For instance there can be security vulnerabilities. Those can be
very harmful to the company of the software product. If developers do not maintain the
software products properly more issues arise, such as bugs or performance
problems. Therefore, maintaining a software product comes with responsibility. If
developers disregard maintenance and operations work, the quality of the software product
suffers.

% operate and maintaining software means: config, monitoring, incident management
What does it mean to operate and maintain a software product in production? It is all the
work related to running the software product. It includes the configuration of the
servers. It includes monitoring of performance, errors and security issues. And it
includes handling of incidents. For example if there are any issues, you would need to
react to those issues and to make the software product for instance scale or apply
security patches.

% (maintenance is a lot of work and) is easily disregarded in favor of velocity
Agile and continuous practices focus primarily on feature development. Those practices
altered software development to deliver features faster into production. Highsmith argues
”velocity kills agility”~\cite{velocity_kills}. Also Fitzgerald observes the issue of a
”misplaced focus on speed rather than continuity”~\cite{continuous_fitzgerald}. Focusing
on development velocity only has a major drawback. Operating software in production and
maintaining it needs to be done properly to ensure the quality of a software
product. Companies easily disregard the importance of operation and maintenance of the
software product.


\section{Underestimated Amount of Maintenance Work}

% maintenance of the written code itself is a major source of changes
The software written by the development team needs to be maintained as well. As I
illustrated previously, different issues occur in production. For instance performance and
scaling issues or simply bugs. The developers need to refactor the software without
changing the actual functionality. Those changes are mostly invisible or do not alter any
function visible to users. At Facebook~\cite{dev_at_fb} those invisible changes are more
than 50\% of the total changes.

% dependencies are becoming more
Moreover, software products have a high amount of dependencies. This totally makes sense,
because when programming a software you do not want to do it from scratch. Firstly, the
dependencies are different abstraction levels, such as a processor, a programming language
and a programming framework. And secondly, the dependencies are modules which encapsulate
a certain functionality. Those are very common functionalities for applications, for
instance authentication of a user. The abstraction as well as the reusable software
modules make it easier and faster for developers to reach their goals.

% many versions in open source
The open source communities change and release their software continuously. Sometimes
those changes consist of new feature, often times just refactorings and every now and then
security patches. This results in releases and new versions.

% almost every software product relies on open source
On the web, there is rarely a software product which does not depend on open source
software. The most web servers run linux and there are many popular open source
frameworks which make up the basis of the running software products.

% RedHat/Ubuntu companies provide bundling and update service
Companies like RedHat or Ubuntu provide enterprise support to those open source
projects. Those companies bundle those open source projects, care about the bugs and
security issues and provide tools to upgrade from one version to another easily. As a
result companies with a software product can outsource the maintenance work of the
dependencies. There is still the need though, to integrate those changes to the own
software product.

% hard to upgrade at once
One problem is that it is not always easy to upgrade from one version to
another. Refactorings result in breaking changes and the depending software needs to adapt
to those changes. As a result software such as windows xp is still out in the wild. Due to
the fact that companies are not able to adapt to those major changes at once. However the
very same applies to open source frameworks such as django or ruby on rails.

% microservices create ability to continuously upgrade one part by another
% it is much work though
One reason for the introduction of microservice is the flexibility to update only a small
specific part of the whole software product~\cite{microservices_fowler}. With a
microservice architecture, developers are able to update a single service without having
to upgrade the whole product at once. Developers can choose a microservice for the update
depending on its importance. Or developers can update microservices one by one without the
major problem of upgrading the whole software product at once. Consequently this means
developers need to update more often and the amount of work of updates itself is higher.

\section{Disregard of Automation for Maintenance Work}

% compensation by automation instead of more humans
There are several things companies can do to keep up with the amount of maintenance
work. Facebook~\cite{dev_at_fb} for example hires more developers. This is expensive
though. A much better solution is automation. Developers should automate some of the
maintenance work.

% practices exist which connect dev and ops
You could argue that devops practices already exist and connect development and operations
including maintenance of a software product. However, the focus lies mostly on testing the
quality of a change in a lean fashion and to reduce risk of a change in
production~\cite{devops_definition}. It is true that those practices help to deliver
changes faster to production and help to detect issues at low risk. Nevertheless,
monitoring and reacting to errors requires manual work.

% testing in production is already done
There exist practices which already enable production testing~\cite{dev_at_fb}. One
example is A/B testing, where you compare two different versions in production. Another
canary releasing, with which you change only a few replicated parts of the production
environment to reduce risk. There is also shadow releasing, where users are not affected
by the change, which goes into production. Netflix tests its services with the simian army
in production to detect unforeseen events in a controlled
environment~\cite{antifragile_org}.

% containerization reduces risk for updates
Containerization reduces the risk of updating a service. Morris describes phoenix
replacement in immutable infrastructure~\cite{infra_as_code_phoenix}. Kubernetes has the
approach of rolling updates which implements the same practices. A containerized ecosystem
is typically immutable. Rather than changing a running container directly, the container
image repository would receive and store an updated container image including the changes
to deploy. Then kubernetes would create a new container based on that updated image. After
kubernetes started this new container, it can stop the old container. The container image
repository still stores every container image version. Those saved images allow to easily
rollback to a previous version. Automation enables developers to deploy more frequently
which usually leads to deploys with few changes. Few changes per deploy and the ability to
rollback makes updates less risky. In result, updates in an automated containerized
immutable infrastructure are less risky compared to a manually operated infrastructure.

Murphy et al. illustrate the value of automation~\cite{sre_automation}. Software
reliability engineers automate tasks to firstly save the valuable human resources and
secondly reduce the risk in those tasks because of human errors. This is a good approach
and focuses the quality of the software product.

Immutability reduces some of the risks and automation brings value. Combining those two
arguments, there is an obvious need of an automated process for updating services.

% cd provides good automation, but has lack of maintenance focus
There is a lack of truly automatable practices for maintaining a software
product. \gls{cd} brought up good automatable practices to continuously deliver changes
and features into production. It has the lack of good practices though to maintain a
software product.

\gls{cd} does not focus on the maintenance work and the devops approach does not focus on
automating the maintenance work as previously examined. I conclude that there is a need
for practices, which continuously support and automate maintenance work.

\section{Contribution of this Thesis}

The contribution of this masterthesis is a practice, which can fully be automated in order
to maintain a software product. It extends the \gls{cd} and continuous deployment pipeline
as those go until release and no further. \gls{nprt} extends the pipeline to the
production environment, where we need to operate and maintain a system.

The thesis suggests \gls{nprt} as a practice which consists of different techniques. Those
technique cover three major steps. First, quickly deploy a change to production. Second,
monitor the change, having it in production at low risk. And third in case of a regression
automatically roll back.

I implemented the tool, Deployer, in order to connect essential components of the
practice, \gls{nprt}. The thesis evaluates the performance of the tool as well as the
practice itself via experiments as well as in the production environment of an existing
software product.

\chapter{Background}
\label{chap:background}

\section{Prerequisite Practices and Terms}

Some background is required to understand the thesis. The glossary lists those most
important practices and terms. Moreover it provides well chosen references in order to
enable the reader to read up on more detailed descriptions of the prerequisites. Those
practices and terms are usually well known in software engineering.

\section{Typical 3 Tier Webapp in Kubernetes}

I'll explain the typical implementation of a 3 tier webapp in kubernetes as it helps a lot
to understand the next chapters. The application tier is usually implemented with a
\emph{deployment} which monitors the presence of a specified number of stateless
\emph{pods}. A \emph{service} acts as a loadbalancer and selects the pods and forwards the
request. The data tier is implemented with a \emph{statefulset} which monitors the
presence of the stateful pods. Those stateful pods keep their host name and mount the same
volumes when they are being restarted.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0 \textwidth]{3tierwebapp}
  \caption[3tierwebapp]{Typical 3 tier webapp in kubernetes.}
  \label{fig:3tierwebapp}
\end{figure}

When the service receives a request from the client, it selects a pod via round robin and
proxies the request to the pod. The pod probably communicates with the database and sends
the request back to the client, where the service acts again as a reverse proxy.

\section{Software Dependencies}

This section shortly mentions the dependencies, which I used to implement Deployer and the
tools for other components of the pipeline.

Deployer is implemented in ruby\footnote{\url{https://www.ruby-lang.org}}. it is a
webserver and sinatra\footnote{\url{http://sinatrarb.com}} is its framework. Deployer uses
git\footnote{\url{https://git-scm.com}} and
subversion\footnote{\url{https://subversion.apache.org}} to communicate with either git or
subversion as the control version system. Moreover deployer has a plugin which sends
messages to bugsnag\footnote{\url{https://www.bugsnag.com}} and
slack\footnote{\url{https://slack.com}} to inform the developer in case it identifies a
problem. Deployer uses kubectl to interact with
kubernetes\footnote{\url{https://kubernetes.io}, see also
  \cite{kubernetes_architecture,kubernetes_architecture2}} master. Deployer itself is
containerized with docker\footnote{\url{https://www.docker.com}} and its natural hosting
solution would be kubernetes. We published deployer on
github\footnote{\url{https://github.com/gapfish/deployer}} under GPL-3.0, a free software
license.

The previously developed GemUpdater\footnote{\url{https://github.com/schasse/gem_updater}}
creates pull request with dependency updates.

The monitoring system, which I used in the evaluation, is
datadog\footnote{\url{https://www.datadoghq.com}}, a commercial service. Nevertheless
prometheus\footnote{\url{https://prometheus.io}} is also a suitable open source solution
as monitoring system. The \gls{ci} systems we use for the evaluation are
codeship\footnote{\url{https://codeship.com}} and
jenkins\footnote{\url{https://codeship.com}}. As an image repo, we use docker
hub\footnote{\url{https://hub.docker.com}}. Goreplay\footnote{\url{https://github.com/buger/goreplay}}
served to make experiments with production traffic.

\chapter{NPRT Approach}
\label{chap:macro}

In \gls{nprt} the pipeline deploys a nonfunctional change in form of a canary to the
production environment. The new version is compared with the stable version side by side
in production. In case the new version shows a regression, the pipeline will automatically
roll it back. The novelty of the approach is that it creates a feedback loop between
development and production, which is fully automatable. The following elaborates on the
term in more detail.

\emph{Nonfunctional} refers to the change, which the monitoring system monitors. Usually
maintenance changes are nonfunctional changes, for instance performance improvements or
security updates. \emph{Production} refers to the environment, in which the versions are
monitored. The production traffic reaches the new version as well as the stable
version. The term \emph{regression} refers to the testing strategy. The monitoring system
tests the metrics of the stable version and the new version for a regression.

\gls{nprt} provides some further features, which are not included in the term. Indeed the
testing approach is completely automatable and developers are able to continuously apply
it to new versions. I designed \gls{nprt} in respect to failing as fast as possible and to
inform developers as early as possible. \gls{nprt} is most useful for maintenance
changes. Even automated maintenance changes, for instance dependency updates, are
possible.

\gls{nprt} naturally evolved from common practices such as \gls{ci}, \gls{cd} and
continuous deployment and extends those practices. The already established practices
support developers before and until the software deployment. In contrast to that,
\gls{nprt}, supports developers during and after the deployment. In other words it
supports developers to run and maintain applications in production, which formerly has
been a business of operations teams.

\section{Pipeline Overview}

\begin{figure}[htbp]
  \includegraphics[width=0.97\textwidth]{nprt_overview}
  \caption[nprtflow]{Overview of the NPRT flow.}
  \label{fig:nprt_flow}
\end{figure}

To understand the testing approach in a whole, it is necessary to show a complete overview
of the whole pipeline and environment. Figure \ref{fig:nprt_flow} pictures an overview. In
the following I will go through the steps of the pipeline and discuss them.

\section{Continuous Integration as Requirement}

The first parts of the pipeline are commonly known and established practices: \gls{ci},
\gls{cd} and continuous deployment. It is necessary, though, to touch on them and
integrate them in the whole picture. It will illustrate important design decisions for
\gls{nprt}.

First the developer changes the code on his local machine and creates a new version. He
then pushes the new version to the \gls{vcs} as shown in~\texttt{(1)}.

After the push of the new version, the second step is a message~\texttt{(2)} to the
\gls{ci} system. This message holds the reference of the new version and the \gls{ci}
server pulls the new version from the \gls{vcs}. Now the \gls{ci} system has three major
jobs. Firstly it starts a build process~\texttt{(3-4)}, secondly it runs the
tests~\texttt{(5)} and the thirdly gives the deploy signal~\texttt{(6)}.

In step~\texttt{(3)}, namely the build, the \gls{ci} system typically compiles binaries,
renders assets and may create further artifacts. For our purposes it is especially
necessary to build at least one or multiple container images. The \gls{ci} system then
pushes the ready built container image to an image repository in~\texttt{(4)}. Later the
image repository serves the built images.

One import thing is that every built container image relates to a specific version. The
version reference, which was created by the \gls{vcs}, serves for this purpose. It is
important to be able to trace the version through every step in the pipeline. With this
thought in mind, the \gls{ci} system tags the container image with the version reference
and an extra name. Just to mention that the extra name is not absolutely necessary to
definitely associate the container image with a version. But it turned out that a human
readable name is very helpful to recognize a version and to know what the version is about
at first sight. The \gls{ci} system derives that extra name from the branch name. This
tag, consisting out of the version and extra name, will follow us through the whole
pipeline as readable and unique reference.

The second \gls{ci} step is to run the tests. The figure~\ref{fig:nprt_flow} shows this in
step~\texttt{(5)}. The tests themselves can be split into multiple stages, such as unit,
feature and smoke tests. At this point, though, I will not recall all the details of
automated testing at this point.

The last step of the \gls{ci} system is to send a deploy signal, step~\texttt{(6)} in the
figure. But it depends on the results of the tests, whether to send that deploy signal or
not. The tests can be successful or fail. If the tests fail, the \gls{ci} system does not
send a deploy message, the pipeline stops and the \gls{ci} system may inform the
responsible developer. If the tests are successful in all test stages, the \gls{ci} system
will send the deploy signal to Deployer.

It makes sense to deploy only specific versions and not every commit. The practice which
is pretty common, is that you develop new features in a seperate branch. For those
versions you usually do not send a deploy signal even though the branch's build and tests
are successful. Usually after the tests, a review and the developers decide to deploy the
changes to production. When the decision is made and one developer merges the change into
a specified branch, for instance the master branch, and the version will go to production.

However, just to clarify, the \gls{ci} system sends each built image for every single
version to the image repository. This is independent of successful tests or the decision
to go to production. The reason why I want to have every built image in the repository is
to be able to test it in the \gls{ci} system, locally and maybe a staging system. But this
is just a side note.

The \gls{ci} system gives the deploy signal when two requirements are fulfilled: the build
and tests are successful and developers decided that the version will go to production.

Until this point, as I already mentioned, this is \gls{ci} practice, which is commonly
used in software development. I require it for \gls{nprt}. For \gls{nprt} it is especially
important to associate and trace every step through the whole pipeline. For that purpose I
need the \gls{ci} system to tag, as the previous paragraphs explain, the container images
with the exact version reference.

\section{Customizations of Continuous Deployment}

\gls{nprt} is an approach, which I designed to be completely automatable. It is crucial to
not only have \gls{ci}, but to have a customized continuous deployment process as well.

The next component in the pipeline is Deployer. Deployer is a service, which realizes the
customized parts of the continuous deployment practice. In the context of this thesis, I
implemented Deployer. Chapter~\ref{chap:detail} describes Deployer in detail. This chapter
demonstrates how Deployer integrates in the pipeline and in the environment among all the
other tools. It is crucial to have full control over the deployment process and as a
consequence it was necessary to implement Deployer.

It would also be possible to implement the logic of the deploy in the \gls{ci} system. But
I had to decide against that, because the deploy needs full access to the production
system and the \gls{ci} system is in our case outsourced to a third party company. I did
not want to give other companies full access to the production cluster. However, this
meant that I had to implement some steps again, which a \gls{ci} server already
implements. Deployer pulls the version from the \gls{vcs} as well as the \gls{ci}. The
difference to the \gls{ci} system is that Deployer's interest lies in the resource
definitions and not the application code. The resource definitions define the application
infrastructure.

Deployer receives the deploy signal from the \gls{ci} system\texttt{(6)}. The deploy
signal again includes the version reference. Now Deployer executes three major steps:
firstly pull the resource definitions~\texttt{(7)}, secondly modify the resource
definitions~\texttt{(8)} and thirdly send the resource definitions to the production
cluster~\texttt{(9)}.

In the first step Deployer pulls the code from the \gls{vcs}~\texttt{(7)}. The \gls{vcs}
also holds the resource definitions, which describe the application infrastructure. I
wanted to version control the resource definitions in order to be able to relate the
version of the infrastructure, the version of the code and the version of the
artifacts. In kubernetes those definitions contain different resources, which
chapter~\ref{chap:background} explains with an example.

In the second step, Deployer modifies those resource definitions~\texttt{(8)} in a way
that the production system uses the correct container image which relates to the version
to deploy. The modifications of Deployer also achieve that the running container is aware
of its version. The latter is important to later tag the metrics with the version
reference.

The third step of Deployer is to apply the modified infrastructure definitions to the
production cluster which is shown in the figure as step~\texttt{(9)}. In result a deploy
changes two things: application infrastructure as well as application code changes.

The production cluster receives the modified infrastructure definitions. The production
changes the cluster state according to the definitions. Most important is that pod
instance of two versions in parallel are in the cluster. The production cluster fetches
the container images in step~\texttt{(10)} from the image repository. The images, which
the \gls{ci} system built in step~\texttt{(3)}. The production cluster is aware of the
specific image identified by the tag.

\section{Metric Collection and Comparison}

Figure~\ref{fig:nprt_flow} illustrates the two differing versions with two colors. Most of
the running instances are in the stable version, red, and only one instance or in practice
few instances are in the new version, orange. The loadbalancer sends traffic to both
versions and both versions respond to clients, on which section~\ref{test_architecture}
elaborates.

The production cluster collects the metrics, which are necessary for the regression tests.
I adapted those metrics from the four golden metrics of Google's
\gls{sre}~\cite{sre_monitoring}. The metrics are throughput, latency, errorrate and
utilization.

The interest lies in the metrics of the two specific versions. Consequently the production
cluster labels those metrics with the version references. This is important since the
metrics of the differing versions will be compared with each other. The production cluster
labels and then sends those metrics to the monitoring system~\texttt{(10)}.

The monitoring system stores all the metrics of the two versions in a timeseries
database. The monitoring system evaluates those metrics by drawing graphs and comparing
those graphs with each other. One example would be that it draws two graphs for the
latency in one diagram. The first latency graph is the one of the stable version. The
second latency graph is the one of the new version. The monitoring system monitors now
those two graphs for a regression. That means in the case of latency, that if the latency
of the new version is much bigger than the latency of the stable version, the monitoring
system detects this as a regression. I explain the specifics of the metric comparison and
in the section~\ref{sec:metric_comparison}.

\section{Rollouts and Rollbacks}

Two possible scenarios arise. The first one would be that the new version runs in
production for a certain amount of time and the monitoring system does not identify any
regression. In this case the monitoring system does nothing. A deploy with the reference
to the new version triggers the full rollout of the new version. A usual deploy message
with the new version reference is sufficient to do so. Deployer receives the deploy
message, deletes the canary and modifies resource definitions to have the new version as
the new stable version and the production cluster proceeds, stops and starts the running
instances accordingly.

The second scenario is that the new version turns out to be a regression compared to the
stable version. In that case the new version in form of the canary should be rolled
back. The monitoring system identifies the regression and sends a message to Deployer, as
figure~\ref{fig:nprt_flow} pictures in step~\texttt{(12)}. Accordingly the test for
regression has failed.

Deployer receives the rollback message, pulls the resource definitions again, modifies the
resource definitions accordingly and applies the modified resource definitions to the
production cluster. Just to be precise, it is actually a deploy message with the commit
hash of the stable version, which the monitoring system is aware of because of the metrics
it monitors. Deployer modifies the resource definitions similar to step~\texttt{(8)}. The
deploy is idempotent, so that Deployer it can repeatedly receive the same deploys. This
can happen in the case a developer as well as the monitoring system trigger the
rollback. The production system itself takes care of deleting the canary instances in the
new version. Since the instances of the stable version are already running in the
production cluster, the production cluster does not touch the running stable instances.

\chapter{Implementation of Deployer and Metric Comparison}
\label{chap:detail}

I implemented Deployer and setup metrics collection and comparison in order to realize
\gls{nprt} with kubernetes. Deployer receives a deploy message, modifies kubernetes
resource definitions and applies those modified resources to the production
cluster. Deployer is able to automatically deploy a canary of a new version of an
application which runs in parallel to instances of the stable version. The kubernetes
cluster is setup to collect the metrics from the instances of both versions. Datadog, the
monitoring system compares the metrics and checks them for regressions.

\section{Deployer Architecture}

Similar to the previous chapter, I demonstrate the implementation with the help of
figures. The following sections will explain all the steps of
figure~\ref{fig:deployer_architecture}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{deployer_architecture}
  \caption[nprtflow]{Detailed Deployer Architecture and Steps.}
  \label{fig:deployer_architecture}
\end{figure}

As mentioned earlier, Deployer is a webserver which runs in kubernetes itself and is
stateless. The production cluster in figure~\ref{fig:deployer_architecture} is the same
kubernetes cluster in which the application to test runs in. The yellow box is a
kubernetes service. The service is, as described in chapter~\ref{chap:background}, a
loadbalancer. Clients can reach Deployer via an http interface. The figure shows that
Deployer consists of multiple pods. These pods are replicated and are identical in their
behaviour. The pods are basically stateless, yet every pod has its own caching layer which
section~\ref{sec:cache} explains in more detail. Deployer uses approximately 50mb memory
and few cpu, so computing resources are no problem. I designed Deployer to have
synchronous communication with a client. Deployer waits for the deploy process to complete
before it responds. This has the advantage of clients receiving feedback about the
deploy. Furthermore it enables Deployer of being independent of other services such as a
queuing or worker system. Deployers statelessness leads to easy horizontal scaling of
Deployer. It is possible to add more pods if needed in order to achieve more concurrent
deploys.

\section{Deployer Interface}
\label{sec:interface}

I designed requests to Deployer to be simple and usable. Curl or any other http client
serves as a client. In result Deployer can easily integrate with other systems for
instance a \gls{ci} system. Another way to interact with Deployer is to use the depctl
command line interface.

The depctl command line interface wraps http calls and assists developers with automatic
completion of the service to deploy and completion of the version to deploy. With depctl,
developers can easily skip steps of the whole \gls{nprt} flow. For instance they can skip
the tests which would run on the \gls{ci} system. This makes development more fluent and
developer friendly.

\begin{table}[!htbp]
\begin{verbatim}


   | Endpoint            | depctl command | Parameters  |
   |---------------------+----------------+-------------|
   | GET /               | ls             |             |
   | GET /SERVICE        | show           |             |
   | GET /SERVICE/tags   | tags           |             |
   | PUT /SERVICE/deploy | deploy         | commit, tag |
   | PUT /SERVICE/canary | canary         | commit, tag |
   | GET /version        | version        |             |

\end{verbatim}
\caption{Deployer interface.}
\label{tab:interface}
\end{table}
Examples:
\begin{verbatim}
curl --data commit=025838f \
  https://auth_token:secret@deployer.me.com/app/deploy

depctl deploy
\end{verbatim}

The following will go through the interface of Deployer. Deployer provides different
endpoints: ls, show, tags, deploy, canary and version. The following descriptions uses the
term service in the meaning of a microservice.

The \emph{ls} or index endpoint returns the configured services and returns the endpoints
for the service, which the configuration defines.

The \emph{tags} endpoint shows the available tags for that service. Deployer queries the
docker image registry for all available tags for all images for a service and returns them
to the client.

\emph{Deploy} and \emph{canary} are the endpoints for either a regular or a canary
deploy. The requests needs to provide either a version reference, a tag or both. The
request updates the service, this is why the endpoint defines a http put.

The \emph{version} endpoint simply returns the Deployer version.

\section{Logic and Flow of a Deploy}

\subsection{Authorization}

Next I want to go through the steps involved in the deploy process. Initially a client
sends a http request to Deployer~\texttt{(1)}. The service resource proxies the request
to Deployer~\texttt{(2)}. After that, Deployer needs to authenticate the client via http
basic authentication. In other words, the client authenticates itself via a username and
password. As I mentioned earlier, the \gls{ci} system is a software as a service solution,
and one of the reasons why I implemented Deployer is the concern about security. The
\gls{ci} system can not have full production cluster access. In the case of Deployer the
auth token authorizes to only deploy a specific version from the repository. For us that
means, that the \gls{ci} system is only able to deploy versions from the \gls{vcs}. The
\gls{ci} system is for instance whether able to deploy other code than ours nor read
credentials from the production cluster.

\subsection{Fetching the Code and Caching It}
\label{sec:cache}

After Deployer authenticated a valid client, it then fetches the code from the
\gls{vcs}~\texttt{(3)}. If git is the \gls{vcs}, Deployer uses commit hashes and branches
as version reference and to determine the code version. If subversion is the \gls{vcs},
Deployer uses revision numbers instead. Deployer implements subversion, which is more of
an ancient technology, to be able to do the evaluation with the \gls{din} system.

It is obvious that the process of fetching a repository includes persistence and disk
interaction. Deployer uses the volume just as cache, though. The reason for the existence
of the cache is performance. One of the bottlenecks of a deploy is downloading the
repository. If a repository is large, for instance because of pictures or a long history
of commits, it lasts quite an amount of time to download it. If the download rate is
additionally low the duration is even longer. This leads to long running deploys and a bad
development flow experience. Therefore Deployer keeps the already downloaded repositories
on its volume as a cache. The next time Deployer deploys the same repository in a
different version and Deployer only fetches the changes.

As a simplification, every pod has its own cache and lives as long as the pod. Thus every
pod utilizes its own volume as a cache and the need for communication and an extra caching
service does not exist. Since docker containers are immutable, every time kubernetes
recreates a pod, kubernetes destroys the volume. Thus the cached data is not available
anymore and in other words the cache is empty again.

The version control system contains application code as well as the resource
definitions. In kubernetes these resource definitions define the application
infrastructure. Resources define for instance deployments, statefulsets and
services. These resource definitions should be stored in the 'kubernetes' directory. This
is a convention of Deployer and it assumes that the 'kubernetes' directory is the standard
location. If application is unable to store the resource definitions in the 'kubernetes'
directory, the configuration of deployer provides an option to reconfigure the location.

The configuration of the kubernetes resource directory enables two different methodologies
of the microservice approach. The first one is the multiple repository methodology. In
this methodology every single service or microservice receives a dedicated repository.
The second methodology is the single monolithic repository. Such a repository contains
multiple microservices.

\subsection{Validations}

After Deployer fetched the specific version reference from the repository, Deployer starts
the deploy procedure. At first Deployer validates the arguments, which the client
sent. The deploy request requires the service and the version to deploy. The client
defines the version by passing either the commit hash, the docker image tag or both. The
tag includes the branch name, which makes the reference more readable.

Deployer initially validates the arguments of the request. Deployer checks if the service
exists in the configuration. Deployer checks if the commit hash exists in the repository
and Deployer checks if the tag exists for all the images, which are necessary in order to
deploy the service in the new version. Deployer receives the available tags for a docker
images by communicating with the image repository~\texttt{(4)}.

For simplicity and usability the client sends usually only one argument, either the commit
hash or the tag. One argument is completely sufficient to determine the version, so a
client can provide either one of them. For instance, a developer has to pick only the
commit hash for a manually triggered deploy. This creates a more efficient development
flow. Regardless of the whether Deployer receives multiple or only one argument, Deployer
validates if the deploy of the version is feasible.

The http client and depctl do not have any validations. Consequently, Deployer is the
single point of defining validations.

\subsection{Modifications of Resource Definitions}

After Deployer validated the deploy request, Deployer takes the previously checked out
resources and modifies them in the next step~\texttt{(5)}. Especially deployments and
statefulsets are relevant to those modifications. In contrast to those resources, Deployer
does not modify all other resources, such as services.

Deployer applies two main modifications: the \emph{environment} and the image tag. The
former is especially important for the metrics collection. Deployer modifies the
environment in order to enable the instrumentation to differentiate between the stable
version and the canary version. The running pods can later read the version information
from the environment variables and are able adjust the instrumentation configuration to
add the version information.

The second important modification is the \emph{image tag}. The image tag specifies the
container image. The specific container image is import as it holds the specific
application version. Deployer picks the correct image tag belonging to the version
reference from the \gls{vcs}.

Deployer modifies the resource definitions only when the image tag is unspecified. In case
a deployment resource already specifies an image tag, for instance somebody else maintains
the image. Then Deployer does not change the specified tag. The reason is that the
\gls{ci} does not build images, which are maintained by somebody else. Such images do not
have a tag with the specific version reference of the \gls{vcs}. One example is an
additional statsd container running as a sidecar in the application pod.

The other case would be that kubernetes resource defintion leaves the tag specification
open. Then deployer appends the tag to the container image according to reference the
which the client provided.

The next step is to communicate the modifications to the kubernetes
api~\texttt{(6)}. Deployer sends the modified resources to the kubernetes api. The
kubernetes master manages the rollout of the changes. It swaps out one pod by another and
replaces the old version with the new version. The procedure is called rolling update.

\subsection{Errorhandling}

Sometimes something unexpected happens during the deploy. For example Deployer does not
find the tag to the corresponding commit hash, then the verification fails. Another
example is that the kubernetes api server returns an error. If this happens, Deployer will
inform the developers. Deployer distinguishes between two different clients.

The first one is a \gls{ci} system, which sent the deploy request. In this case the error
the error reporting system\footnote{Chapter~\ref{chap:background} mentions bugsnag. The
  application of GapFish uses it for error reporting.} comes into play. Deployer sends the
error to reporting system ~\texttt{(7)}, which collects the errors of multiple
systems. The error reporting system informs the developers via sending notifications to a
specified channel. GapFish has for example a slack chat channel, on which the person on
call subscribes and receives notifications from.

The other client is a developer sending a deploy request manually. In this case the
developer typically uses depctl. Deployer differentiates usual curl requests from requests
with depctl. Instead of utilizing the error reporting system, deployer answers the http
request with an errorcode and a message in the http body. As mentioned earlier the
communication with deployer is synchronous.

In result developers always receive feedback as fast as possible in case a deploy
fails. The information receives the developer in both cases, whether the \gls{ci} system
triggers a deploy or a developer manually triggers a deploy.

\section{Testing Architecture}
\label{test_architecture}

This section explains the testing architecture inside the production cluster. Few canary
pods in the new version run next to the stable version. These pods are able to receive and
respond to production traffic.

\begin{figure}[htbp] \centering \includegraphics[width=\textwidth]{deployer_flow}
  \caption[nprtflow]{Testing Flow Implementation.}
  \label{fig:testing_flow}
\end{figure}


To deploy a canary, the client sends the deploy canary request to a separate http path,
which section~\ref{sec:interface} previously describes. The client needs to provide the
same arguments as for the deploy request. Namely those arguments are the service and
either a commit hash, a tag or both. When Deployer receives the canary deploy request, it
proceeds almost identical to the deploy request. The steps are verification, fetching the
version, resource modification and application to the kubernetes api. The difference to
the deploy lies in the modification step. Instead of deploying a new version, Deployer
creates a additional new canary deployment resource.

In the modification step Deployer does not only change the container image version, but it
adds another canary deployment. Deployer names the new deployment resource with the name
of the original deployment resource and a canary suffix. Kubernetes differentiates the
canary deployment from the original deployment. Kubernetes creates pods for the additional
canary deployment in the different version, which Deployer specified.

The service though, which does the loadbalancing, selects both. It selects the pods which
belong to the original deployment. And it selects the pods, which belong to the canary
deployment. Hence the service selects its pods in account to the label. The original and
the canary deployment share a subset of their labels. A shared label would be for instance
'deploy=webserver'. And the two differentiating labels would be 'track=stable' and
respectively 'track=canary'.

I designed the canary deploy to result in fewer canary pods than stable pods in order to
reduce risk as the previous chapter describes. Deployer does this simply by scaling the
canary deployment to a single replica.

\section{Metrics}

\subsection{Metric Collection}

This section focuses on how metrics are being collected during the two versions are in
production. There exist two separate collection mechanisms. Those are hosts, which collect
metrics, and the application instrumentation, which sends metrics.

Firstly the group of metrics, which the host picks up. The host has information about the
pods' utilization of cpu and memory. Accordingly, on each kubernetes host, a monitoring
agent runs, which watches the '/proc' directory and the docker daemon. The agent picks up
the information frequently and then sends it to the monitoring system.

Secondly, the group of metrics, which the application sends. In order to collect the
metrics, the application needs to be instrumented. Examples for those metrics are
throughput, latency and errorrate. In practice, I instrumented the applications to use the
statsd protocol and statsd server for that purpose. Simple statsd libraries for most
languages and frameworks exist even though the instrumentation could be better. In case
there is a good instrumentation, developers can use it without much effort. The
instrumented application then sends the data to the statsd server after each request and
the statsd server aggregates the data. From there the statsd server forwards the
aggregated metrics data to the monitoring system.

It is important to correctly label the metrics in order to decide whether a canary or a
stable version sends the metric. The monitoring system is then able to distinguish between
the metrics of the stable version and the metrics of the canary version. For the first
group of metrics collection, the monitoring agent can pick up the label from the labeled
pod. And for the second group of metrics collection, the instrumentation code of the
application picks up the label from environment variables, which Deployer has set to
either stable or canary.

\subsection{Metric Comparison}
\label{sec:metric_comparison}

The comparison between the canary and the stable version is the part of \gls{nprt}, which
creates the feedback loop.

The monitoring system basically consists out of a time series database, a graphing user
interface and an alarm system. The time series database persists the metrics. And the user
can define graphs from the metrics, which the user interface then displays. Moreover, the
monitoring systems enables developers to define rules which monitor the metrics in the
time series database. In case the metrics violate any rule, the monitoring system sends a
notification.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0 \textwidth]{metric_comparison}
  \caption[metric_comparison]{Comparing the stable version with the canary version.}
  \label{fig:metrics_comparison}
\end{figure}

Figure~\ref{fig:metric_comparison} pictures an example of a rule on latency. The red
continuous line is the latency of the stable version. The yellow continuous line is the
latency of the canary version. The graph compares them side by side. The red dotted line
depicts a threshold. The threshold depends on the latency of the stable version and is
always 50ms higher than the stable version in the figure. In case the latency of the
canary goes above the threshold, the canary violates the rule and triggers an alarm. The
alarm is in fact a webhook, which would roll back the canary instance.

Next I elaborate on the metrics to monitor: Latency, utilization, errors and throughput.

Latency defines the duration of successful requests. Latency is a typical performance
metric of a webserver. Utilization mostly refers to metrics such as cpu and memory
consumption. The errorrate is defined as counted errors in a time period. Applications
sometimes have the problem of reporting too many errors without focus on important
ones. Also throughput of successful requests can be an indicator of the health of an
application.

Deviations between the stable and the canary always exist. The monitoring rules should
identify problematic regressions. This is oftentimes hard to
achieve. Section~\ref{sec:eval_metrics}, in chapter~evaluation, illustrates one such
example of a metric comparison which serves as a test.

\subsection{Metric Parameters}

A comparison includes parameters such as a threshold, an aggregation window and a duration
above the threshold. These parameters need adjustment in order to calibrate the rule and
have a good recall and precision. The following gives insights on the factors, which
affect those parameters.

The ratio of the stable and canary metrics as well as the throughput affect the size of
the \emph{aggregation window}. Let's assume the application has a very low throughput of
only 1 request per minute and a ratio of 2 stable to 1 canary instance. In case the
aggregation window is 1 minute large, then either the stable pods would have 1 request or
the canary pod would have 1 request. This would be an improper aggregation window. Instead
the aggregation window could be rather large with a size of 120 minutes. As result, the
stable pods would receive 80 requests and the canary pods would receive 40 requests. This
would be a much more appropriate aggregation window. Consequently the lower the throughput
and the lower the stable canary ratio is, the larger the window should be.

The distribution of throughputs and latencies over the routes of the application affects
which percentile to use. I make an example: If there would be a single route, which has a
low throughput and a very high latency compared to other routes. The average would be an
improper \emph{aggregation}. The outliers of low latencies would affect the
average. Instead a percentile or the median would represent the latency much better.

The range of throughputs and latencies also affects the \emph{threshold}. If the
application had a larger range of throughputs and latencies, a proper throughput would
need to a higher. However we can affect the volatility of the metrics by choosing a larger
aggregation window. This allows to lower the throughput.

As a result, application with higher and the more evenly distributed throughputs have
better metrics in order to define proper monitoring rules.

\chapter{Evaluation}
\label{chap:eval}

This chapter evaluates the \gls{nprt} approach in three different aspects, experiments, a
lessons learned section and a related work section. Firstly, the experiments aim to
present an evaluation the approach's performance in the context of a whole delivery
pipeline and how a sample metric comparison performs. The second part is a qualitative
discussion of the lessons learned of the application use cases at \gls{din} and GapFish.
In the case of GapFish it was a application in production. The third aspect, related work,
compares \gls{nprt} with other approaches in production and references similar tools like
Deployer.

\section{Experiments}

I had the opportunity to do experiments with two different production applications. The
first one is an apache webserver, which is the middleware in front of a tomcat application
server. This application serves \gls{din}'s
website\footnote{\url{https://www.beuth.de}}. In the case of \gls{din}, I setup a version
control system, a kubernetes cluster with deployer, instrumentation and monitoring
setup. Software engineers working for \gls{din} helped to setup a container image
registry. I also migrated the apache webserver to be able to run on kubernetes. The
kubernetes resources for the apache webserver consisted of a service and a deployment with
two pods.

The second application is a ruby on rails application which employees of GapFish utilize
as backoffice tools for several panel
platforms\footnote{p.e. \url{https://www.entscheiderclub.de} and
  \url{https://www.spiegel-panel.de}}. As an employee of GapFish I previously migrated the
ruby on rails application to kubernetes and utilized codeship as \gls{ci}, docker hub as
container image registry and integrated datadog as monitoring system. The application
consists of a service and nine deployments. Eight of the deployments are different
background workers each with one pod. And one deployment is for the ruby on rails
application server with two pods.

The experiments for the \gls{din} apache application were conducted on virtual machines
rented from Atos. The experiments for the GapFish application were conducted on virtual
machines hosted at Google. The appendix describes more about the specific details and the
data used for the plots.

Each of the experiments has three consecutive executions with three different canary
versions in the deploy and rollback experiments. The measurements had to be manually
collected from all the different logs of the involved systems, that are a local http
client, \gls{ci} system, deployer and kubernetes.

\subsection{Deploy}

The first experiment examines the performance of a deploy of a canaries. In account to
that an http client triggered a deploy as described in previous
chapters. Figures~\ref{fig:deploy_din} and \ref{fig:deploy_gapfish} show the duration
between the deploy request (6) and the running canary. The blue part of the bar indicates
the time of the deployer pulling the infrastructure code from the version control system,
check of the version reference and image availability, modification of the resource
definitions and application to the kuberentes cluster (6-8). The red part is the duration
of kubernetes starting the canary (9-running).

\begin{figure}[htbp]
  \label{fig:deploy}
  \centering
  \begin{subfigure}{.5\textwidth}
    \includegraphics[width=\linewidth]{deploy_din}
    \caption[DIN]{\gls{din} Apache Middleware}
    \label{fig:deploy_din}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \includegraphics[width=\linewidth]{deploy_gapfish}
    \caption[GapFish]{GapFish Rails Application Server}
    \label{fig:deploy_gapfish}
  \end{subfigure}
  \caption[Deploy]{Deploy}
\end{figure}

Figure~\ref{fig:deploy_din} shows that \gls{din}'s apache middleware takes between 9 and 12
seconds with a median of 10 seconds. Deployer took in the third execution 9 seconds and
the start of the canary was in every execution at 1 second.

Figure~\ref{fig:deploy_gapfish} shows that GapFish's rails application server takes
between 65 and 106 seconds with a median of 93 seconds. The time deployer took was 18
seconds in median.

In both experiments the second and third execution are faster than the first one. That is
because deployer caches the code from the version control system and does only need to
download the difference in the consecutive executions. The impact of the cache on \gls{din}'s
apache webserver was not that big, because the repository size is comparably
small. Whereas the cache could save almost half of the time at the GapFish rails
application.

The start of \gls{din}'s apache pods is with 1 second quite fast. This demonstrates the quick
scheduling of kubernetes and quick startup of docker containers compared to virtual
machines. The start of GapFish's application is between 45 and 75 seconds with median of
70 seconds.

Accordingly \gls{din}'s apache application deploy significantly faster. So deployer is able to
handle multiple deployments as in the case of GapFish. The multiple requests to the
kubernetes api slightly reduce the deploy duration though. An improvement would be a
batched request. Deployer's cache works well for big repository sizes.

It is important to pay attention with slow application startup durations, when looking for
fast deployments. In the case of GapFish's rails applicatoin server, this makes the
biggest impact.

The variation of the second and third executions come from the nature of the cloud
environment and internet services. In particular the requests to the image container
registry differ in the duration.

\subsection{Deploy in Pipeline}

This section shows a deploy in the full context of the \gls{cd} pipeline. The steps start
with the commit and push of a new version in the version control system, the build of the
docker image(s), saving the recently built images in the image registry, time spend by
deployer and the remaining startup of the canary pods. The testsuite runtime is not
included in this diagram, so that focus lies on the systems this thesis integrated.

At GapFish I could access all the involved system. This was not the case at \gls{din}. That is
why the experiment was conducted for the GapFish rails application only.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\linewidth]{whole_pipeline}
  \caption[Metric Comparison]{Whole Pipeline}
  \label{fig:whole pipeline}
\end{figure}

The total time of the \gls{cd} pipeline was between 172 and 414 seconds with a median of
192 seconds. Again the caching saves time in the second and the third execution. This time
the \gls{ci} system and the image registry partly cache the layers of the images. They
only have to build and send the differing parts of the container image.

As a result we see the overall pipeline duration for a typical canary change at round
about 3 minutes, including a build and deployment. The makes a continuous deployment of
new canaries absolutely feasible.

\subsection{Rollback}

In this section we take a the look at the rollback performance of deployer. The blue part
of the bar again represents the time deployer is involved, that is from the rollback
request including steps (7) and (8) until the kubernetes api responded. The red bar
represent the remaining time the pods take to shut down.

Figure~\ref{fig:rollback_din} shows the rollback duration of a \gls{din} apache canary
is between 4 and 5 seconds. The duration send in deployer is less than in the deploy case.
That is mostly because it the delete request to the kubernetes api is faster.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}{.5\textwidth}
    \includegraphics[width=\linewidth]{rollback_din}
    \caption[DIN]{DIN}
    \label{fig:rollback_din}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \includegraphics[width=\linewidth]{rollback_gapfish}
    \caption[GapFish]{GapFish}
    \label{fig:rollback_gapfish}
  \end{subfigure}
  \caption[Deploy]{Rollback}
\end{figure}

In the case of GapFish's rails application, we have longer rollback times. All three
executions of the rollback were 63 seconds. This is due to slow shutdowns of the worker
pods. The kubernetes partly waits for the shutdown process of the pods. That is why
deployer spends most of the 48-49 seconds waiting for kubernetes api responses.

Also for the rollback case, deployer performs better with \gls{din}'s apache system, because of
less deployment resources and the fast apache application. Nevertheless the deploys as
well as the rollbacks involve zero downtime. So it's acceptable to have a bit longer
durations.

\subsection{Metric Comparison}
\label{sec:eval_metrics}

This last experiment aims at a first evaluation of the feasibility and a first assessment
of a metric comparison and the time of a broken canary being in production. \gls{din}
provided one hour of production traffic of their website, which a goreplay process
recorded on production and replayed with the test setup.

Due to the fact the test setup database state differs to the production database, the
throughput of the requests with http 200 responses was at the low rate of 25 requests per
minute. Requests of in production logged in users mostly had http 404 as responses. Those
successful requests basically consisted of the publicly visible pages of the website.

At first I deployed a canary version with no changes compared to the stable version. The
two version side by side calibrated the metric comparison. The first half of the
production traffic determined the size of the aggregation window as well as the threshold
of the comparison.

After the calibration was done, the actual experiment starts. The experiment measures the
time between a defect canary being started and the canary being rolled back. Each
of the three executions had its own unique part of the second half hour production traffic.

\begin{verbatim}
ratio =
  apache_GET_200_count.rollup(sum, 30) /
  (2 * apache_canary_GET_200.rollup(sum, 30))
threshold = 3
abs(1 - ratio) > 3
\end{verbatim}

The test compares throughput of successful responses. The canary responses are weighted by
the factor 2, because there was one canary pod and two stable pods. An aggregation window
of 30 seconds and a threshold 3, which means the canary throughput had to differ 3 times
as much as the one of the canary.

Figure~\ref{fig:metric_comparison} shows that in each of the three executions, the test
has successfully detected the broken canary. The duration of the defect canary in
production was between 129 and 400 seconds with a median of 185 seconds.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.5\linewidth]{metric_comparison.png}
  \caption[Metric Comparison]{Metric Comparison}
  \label{fig:metric_comparison}
\end{figure}

The experiment demonstrates that the automation of a regression test in production is
feasible and completely automatable.

\section{Lessons learned}

The GapFish ruby on rails application of the previous experiments as well as two other
GapFish ruby services utilize the \gls{nprt} approach in production. This section presents
qualitative feedback, which comes mainly from use of \gls{nprt} in production.

On the one hand nonfunctional regression testing was used to test maintenance of own code,
such as refactorings and performance improvements. On the other hand \gls{nprt} was also
used to maintain dependencies. The previously developed GemUpdater regularly and
automatically opened pull requests with dependency updates. The canary was tested using
error metrics, which I explain later on in more detail.

\subsection{Quick Debuggable Deploys}

Deploys of a round about 1.5 minutes conform the need of developers of sufficiently quick
deploys. This is good compared to oftentimes long running test suites as it is the case with
GapFish's rails application, which has a current median duration of 12.5 minutes. The time
of a deploy is compared to that much faster and engineers do not get distracted and when
they deploy manually.

One benefit is the ability to track the version reference from commit to production the
test in production throughout the whole pipeline. This makes it easy to debug versions,
which run or used to be running in production. It is also possible to track the code
change of stable and a canary version which used to be running side by side.

It has been a good decision to version the kubernetes resources, which are basically
infrastructure definitions as well. This made it possible to track also track the
infrastructure changes.

Due to the nature of containerized infrastructure and its immutability, rollbacks to
specific versions are simple. The rolling update mechanism provides deploys and rollbacks
without downtime. This fact lead to a certain confidence of the engineering team to deploy
and rollback and the amount of deploys increased.

\subsection{Engineers Know the Product}

The experiments already showed that a simple side by side comparison of the throughput can
have good results for a test metric with production traffic. You could argue that
production metrics can be hard to evaluate and in a test environment it is rather easy to
achieve good results. The problem of overfitting in statistics is well known.

When choosing the wrong metrics for the production regression tests of the ruby
applications of GapFish, it would not have been feasible to deliver as good results as the
first test showed. The problem lies in the granularity of the metrics. For example a total
throughput or total error rate of a comprehensive application is misleading. GapFish's
rails application serves not only humans, but provides also interfaces for other
services. This leads on the one hand sparse usage\footnote{The users of the rails
  application are only internal employees.} on routes for users and on the other hand
machines machines utilized some routes extensively. The overall throughput was a poor fit.

The much better choice for a metric was using the error aggregation of bugsnag. Bugsnag
aggregates errors, that it shows specifically newly introduced errors. This has been the
best metric for the regression tests.

% Nevertheless the regression tests have to be maintained as well, just like a
% testsuite. For this purpose it would be better to have the ability to version the
% regression tests.

In conclusion as a developer and engineer of the already have a good intuition on what is
working and what not. This is another evidence for the benefits of the same team building
and running a software product.

\subsection{Continuous and Fully Automatable}

Deployer connects the \gls{ci} system, kubernetes and the monitoring system. These systems
create the ability to create a fully automatable delivery pipeline and a system for fully
automated rollbacks. Those are the step stones for more possibilities.

\gls{nprt} reduced the efforts of maintenance work on own code. The Engineers were able to
test changes in production quickly and have a small feedback loop.

The engineers working for \gls{din} are still sceptical to have unsupervised deploys. But in the
example of GapFish, the engineers became confident about the automation of the deployment
and regression test in production and start to use unsupervised deploys and tests more often.

The possibility of dependency update automation with GemUpdater turned out to save effort
at low risk as well. The combination of GemUpdater and \gls{nprt} lead to a continuous
fully automated approach for dependency updates, which used to be an important, but
tedious maintenance work.

\section{Related Work}

There are multiple publications, which use methods to test software production in
production. Tang et. al. demonstrates how Facebook does gradual rollouts with
gatekeeper~\cite{fb_config_management} and tests with high amounts of traffic. But not
only canary or A/B tests are done in production. For example Falé proposes synthetic
monitoring~\cite{synthmonitoring}, which aims at functional tests being made in
production. This enables testing microservices in complex environments. Also Tseitlin
describes how Netflix~\cite{antifragile_org} tests their systems with the simian army,
which produce random defects in a controlled production environment. This practice lays
open unknown issues, which can be fixed in order to gain better reliability.

There are also countless open source tools, which help to implement \gls{cd}
similar. Vamp\footnote{\url{https://vamp.io}} has the ability to deploy applications to
kubernetes automatically. Also spinnaker\footnote{\url{https://www.spinnaker.io/}} can
deploy applications to cloud environments, with kuberentes being one of them. Spinnaker
very recently released a feature to analyse canaries, just like for \gls{nprt} needed. The
difference to deployer is that vamp and spinnaker are very complex and exhaustive. They
also require other databases. Whereas deployer is a very simple and lightweight tool with
focus on a single purpose and consists of only one docker image. Nevertheless deployer can
be integrated to several monitoring solutions by the simple http interface.

Schermann et. al. developed bifrost~\cite{bifrost}, an open source tool similar to the
previously mentioned gatekeeper at Facebook. It implements canary testing and other
testing strategies as well. The approach is a complete different, though, to
deployer. Bifrost is technically an automatically configurable middleware or
loadbalancer. It does not provide any automation for the deploy itself. Deployer
implements the deploy automation. It could be interesting to combine the two tools to have
more testing strategies available.

I used datadog in the evaluation as monitoring system. However, there also exist other
monitoring systems, such as the open source monitoring system prometheus, which provides
very similar features. Google has a similar monitoring system in there internal
infrastructure~\cite{sre_monitoring}.

Lastly, I am going to mention publications, which deal with metric analysis. Zimmerman
motivates the need for data scientists software teams~\cite{data_science_role2} and
identifies the multiple roles~\cite{data_science_role}. Farshchi et al. evaluates a
regression based approach in order to detect errors during rolling
upgrades~\cite{anomaly_detection}. They demonstrate how to identify the set of metrics,
which have the highest chance to detect the errors. Their approach has a high precision
and recall. In comparison to that Bakshy and Frachtenberg developed a statistical model to
analyse errors of a distributed service in an A/B
experiment~\cite{error_analysis_of_distr_system}. They provide guidelines for the design
of error analysis models.

\chapter{Conclusion}
\label{chap:conclusion}


% design decsion: argumentation of historical comparison
We have different possibilities to compare those version. One possibility is, that we
compare current and historical data. For instance to compare the metrics of the current
production system with the metrics of the production system of the day before or even the
week before and compare the different versions of those times.

We are following a different approach, because when we are comparing the current
production system with the production system of last week, we have lots of different
changes. The current traffic must not be the same traffic as last week, the load of the
production system must not be the same load and other system with which the application is
interacting with must not be the same.

That is why we decided to compare two different versions which run in the production
system concurrently. This brings not only the advantage, that you have the very same
traffic, but also the advantage, that there is less risk involved. We illustrate the
advantage of less risk now by demonstrating the process of deploying the second version
and comparing it to the old version.

% deployer creates canaries and compares. don't know where to put this
Ok, if you compare the two versions with each other, you will do it as follows. Deployer
create another deployment resource from the one that already exists. Deployer calls this
other deployment resource canary deployment. The creation of the canary deployment
resource has the effect, that not only pods of version I are running in production, but
there is pods running in version II as well. Similar to the regular deployment, the canary
deployment defines how many pods in which version are supposed to be running.

% only 25% canaries -> less risk
We want to test, if there is a regression respectively a degradation between the two
versions. On account to the fact, that a regression is possible and when introducing
change, a regression is very likely, we at least want to affect as little users. So what
do we do for that? In our example there three pods running in version I and only one pod
in version II. This is a ratio of three to one and due to the fact that the loadbalancer
uses round robin as the scheduling algorithm, only one in four requests, so 25\% of the
total traffic is sent to the pods in version II, which is to test.

This certainly lowers the risk of failure and that users are affected by a
regression. Even if the request of specific single user hits the degraded pod, the next
request of the same user has the probability 75\% to hit the old stable version.

% limitation to nonfunctional changes
A limitation to this technique is that the new version II needs to be able to run side by
side with the old version I. In most cases, that means that the new version needs to be
semantically almost identical to the old version. So version II should not provide
functional changes compared to version I, but only nonfunctional changes. However that
means we cannot test new feature like in an A/B test. Instead we can test performance
improvements, refactoring or updates.

% reference to canary deploy
They call this technique canary releasing. Again, you change would only change a part of
the production system, the canary instead of the whole. Devops~\cite{cd_humble_deploy}
examines this technique in more detail.
% TODO: difference to canary testing

% limitation to nonfunctional changes
Assuming we would want to test features in production, the current implementation of the
technique is not suitable. If we wanted to do that, we would need to include the
loadbalancer. The loadbalancer would need to remember which user is proxied to which
version, so that the next request of that user goes to the same version, thus the user
sees the same set of features as before. The design of the database could potentially be
affected as well and could be needed to be loadbalanced for the users. The technique we
just described is usually called an A/B test. The disadvantage of the A/B test is that the
same user will hit on the same potentially degraded service and it is not that simple to
automatically provide a stable service to the user. Due to simplification, we did decide
to not include the implementation of the loadbalancing.

% two versions in production suboptimal, rolling updates presume it
We want to state that it is suboptimal to run multiple versions in the cluster like also
mentioned in devops TODO. Rolling updates require it to be able to have two versions in
production, though. And kubernetes utilizes rolling updates as a technique to provide zero
downtime deployments. Accordingly our proposed technique does not introduce a worsening to
that. But as in devops mentioned, you should avoid to run more than two versions at the
same time in production. Deployer ensures that by either updating a deployment, creating a
canary deployment, or creating a deployment in a new version, just before it deleted the
canary deployment.

% enhancement to use gemupdater
Especially to test the latter, security updates, is absolutely appealing, since we can
fully automate the procedure of updating the dependencies of our application in a fully
automated and in a way, which would have a very low risk. We could have a job, which
checks frequently for any new version, pushes the updates to the version control system,
the continous integration system runs the pre deploy tests, deployer deploys the update
and even in production we check the update for an regression. We could save a lot of
developer time, who would usually need to take care of the whole updating procedure.

% low risk because of automatic rollbacks
And even if there is a degradation in production, a small amount of requests is affected,
because we send only a reasonable amount of traffic, which arrives at the same time, to
the potentially degraded version. Further more we limit the time the degraded version is
in production, because we automate detection of the degradation and the rollback to the
old stable version.


% argument pro microservices
We let this running for a specified time in production. We need to decide on how long we
want to compare the versions. That depends on how much traffic is in production, because
when we would few traffic in production, we wanted to compare for a longer time. We
suggest to have a well balanced test scenario in terms of load. The time depends on how
much traffic there is in production and how often a team wants to deploy its
application. A team which is working with a monolithic application has the disadvantage,
that every change in every part of the software causes a deploy and deploys are more
frequent. This limits the time in production for the canary. Instead if we have a
microservice environment, the deploy affects only a specific service, hence little part of
the whole application. As a result deploys are less frequent and we have more time for the
canary in production.

% advantage: no need to create test traffic
We do not need to generate the test traffic, we do not need to weight traffic and we do
not need to think about edge cases. These are all advantages, that we get for free from
the production traffic. We save time and work, because the users generate the test data,
instead of us.

% advantage: edge cases included
The users create more requests and with that test data for parts of the application, which
are more important. Consequently the users reasonably weight the test data. And lastly the
longer we run the comparison in production, users will produce more of those edge cases,
which would be hard to make up.

% request cloning to staging
We could simply clone the requests, send the original request to the stable version and
send a cloned request to the canary. The loadbalancer could then differentiate between the
two responses of the two versions. We would reject the response of the canary. And we
would forward the response of the stable version.

As a result we even lower the risk, because the potentially degraded version does not even
respond to real users. Ergo we do not have any risk of a degradation of our production
service which we cause by testing the new version.

\section{Summary}

We illustrated that we are able to automate the whole testing procedure, which is the
advantage of \gls{nprt}. We can extend the \gls{cd} pipeline in a natural way and support
developers not only until the deploy, furthermore we automated a part of the developer's
job during run time. The pipeline is now advanced in a way, that we can change application
code and test it without the supervision of the developers at a very low risk. The change
goes through the whole pipeline including tests in a testing environment, an automated
deploy and tests in the production environment. If the \gls{nprt} and automated deploy is
successful, developers will be completely free of work. Potential changes of bots would be
robust enough to be able to act fully automated and unsupervised. However we still test
the changes they make and can be sure to not have a regression in our production system.

Finally we want to summarize, what we examined so far: The whole process of \gls{nprt} is
fully automatable, every step is determined and traceable throughout and even reproducible
until the deploy on the basis of the version reference. We save time not only by
automization, but also by not having to write load and integration tests with edge cases,
which occur in production. And ultimately the amount of work and time saved comes at a low
risk.


Finally in this chapter, we summarize what we have discussed so far. We went through the
infrastructure of deployer, we saw, that deployer scales horizontally. We have seen that
deployer has simple interface, with the two most important commands deploy and canary. We
went through a canary deploy and saw that deployer validates the given arguments and how
it notifies developers if an error occurs. We discussed how and what modificactions
deployer does to the kubernetes resources. We demonstrated the different test metrics and
how the production system sends those metrics to the test system. We went briefly through
the deployer interface and which commands are all provided. We had a look on the
monitoring system and the tests itself and how we rollback the canary in case the test
fails.

Another nice thing to mention about deployer, is that it deploys itself, which fits to the
declarative model and recursion. That means we develop deployer itself with the continuous
deployment flow and can apply \gls{nprt} to deployer.



\section{Outlook and Future Work}

\gls{nprt} is good approach to continuously automate the maintenance of a software
product. Nevertheless there are possible improvements. A automatic approach such as
GemUpdater works well. More automatic service of such kind are needed. One obvious use
case is the automation of updating docker base images. A docker base image updater would
help to maintain continuously update the operating systems and software, on which the
applications docker images are based. Furthermore there is a similar need for update
mechanism for other languages such as java or python, which come with their own dependency
managers.

I image a completely unsupervised update mechanism for updates. An engineer would
resultantly only have to do manual work in case of a regression. Engineers would need to
have more confidence in unsupervised deploys, though.

Another aspect is the lack of good open source instrumentation. On the one hand good
instrumentation exists. On the other hand the instrumentation is proprietary and it cannot
be utilized outside of the proprietary ecosystem. We should invest more in more fine
grained open source instrumentation for popular application frameworks and webservers.

In order to be able to better maintain the metric comparisons, these would need to go into
version control as well. With datadog it was not possible to version those tests,
though. With other tools for instance prometheus this may be possible.

% extend nprt for functional changes by including a loadbalancer
